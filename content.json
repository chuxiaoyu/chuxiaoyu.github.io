{"meta":{"title":"Memex","subtitle":"","description":"","author":"Xiaoyu CHU","url":"http://example.com","root":"/"},"pages":[{"title":"Categories","date":"2021-07-14T02:32:38.665Z","updated":"2021-07-14T02:32:38.665Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"About","date":"2021-10-13T11:27:05.806Z","updated":"2021-10-13T11:27:05.806Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"What I Believe Be the change you wish to see in the world. To be who you want to become, let go of who you think you are. Not everyone can become a great artist, but a great artist can come from anywhere. Don’t let schooling interfere with your education. Talk is cheap, show me the code. Make it your ambition to lead a quiet life. 卑鄙是卑鄙者的通行证，高尚是高尚者的墓志铭。"},{"title":"Tags","date":"2021-07-14T02:32:38.667Z","updated":"2021-07-14T02:32:38.666Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Lecture01(elective) 反向传播算法","slug":"leeml-bp","date":"2021-10-15T06:55:13.000Z","updated":"2021-10-15T07:01:39.603Z","comments":true,"path":"leeml-bp/","link":"","permalink":"http://example.com/leeml-bp/","excerpt":"","text":"（TODO） 参考资料","categories":[{"name":"04 组队学习","slug":"04-组队学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"第30期 李宏毅机器学习","slug":"04-组队学习/第30期-李宏毅机器学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"http://example.com/tags/machine-learning/"},{"name":"team leaning","slug":"team-leaning","permalink":"http://example.com/tags/team-leaning/"},{"name":"back propagation","slug":"back-propagation","permalink":"http://example.com/tags/back-propagation/"}]},{"title":"Chapter04 PyTorch基础实战——FashionMNIST图像分类","slug":"pytorch-chap04","date":"2021-10-14T10:02:55.000Z","updated":"2021-10-14T10:09:21.110Z","comments":true,"path":"pytorch-chap04/","link":"","permalink":"http://example.com/pytorch-chap04/","excerpt":"","text":"(TODO) 参考资料 Datawhale开源项目：深入浅出PyTorch https://github.com/datawhalechina/thorough-pytorch/ 李宏毅机器学习2021春-PyTorch Tutorial https://www.bilibili.com/video/BV1Wv411h7kN?p=5 动手学深度学习pytorch版 https://zh-v2.d2l.ai/chapter_preface/index.html PyTorch官方教程中文版 https://pytorch123.com/SecondSection/training_a_classifier/","categories":[{"name":"04 组队学习","slug":"04-组队学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"第30期 深入浅出PyTorch","slug":"04-组队学习/第30期-深入浅出PyTorch","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAPyTorch/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://example.com/tags/PyTorch/"},{"name":"team learning","slug":"team-learning","permalink":"http://example.com/tags/team-learning/"},{"name":"note","slug":"note","permalink":"http://example.com/tags/note/"}]},{"title":"Lecture01 机器学习和深度学习简介","slug":"leeml-lec01-introduction","date":"2021-10-12T05:40:12.000Z","updated":"2021-10-15T06:58:56.176Z","comments":true,"path":"leeml-lec01-introduction/","link":"","permalink":"http://example.com/leeml-lec01-introduction/","excerpt":"","text":"前言本文学习的课程是：李宏毅2021春机器学习课程 https://www.bilibili.com/video/BV1Wv411h7kN?p=2 课程大纲第一节 Introduction 作业 HW1: Regression第二节 Deep Learning 作业 HW2: Classification第三节 Self-Attention 作业 HW3: CNN HW4: Self-Attention第四节 Theory of ML第五节 Transformer 作业 HW5: Transformer第六节 Generative Model 作业 HW6: GAN第七节 Self-Supervised Learning 作业 HW7: BERT HW8: Autoencoder第八节 Explainable AI / Adversarial Attack 作业 HW9: Explainable AI HW10: Adversarial Attack第九节 Domain Adaptation/ RL 作业 HW11: Adaptation第十节 RL 作业 HW12: RL第十一节 Privacy v.s. ML第十二节 Quantum ML第十三节 Life-Long/Compression 作业 HW13: Life-Long HW14: Compression第十四节 Meta Learning 作业 HW15: Meta Learning Buffet Style Learning/自助式学习 Everyone can take this course. You decide how much you want to learn. You decide how deep you want to learn. 机器学习基本概念简介Machine Learning 约等于 Looking for Function 不同类型的函数Regression/回归：函数的输出是数值。 Classification/分类：给定选项（类别），输出正确的类别。 Structured Learning：create something with structure (image, document etc.) How to find a function: A caseCase: Predict YouTobe views 模型的训练： Step1. Function with Unknown Parameters基于领域知识写出一个带有位置参数的函数 y = b + wx1 w: weight, b: bias Step2. Define Loss from Training DataLoss is a function of parameters. L(b, w) Loss is how good a set of values is. Step3. Optimizationw*, b* = arg min L(w, b) Gradient Descent/梯度下降（1个参数，多参数同理） 随机选取一个初始点w0 计算w0对L的微分。learning rate/学习率。 Negative: increase w Positive: decrease w Update w iteratively/迭代更新w Training: Step1-3深度学习基本概念简介Regression/回归，梦开始的地方如何逼近复杂的曲线？——Sigmoid Function 用多段sigmoid逼近复杂函数： 给sigmoid加更多的feature（就是多段 y=b+wx组合起来）： sigmoid的矩阵运算： Loss/损失函数什么是损失函数： 一个关于参数的函数L(𝜃) 衡量这一组模型参数的效果 Optimization/优化器使用梯度下降更新参数： 一些概念： Batch: 数据包 Update：每次更新一次参数 Epoch：所有batch过一遍 Activation function/激活函数 sigmoid ReLU Deep LearningA fancy name: Deep Learning. It is just so deep, right? Go deeper and deeper: Think: Why deep network, not fat network? (选修)深度学习简介Deep Learning的发展历程 Deep Learning的三个步骤 Full Connect Feedforward Network/全连接前馈神经网络 Matrix Operation/矩阵运算参数矩阵： Output Layer Example Application手写识别案例： You need to decide the network structure to let a good function in your function set. Loss计算损失函数： 找一个最小化损失函数的函数，即找一组参数可以最小化损失函数： Gradient Descent如何寻找这样的参数？——梯度下降。 Backpropagation/反向传播如何计算梯度？——反向传播算法。 参考资料 李宏毅2021春机器学习课程 https://www.bilibili.com/video/BV1Wv411h7kN?p=2","categories":[{"name":"04 组队学习","slug":"04-组队学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"第30期 李宏毅机器学习","slug":"04-组队学习/第30期-李宏毅机器学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"组队学习","slug":"组队学习","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://example.com/tags/machine-learning/"},{"name":"regression","slug":"regression","permalink":"http://example.com/tags/regression/"}]},{"title":"Summary Transformer课程总结","slug":"nlp-transformer-summary","date":"2021-09-30T07:44:01.000Z","updated":"2021-10-15T07:00:47.781Z","comments":true,"path":"nlp-transformer-summary/","link":"","permalink":"http://example.com/nlp-transformer-summary/","excerpt":"","text":"我的背景第一次参加Datawhale组队学习课程，我的相关知识背景是： Transformer：0基础 PyTorch：0基础 NLP：0.1基础 Python：0基础 作为NLP情感分析的领航员和Transformers的学员，我将从课程内容和运营两方面写一下自己的感受和想法。 Transformer课程大纲 课程内容方面 课程内容对零基础入门的人是比较友好的。比如我是第一次学习Transformer，但是图解系列很容易理解。 但是每个task的内容难度差别过大。比如Task02的Transformer原论文代码标注 ，和Task05 transformers源码讲解。 每个task的工作量不平衡，有的特别多，有的相对少。 第四章可以任选一个任务应用，把更多时间留给学习如何使用huggingface的transformers。（就是那个官方课程） 课程运营方面 优秀队员和优秀队长评选标准需要统一 补卡规则需要统一 逐步完善课程体系 小程序，用起来不是很方便 （脑洞1）每次打卡之后马上进行作业评审和反馈（过于耗费助教精力） （脑洞2）给每个小组或个人安排一个期末大project，或者布置平时作业（过于耗费学员精力） （脑洞3）直接用一个课程管理系统进行管理（类似于canvas,雨课堂）(逐渐学院化) 参考资料清单（总）Transformer在网上有很多很多教程，其中公认的、普遍性的比较好的资料如下： 理论部分[1] (强推)李宏毅2021春机器学习课程 https://www.bilibili.com/video/BV1Wv411h7kN?from=search&amp;seid=17090062977285779802&amp;spm_id_from=333.337.0.0[2] 基于transformers的自然语言处理(NLP)入门（涵盖了图解系列、annotated transformer、huggingface） https://github.com/datawhalechina/learn-nlp-with-transformers[3] 图解transformer|The Illustrated Transformer http://jalammar.github.io/illustrated-transformer/[4] 图解seq2seq, attention|Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ 代码部分[5] The Annotated Transformer http://nlp.seas.harvard.edu//2018/04/03/attention.html[6] Huggingface/transformers https://github.com/huggingface/transformers/blob/master/README_zh-hans.md 论文部分Attention is all “we” need. 其他不错的博客或教程[7] 基于transformers的自然语言处理(NLP)入门–在线阅读 https://datawhalechina.github.io/learn-nlp-with-transformers/#/[8] 李宏毅2021春机器学习课程笔记——自注意力机制 https://www.cnblogs.com/sykline/p/14730088.html[9] 李宏毅2021春机器学习课程笔记——Transformer模型 https://www.cnblogs.com/sykline/p/14785552.html[10] 李宏毅机器学习学习笔记——自注意力机制 https://blog.csdn.net/p_memory/article/details/116271274[11] 车万翔-自然语言处理新范式：基于预训练的方法【讲座+PPT】 https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true[12] 苏剑林-《Attention is All You Need》浅读（简介+代码）https://spaces.ac.cn/archives/4765","categories":[{"name":"04 组队学习","slug":"04-组队学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"第29期 基于transformer的NLP","slug":"04-组队学习/第29期-基于transformer的NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"组队学习","slug":"组队学习","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"预训练模型","slug":"预训练模型","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"transfomer","slug":"transfomer","permalink":"http://example.com/tags/transfomer/"},{"name":"总结","slug":"总结","permalink":"http://example.com/tags/%E6%80%BB%E7%BB%93/"}]},{"title":"Chapter03 PyTorch的主要组成模块","slug":"pytorch-chap03","date":"2021-09-28T01:31:42.000Z","updated":"2021-10-14T10:04:01.506Z","comments":true,"path":"pytorch-chap03/","link":"","permalink":"http://example.com/pytorch-chap03/","excerpt":"","text":"第三章 PyTorch的主要组成模块完成深度学习的必要部分机器学习： 数据预处理（数据格式、数据转换、划分数据集） 选择模型，设定损失和优化函数，设置超参数 训练模型，拟合训练集 评估模型，在并在验证集/测试集上计算模型表现 深度学习的注意事项： 数据预处理（数据加载、批处理） 逐层搭建模型，组装不同模块 GPU的配置和操作 基本配置导入必须的包： 123456import osimport numpy as npimport torchimport torch.nn as nnfrom torch.utils.data import Dataset, DataLoaderimport torch.optim as optimizer 超参数设置： 123batch_size = 16 # batch sizelr = 1e-4 # 初始学习率max_epochs = 100 # 训练次数 GPU的设置： 12345# 方案一：使用os.environ，这种情况如果使用GPU不需要设置os.environ[&#x27;CUDA_VISIBLE_DEVICES&#x27;] = &#x27;0,1&#x27;# 方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可device = torch.device(&quot;cuda:1&quot; if torch.cuda.is_available() else &quot;cpu&quot;) 数据加载和处理PyTorch数据读入是通过Dataset+Dataloader的方式完成的，Dataset定义好数据的格式和数据变换形式，Dataloader用iterative的方式不断读入批次数据。 我们可以定义自己的Dataset类来实现灵活的数据读取，定义的类需要继承PyTorch自身的Dataset类。主要包含三个函数： __init__: 用于向类中传入外部参数，同时定义样本集 __getitem__: 用于逐个读取样本集合中的元素，可以进行一定的变换，并将返回训练/验证所需的数据 __len__: 用于返回数据集的样本数 batch_size：样本是按“批”读入的，batch_size就是每次读入的样本数 num_workers：有多少个进程用于读取数据 shuffle：是否将读入的数据打乱 drop_last：对于样本最后一部分没有达到批次数的样本，不再参与训练 下面是本部分代码在notebook中的运行情况。主要参考 PyTorch官方教程中文版 https://pytorch123.com/SecondSection/training_a_classifier/ 模型构建神经网络的构造PyTorch中神经网络构造一般是基于 Module 类的模型来完成的。Module 类是 nn 模块里提供的一个模型构造类，是所有神经⽹网络模块的基类，我们可以继承它来定义我们想要的模型。下面继承 Module 类构造多层感知机（MLP）。 12345678910111213141516import torchfrom torch import nnclass MLP(nn.Module): # 声明带有模型参数的层，这里声明了两个全连接层 def __init__(self, **kwargs): # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例例时还可以指定其他函数 super(MLP, self).__init__(**kwargs) self.hidden = nn.Linear(784, 256) self.act = nn.ReLU() self.output = nn.Linear(256,10) # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出 def forward(self, x): o = self.act(self.hidden(x)) return self.output(o) 我们可以实例化 MLP 类得到模型变量 net 。下⾯的代码初始化 net 并传入输⼊数据 X 做一次前向计算。其中， net(X) 会调用 MLP 继承⾃自 Module 类的 call 函数，这个函数将调⽤用 MLP 类定义的forward 函数来完成前向计算。 1234567891011121314151617181920212223242526272829303132333435&gt;&gt;&gt; import torch&gt;&gt;&gt; X = torch.rand(2, 784)&gt;&gt;&gt; Xtensor([[0.3277, 0.2204, 0.5239, ..., 0.4333, 0.1906, 0.1318], [0.9850, 0.2121, 0.8405, ..., 0.3796, 0.2717, 0.5553]])&gt;&gt;&gt; from torch import nn&gt;&gt;&gt; &gt;&gt;&gt; class MLP(nn.Module):... # 声明带有模型参数的层，这里声明了两个全连接层... def __init__(self, **kwargs):... # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例例时还可以指定其他函数... super(MLP, self).__init__(**kwargs)... self.hidden = nn.Linear(784, 256)... self.act = nn.ReLU()... self.output = nn.Linear(256,10)... ... # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出... def forward(self, x):... o = self.act(self.hidden(x))... return self.output(o)... &gt;&gt;&gt; net = MLP()&gt;&gt;&gt; netMLP( (hidden): Linear(in_features=784, out_features=256, bias=True) (act): ReLU() (output): Linear(in_features=256, out_features=10, bias=True))&gt;&gt;&gt; net(X)tensor([[ 0.1317, 0.0702, 0.1707, -0.0081, -0.2730, 0.2837, 0.0700, 0.1718, 0.0299, 0.2082], [ 0.1094, 0.0936, 0.2474, -0.0139, -0.1861, 0.1846, 0.1658, 0.2051, 0.2609, 0.2227]], grad_fn=&lt;AddmmBackward&gt;)&gt;&gt;&gt; 神经网络中常见的层不含模型参数的层下⾯构造的 MyLayer 类通过继承 Module 类自定义了一个将输入减掉均值后输出的层。这个层里不含模型参数。 1234567891011121314&gt;&gt;&gt; import torch&gt;&gt;&gt; from torch import nn&gt;&gt;&gt; &gt;&gt;&gt; class MyLayer(nn.Module):... def __init__(self, **kwargs):... super(MyLayer, self).__init__(**kwargs)... def forward(self, x):... return x - x.mean() ... &gt;&gt;&gt; layer = MyLayer() # 实例化该层&gt;&gt;&gt; layerMyLayer()&gt;&gt;&gt; layer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))tensor([-2., -1., 0., 1., 2.]) 含模型参数的层我们还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。 Parameter 类其实是 Tensor 的子类，如果一 个 Tensor 是 Parameter ，那么它会⾃动被添加到模型的参数列表里。所以在⾃定义含模型参数的层时，我们应该将参数定义成 Parameter ，除了直接定义成 Parameter 类外，还可以使⽤ ParameterList 和 ParameterDict 分别定义参数的列表和字典。 123456789101112131415161718192021class MyListDense(nn.Module): def __init__(self): super(MyListDense, self).__init__() self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)]) self.params.append(nn.Parameter(torch.randn(4, 1))) def forward(self, x): for i in range(len(self.params)): x = torch.mm(x, self.params[i]) return x &gt;&gt;&gt; net = MyListDense()&gt;&gt;&gt; print(net)MyListDense( (params): ParameterList( (0): Parameter containing: [torch.FloatTensor of size 4x4] (1): Parameter containing: [torch.FloatTensor of size 4x4] (2): Parameter containing: [torch.FloatTensor of size 4x4] (3): Parameter containing: [torch.FloatTensor of size 4x1] )) 123456789101112131415161718192021class MyDictDense(nn.Module): def __init__(self): super(MyDictDense, self).__init__() self.params = nn.ParameterDict(&#123; &#x27;linear1&#x27;: nn.Parameter(torch.randn(4, 4)), &#x27;linear2&#x27;: nn.Parameter(torch.randn(4, 1)) &#125;) self.params.update(&#123;&#x27;linear3&#x27;: nn.Parameter(torch.randn(4, 2))&#125;) # 新增 def forward(self, x, choice=&#x27;linear1&#x27;): return torch.mm(x, self.params[choice])&gt;&gt;&gt; net = MyDictDense()&gt;&gt;&gt; print(net)MyDictDense( (params): ParameterDict( (linear1): Parameter containing: [torch.FloatTensor of size 4x4] (linear2): Parameter containing: [torch.FloatTensor of size 4x1] (linear3): Parameter containing: [torch.FloatTensor of size 4x2] )) 下面给出常见的神经网络的一些层，比如卷积层、池化层，以及较为基础的AlexNet，LeNet等。 二维卷积层二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。 12345678910111213141516171819202122import torchfrom torch import nn# 卷积运算（二维互相关）def corr2d(X, K): h, w = K.shape X, K = X.float(), K.float() Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i, j] = (X[i: i + h, j: j + w] * K).sum() return Y# 二维卷积层class Conv2D(nn.Module): def __init__(self, kernel_size): super(Conv2D, self).__init__() self.weight = nn.Parameter(torch.randn(kernel_size)) self.bias = nn.Parameter(torch.randn(1)) def forward(self, x): return corr2d(x, self.weight) + self.bias 填充(padding)是指在输⼊入⾼高和宽的两侧填充元素(通常是0元素)。 在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下 的顺序，依次在输⼊数组上滑动。我们将每次滑动的行数和列数称为步幅(stride)。 （skip） 池化层池化层每次对输入数据的一个固定形状窗口(⼜称池化窗口)中的元素计算输出。不同于卷积层里计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也 分别叫做最大池化或平均池化。 12345678910111213141516171819&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; import torch&gt;&gt;&gt; from torch import nn&gt;&gt;&gt; &gt;&gt;&gt; def pool2d(X, pool_size, mode=&#x27;max&#x27;):... p_h, p_w = pool_size... Y = np.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))... for i in range(Y.shape[0]):... for j in range(Y.shape[1]):... if mode == &#x27;max&#x27;:... Y[i, j] = X[i: i + p_h, j: j + p_w].max()... elif mode == &#x27;avg&#x27;:... Y[i, j] = X[i: i + p_h, j: j + p_w].mean()... return Y... &gt;&gt;&gt; X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])&gt;&gt;&gt; pool2d(X, (2, 2))array([[4., 5.], [7., 8.]]) 模型示例：LeNet（待补充） 模型示例：AlexNet（待补充） 损失函数一个好的训练离不开优质的负反馈，这里的损失函数就是模型的负反馈。 这里将列出PyTorch中常用的损失函数（一般通过torch.nn调用），并详细介绍每个损失函数的功能介绍、数学公式和调用代码。 二分类交叉熵损失函数torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;) 功能：计算二分类任务时的交叉熵（Cross Entropy）函数。在二分类中，label是{0,1}。对于进入交叉熵函数的input为概率分布的形式。一般来说，input为sigmoid激活层的输出，或者softmax的输出。 主要参数： weight:每个类别的loss设置权值 size_average:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和. reduce:数据类型为bool，为True时，loss的返回是标量。 其他损失函数交叉熵损失函数 L1损失函数 MSE损失函数 平滑L1 (Smooth L1)损失函数 目标泊松分布的负对数似然损失 KL散度 优化器什么是优化器深度学习的目标是通过不断改变网络参数，使得参数能够对输入做各种非线性变换拟合输出，本质上就是一个函数去寻找最优解，只不过这个最优解使一个矩阵。那么我们如何计算出来这么多的系数，有以下两种方法： 第一种是最直接的暴力穷举一遍参数，这种方法的实施可能性基本为0，堪比愚公移山plus的难度。 为了使求解参数过程更加快，人们提出了第二种办法，即就是是BP+优化器逼近求解。 因此，优化器就是根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值，使得模型输出更加接近真实标签。 PyTorch提供的优化器Pytorch很人性化的给我们提供了一个优化器的库torch.optim，在这里面给我们提供了十种优化器。 torch.optim.ASGD torch.optim.Adadelta torch.optim.Adagrad torch.optim.Adam torch.optim.AdamW torch.optim.Adamax torch.optim.LBFGS torch.optim.RMSprop torch.optim.Rprop torch.optim.SGD torch.optim.SparseAdam 训练与评估完成了上述设定后就可以加载数据开始训练模型了。首先应该设置模型的状态：如果是训练状态，那么模型的参数应该支持反向传播的修改；如果是验证/测试状态，则不应该修改模型参数。 12model.train() # 训练状态model.eval() # 验证/测试状态 训练过程： 12345678910111213def train(epoch): model.train() train_loss = 0 for data, label in train_loader: # 此时要用for循环读取DataLoader中的全部数据。 data, label = data.cuda(), label.cuda() # 之后将数据放到GPU上用于后续计算，此处以.cuda()为例 optimizer.zero_grad() # 开始用当前批次数据做训练时，应当先将优化器的梯度置零 output = model(data) # 之后将data送入模型中训练 loss = criterion(label, output) # 根据预先定义的criterion计算损失函数 loss.backward() # 将loss反向传播回网络 optimizer.step() # 使用优化器更新模型参数 train_loss += loss.item()*data.size(0) train_loss = train_loss/len(train_loader.dataset) print(&#x27;Epoch: &#123;&#125; \\tTraining Loss: &#123;:.6f&#125;&#x27;.format(epoch, train_loss)) 验证/测试的流程基本与训练过程一致，不同点在于： 需要预先设置torch.no_grad，以及将model调至eval模式 不需要将优化器的梯度置零 不需要将loss反向回传到网络 不需要更新optimizer 验证/测试过程： 12345678910111213def val(epoch): model.eval() val_loss = 0 with torch.no_grad(): for data, label in val_loader: data, label = data.cuda(), label.cuda() output = model(data) preds = torch.argmax(output, 1) loss = criterion(output, label) val_loss += loss.item()*data.size(0) running_accu += torch.sum(preds == label.data) val_loss = val_loss/len(val_loader.dataset) print(&#x27;Epoch: &#123;&#125; \\tTraining Loss: &#123;:.6f&#125;&#x27;.format(epoch, val_loss)) 可视化在PyTorch深度学习中，可视化是一个可选项，指的是某些任务在训练完成后，需要对一些必要的内容进行可视化，比如分类的ROC曲线，卷积网络中的卷积核，以及训练/验证过程的损失函数曲线等等。 参考资料 Datawhale开源项目：深入浅出PyTorch https://github.com/datawhalechina/thorough-pytorch/ 李宏毅机器学习2021春-PyTorch Tutorial https://www.bilibili.com/video/BV1Wv411h7kN?p=5 动手学深度学习pytorch版 https://zh-v2.d2l.ai/chapter_preface/index.html PyTorch官方教程中文版 https://pytorch123.com/SecondSection/training_a_classifier/","categories":[{"name":"04 组队学习","slug":"04-组队学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"第30期 深入浅出PyTorch","slug":"04-组队学习/第30期-深入浅出PyTorch","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAPyTorch/"}],"tags":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"组队学习","slug":"组队学习","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://example.com/tags/PyTorch/"}]},{"title":"Task07 使用Transformers解决文本分类任务","slug":"nlp-transformer-task07","date":"2021-09-25T08:57:45.000Z","updated":"2021-10-02T09:21:50.190Z","comments":true,"path":"nlp-transformer-task07/","link":"","permalink":"http://example.com/nlp-transformer-task07/","excerpt":"","text":"该部分的内容翻译自🤗HuggingFace/notebooks https://github.com/huggingface/notebooks/tree/master/examples中文翻译：Datawhale/learn-nlp-with-transformers/4.1-文本分类 Datawhale/learn-nlp-with-transformers/4.1-文本分类 微调预训练模型进行文本分类我们将使用 🤗 Transformers代码库中的模型来解决文本分类任务，任务来源于GLUE Benchmark.GLUE榜单包含了9个句子级别的分类任务，分别是： CoLA (Corpus of Linguistic Acceptability) 鉴别一个句子是否语法正确. MNLI (Multi-Genre Natural Language Inference) 给定一个假设，判断另一个句子与该假设的关系：entails, contradicts 或者 unrelated。 MRPC (Microsoft Research Paraphrase Corpus) 判断两个句子是否互为paraphrases. QNLI (Question-answering Natural Language Inference) 判断第2句是否包含第1句问题的答案。 QQP (Quora Question Pairs2) 判断两个问句是否语义相同。 RTE (Recognizing Textual Entailment)判断一个句子是否与假设成entail关系。 SST-2 (Stanford Sentiment Treebank) 判断一个句子的情感正负向. STS-B (Semantic Textual Similarity Benchmark) 判断两个句子的相似性（分数为1-5分）。 WNLI (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. 对于以上任务，我们将展示如何使用简单的Dataset库加载数据集，同时使用transformer中的Trainer接口对预训练模型进行微调。 1GLUE_TASKS = [&quot;cola&quot;, &quot;mnli&quot;, &quot;mnli-mm&quot;, &quot;mrpc&quot;, &quot;qnli&quot;, &quot;qqp&quot;, &quot;rte&quot;, &quot;sst2&quot;, &quot;stsb&quot;, &quot;wnli&quot;] This notebook is built to run on any of the tasks in the list above, with any model checkpoint from the Model Hub as long as that model has a version with a classification head. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:本notebook理论上可以使用各种各样的transformer模型（模型面板），解决任何文本分类分类任务。如果您所处理的任务有所不同，大概率只需要很小的改动便可以使用本notebook进行处理。同时，您应该根据您的GPU显存来调整微调训练所需要的btach size大小，避免显存溢出。 123task = &quot;cola&quot;model_checkpoint = &quot;distilbert-base-uncased&quot;batch_size = 16 加载数据集We will use the 🤗 Datasets library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions load_dataset and load_metric.我们将会使用🤗 Datasets库来加载数据和对应的评测方式。数据加载和评测方式加载只需要简单使用load_dataset和load_metric即可。 1from datasets import load_dataset, load_metric Apart from mnli-mm being a special code, we can directly pass our task name to those functions. load_dataset will cache the dataset to avoid downloading it again the next time you run this cell.除了mnli-mm以外，其他任务都可以直接通过任务名字进行加载。数据加载之后会自动缓存。 123actual_task = &quot;mnli&quot; if task == &quot;mnli-mm&quot; else taskdataset = load_dataset(&quot;glue&quot;, actual_task)metric = load_metric(&#x27;glue&#x27;, actual_task) 上节讲过，这里，最好手动下载glue.py和gule_metric.py，不下载到本地的话，容易出现连接错误。 The dataset object itself is DatasetDict, which contains one key for the training, validation and test set (with more keys for the mismatched validation and test set in the special case of mnli).这个datasets对象本身是一种DatasetDict数据结构.对于训练集、验证集和测试集，只需要使用对应的key（train，validation，test）即可得到相应的数据。 123456789101112131415&gt;&gt;&gt; datasetDatasetDict(&#123; train: Dataset(&#123; features: [&#x27;sentence&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;], num_rows: 8551 &#125;) validation: Dataset(&#123; features: [&#x27;sentence&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;], num_rows: 1043 &#125;) test: Dataset(&#123; features: [&#x27;sentence&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;], num_rows: 1063 &#125;)&#125;) 1234&gt;&gt;&gt; dataset[&quot;train&quot;][0]&#123;&#x27;sentence&#x27;: &quot;Our friends won&#x27;t buy this analysis, let alone the next one we propose.&quot;,&#x27;label&#x27;: 1, &#x27;idx&#x27;: 0&#125; To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset.为了能够进一步理解数据长什么样子，下面的函数将从数据集里随机选择几个例子进行展示。 1234567891011121314151617181920import datasetsimport randomimport pandas as pdfrom IPython.display import display, HTMLdef show_random_elements(dataset, num_examples=10): assert num_examples &lt;= len(dataset), &quot;Can&#x27;t pick more elements than there are in the dataset.&quot; picks = [] for _ in range(num_examples): pick = random.randint(0, len(dataset)-1) while pick in picks: pick = random.randint(0, len(dataset)-1) picks.append(pick) df = pd.DataFrame(dataset[picks]) for column, typ in dataset.features.items(): if isinstance(typ, datasets.ClassLabel): df[column] = df[column].transform(lambda i: typ.names[i]) display(HTML(df.to_html())) 1show_random_elements(dataset[&quot;train&quot;]) The metric is an instance of datasets.Metric: 1pass You can call its compute method with your predictions and labels directly and it will return a dictionary with the metric(s) value:直接调用metric的compute方法，传入labels和predictions即可得到metric的值： 12345import numpy as npfake_preds = np.random.randint(0, 2, size=(64,))fake_labels = np.random.randint(0, 2, size=(64,))metric.compute(predictions=fake_preds, references=fake_labels) Note that load_metric has loaded the proper metric associated to your task, which is:每一个文本分类任务所对应的metic有所不同，具体如下: for CoLA: Matthews Correlation Coefficient for MNLI (matched or mismatched): Accuracy for MRPC: Accuracy and F1 score for QNLI: Accuracy for QQP: Accuracy and F1 score for RTE: Accuracy for SST-2: Accuracy for STS-B: Pearson Correlation Coefficient and Spearman’s_Rank_Correlation_Coefficient for WNLI: Accuracy so the metric object only computes the one(s) needed for your task. 数据预处理Before we can feed those texts to our model, we need to preprocess them. This is done by a 🤗 Transformers Tokenizer which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.在将数据喂入模型之前，我们需要对数据进行预处理。预处理的工具叫Tokenizer。Tokenizer首先对输入进行tokenize，然后将tokens转化为预模型中需要对应的token ID，再转化为模型需要的输入格式。 To do all of this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which will ensure: we get a tokenizer that corresponds to the model architecture we want to use, we download the vocabulary used when pretraining this specific checkpoint. 为了达到数据预处理的目的，我们使用AutoTokenizer.from_pretrained方法实例化我们的tokenizer，这样可以确保： 我们得到一个与预训练模型一一对应的tokenizer。 使用指定的模型checkpoint对应的tokenizer的时候，我们也下载了模型需要的词表库vocabulary，准确来说是tokens vocabulary。 That vocabulary will be cached, so it’s not downloaded again the next time we run the cell.这个被下载的tokens vocabulary会被缓存起来，从而再次使用的时候不会重新下载。 123from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True) You can directly call this tokenizer on one sentence or a pair of sentences: 1tokenizer(&quot;Hello, this one sentence!&quot;, &quot;And this sentence goes with it.&quot;) 输出为：pass To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names: 123456789101112task_to_keys = &#123; &quot;cola&quot;: (&quot;sentence&quot;, None), &quot;mnli&quot;: (&quot;premise&quot;, &quot;hypothesis&quot;), &quot;mnli-mm&quot;: (&quot;premise&quot;, &quot;hypothesis&quot;), &quot;mrpc&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;), &quot;qnli&quot;: (&quot;question&quot;, &quot;sentence&quot;), &quot;qqp&quot;: (&quot;question1&quot;, &quot;question2&quot;), &quot;rte&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;), &quot;sst2&quot;: (&quot;sentence&quot;, None), &quot;stsb&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;), &quot;wnli&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;),&#125; We can double check it does work on our current dataset: 123456sentence1_key, sentence2_key = task_to_keys[task]if sentence2_key is None: print(f&quot;Sentence: &#123;dataset[&#x27;train&#x27;][0][sentence1_key]&#125;&quot;)else: print(f&quot;Sentence 1: &#123;dataset[&#x27;train&#x27;][0][sentence1_key]&#125;&quot;) print(f&quot;Sentence 2: &#123;dataset[&#x27;train&#x27;][0][sentence2_key]&#125;&quot;) 输出为： 1Sentence: Our friends won&#x27;t buy this analysis, let alone the next one we propose. We can them write the function that will preprocess our samples. We just feed them to the tokenizer with the argument truncation=True. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. 1234def preprocess_function(examples): if sentence2_key is None: return tokenizer(examples[sentence1_key], truncation=True) return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True) 12&gt;&gt;&gt; preprocess_function(dataset[&#x27;train&#x27;][:5])&#123;&#x27;input_ids&#x27;: [[101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 1998, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 2030, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 1996, 2062, 2057, 2817, 16025, 1010, 1996, 13675, 16103, 2121, 2027, 2131, 1012, 102], [101, 2154, 2011, 2154, 1996, 8866, 2024, 2893, 14163, 8024, 3771, 1012, 102]], &#x27;attention_mask&#x27;: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]&#125; To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the map method of our dataset object we created earlier. This will apply the function on all the elements of all the splits in dataset, so our training, validation and testing data will be preprocessed in one single command.接下来对数据集datasets里面的所有样本进行预处理，处理的方式是使用map函数，将预处理函数prepare_train_features应用到（map)所有样本上。 1encoded_dataset = dataset.map(preprocess_function, batched=True) 微调模型Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about sentence classification, we use the AutoModelForSequenceClassification class. Like with the tokenizer, the from_pretrained method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which is always 2, except for STS-B which is a regression problem and MNLI where we have 3 labels):既然数据已经准备好了，现在我们需要下载并加载我们的预训练模型，然后微调预训练模型。既然我们是做seq2seq任务，那么我们需要一个能解决这个任务的模型类。我们使用AutoModelForSequenceClassification 这个类。和tokenizer相似，from_pretrained方法同样可以帮助我们下载并加载模型，同时也会对模型进行缓存，就不会重复下载模型啦。 1234from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainernum_labels = 3 if task.startswith(&quot;mnli&quot;) else 1 if task==&quot;stsb&quot; else 2model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels) 输出为： 12345Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: [&#x27;vocab_transform.bias&#x27;, &#x27;vocab_projector.weight&#x27;, &#x27;vocab_layer_norm.bias&#x27;, &#x27;vocab_layer_norm.weight&#x27;, &#x27;vocab_projector.bias&#x27;, &#x27;vocab_transform.weight&#x27;]- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: [&#x27;classifier.weight&#x27;, &#x27;classifier.bias&#x27;, &#x27;pre_classifier.bias&#x27;, &#x27;pre_classifier.weight&#x27;]You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. The warning is telling us we are throwing away some weights (the vocab_transform and vocab_layer_norm layers) and randomly initializing some other (the pre_classifier and classifier layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don’t have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.由于我们微调的任务是文本分类任务，而我们加载的是预训练的语言模型，所以会提示我们加载模型的时候扔掉了一些不匹配的神经网络参数（比如：预训练语言模型的神经网络head被扔掉了，同时随机初始化了文本分类的神经网络head）。 To instantiate a Trainer, we will need to define two more things. The most important is the TrainingArguments, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:为了能够得到一个Trainer训练工具，我们还需要3个要素，其中最重要的是训练的设定/参数 TrainingArguments。这个训练设定包含了能够定义训练过程的所有属性。 1234567891011121314151617metric_name = &quot;pearson&quot; if task == &quot;stsb&quot; else &quot;matthews_correlation&quot; if task == &quot;cola&quot; else &quot;accuracy&quot;model_name = model_checkpoint.split(&quot;/&quot;)[-1]args = TrainingArguments( &quot;test-glue&quot;, evaluation_strategy = &quot;epoch&quot;, save_strategy = &quot;epoch&quot;, learning_rate=2e-5, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, num_train_epochs=5, weight_decay=0.01, load_best_model_at_end=True, metric_for_best_model=metric_name, push_to_hub=False, push_to_hub_model_id=f&quot;&#123;model_name&#125;-finetuned-&#123;task&#125;&quot;,) Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the batch_size defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay. Since the best model might not be the one at the end of training, we ask the Trainer to load the best model it saved (according to metric_name) at the end of training.上面evaluation_strategy = “epoch”参数告诉训练代码：我们每个epcoh会做一次验证评估。上面batch_size在这个notebook之前定义好了。 The last two arguments are to setup everything so we can push the model to the Hub at the end of training. Remove the two of them if you didn’t follow the installation steps at the top of the notebook, otherwise you can change the value of push_to_hub_model_id to something you would prefer.(后面需要连接到hub客户端，太麻烦，所以先设为False) The last thing to define for our Trainer is how to compute the metrics from the predictions. We need to define a function for this, which will just use the metric we loaded earlier, the only preprocessing we have to do is to take the argmax of our predicted logits (our just squeeze the last axis in the case of STS-B):最后，由于不同的任务需要不同的评测指标，我们定一个函数来根据任务名字得到评价方法: 1234567def compute_metrics(eval_pred): predictions, labels = eval_pred if task != &quot;stsb&quot;: predictions = np.argmax(predictions, axis=1) else: predictions = predictions[:, 0] return metric.compute(predictions=predictions, references=labels) Then we just need to pass all of this along with our datasets to the Trainer: 123456789validation_key = &quot;validation_mismatched&quot; if task == &quot;mnli-mm&quot; else &quot;validation_matched&quot; if task == &quot;mnli&quot; else &quot;validation&quot;trainer = Trainer( model, args, train_dataset=encoded_dataset[&quot;train&quot;], eval_dataset=encoded_dataset[validation_key], tokenizer=tokenizer, compute_metrics=compute_metrics) BUG:ValueError: You must login to the Hugging Face hub on this computer by typing transformers-cli login and entering your credentials to use use_auth_token=True. Alternatively, you can pass your own token as the use_auth_token argument.把args里面的该项参数改为False push_to_hub=False,。 We can now finetune our model by just calling the train method: 1trainer.train() 输出为： 1pass We can check with the evaluate method that our Trainer did reload the best model properly (if it was not the last one): 1trainer.evaluate() 输出为： 1pass 超参搜索The Trainer supports hyperparameter search using optuna or Ray Tune. 12pip install optunapip install ray[tune] During hyperparameter search, the Trainer will run several trainings, so it needs to have the model defined via a function (so it can be reinitialized at each new run) instead of just having it passed. We jsut use the same function as before:超参搜索时，Trainer将会返回多个训练好的模型，所以需要传入一个定义好的模型从而让Trainer可以不断重新初始化该传入的模型： 12def model_init(): return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels) And we can instantiate our Trainer like before: 12345678trainer = Trainer( model_init=model_init, args=args, train_dataset=encoded_dataset[&quot;train&quot;], eval_dataset=encoded_dataset[validation_key], tokenizer=tokenizer, compute_metrics=compute_metrics) The method we call this time is hyperparameter_search. Note that it can take a long time to run on the full dataset for some of the tasks. You can try to find some good hyperparameter on a portion of the training dataset by replacing the train_dataset line above by:train_dataset = encoded_dataset[&quot;train&quot;].shard(index=1, num_shards=10) for 1/10th of the dataset. Then you can run a full training on the best hyperparameters picked by the search.调用方法hyperparameter_search。注意，这个过程可能很久，我们可以先用部分数据集进行超参搜索，再进行全量训练。 比如使用1/10的数据进行搜索： 1best_run = trainer.hyperparameter_search(n_trials=10, direction=&quot;maximize&quot;) The hyperparameter_search method returns a BestRun objects, which contains the value of the objective maximized (by default the sum of all metrics) and the hyperparameters it used for that run.hyperparameter_search会返回效果最好的模型相关的参数： 12&gt;&gt;&gt; best_run To reproduce the best training, just set the hyperparameters in your TrainingArgument before creating a Trainer:将Trainner设置为搜索到的最好参数，进行训练： 1234for n, v in best_run.hyperparameters.items(): setattr(trainer.args, n, v)trainer.train() 参考资料 HuggingFace/transfomers/BERT https://huggingface.co/transformers/model_doc/bert.html# 基于transformers的自然语言处理(NLP)入门–在线阅读 https://datawhalechina.github.io/learn-nlp-with-transformers/#/","categories":[{"name":"04 组队学习","slug":"04-组队学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"第29期 基于transformer的NLP","slug":"04-组队学习/第29期-基于transformer的NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"组队学习","slug":"组队学习","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"预训练模型","slug":"预训练模型","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"BERT","slug":"BERT","permalink":"http://example.com/tags/BERT/"},{"name":"transfomer","slug":"transfomer","permalink":"http://example.com/tags/transfomer/"},{"name":"文本分类","slug":"文本分类","permalink":"http://example.com/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"}]},{"title":"Task06 BERT应用、训练和优化","slug":"nlp-transformer-task06","date":"2021-09-24T07:18:42.000Z","updated":"2021-10-02T09:21:45.032Z","comments":true,"path":"nlp-transformer-task06/","link":"","permalink":"http://example.com/nlp-transformer-task06/","excerpt":"","text":"该部分的内容翻译自🤗HuggingFace官网教程第1部分（1-4章），见 https://huggingface.co/course/chapter1。该系列教程由3大部分共12章组成（如图），其中第1部分介绍transformers库的主要概念、模型的工作原理和使用方法、怎样在特定数据集上微调等内容。 环境搭建简单的说，有两种可以跑模型代码的方式： Google Colab 本地虚拟环境 pip install transformers 详见 https://huggingface.co/course/chapter0?fw=pt Transformer模型概述Transformers, 可以做什么？目前可用的一些pipeline是： feature-extraction 获取文本的向量表示 fill-mask 完形填空 ner (named entity recognition) 命名实体识别 question-answering 问答 sentiment-analysis 情感分析 summarization 摘要生成 text-generation 文本生成 translation 翻译 zero-shot-classification 零样本分类 pipeline: 直译管道/流水线，可以理解为流程。 Transformers, 如何工作？Transformer简史Transformer 架构于 2017 年 6 月推出。原始研究的重点是翻译任务。随后推出了几个有影响力的模型，包括： 2018 年 6 月：GPT，第一个预训练的 Transformer 模型，用于各种 NLP 任务的微调并获得最先进的结果 2018 年 10 月：BERT，另一个大型预训练模型，该模型旨在生成更好的句子摘要 2019 年 2 月：GPT-2，GPT 的改进（和更大）版本 2019 年 10 月：DistilBERT，BERT 的蒸馏版本，速度提高 60%，内存减轻 40%，但仍保留 BERT 97% 的性能 2019 年 10 月：BART 和 T5，两个使用与原始 Transformer 模型相同架构的大型预训练模型（第一个这样做） 2020 年 5 月，GPT-3，GPT-2 的更大版本，无需微调即可在各种任务上表现良好（称为零样本学习zero-shot learning） 大体上，它们可以分为三类： GPT类（又称为自回归 Transformer 模型）：只使用transformer-decoder部分 BERT类（又称为自编码 Transformer 模型）：只使用transformer-encoder部分 BART/T5类（又称为序列到序列 Transformer 模型）：使用Transformer-encoder-decoder部分 它们的分类、具体模型、主要应用任务如下： 其他需要知道的： Transformers是语言模型 Transformers是大模型 Transformers的应用通过预训练和微调两个过程 名词解释：Architecture和CheckpointsArchitecture/架构：定义了模型的基本结构和基本运算。Checkpoints/检查点：模型的某个训练状态，加载此checkpoint会加载此时的权重。训练时可以选择自动保存checkpoint。模型在训练时可以设置自动保存于某个时间点（比如模型训练了一轮epoch，更新了参数，将这个状态的模型保存下来，为一个checkpoint。） 所以每个checkpoint对应模型的一个状态，一组权重。 使用Transformers3个处理步骤将一些文本传递到pipeline时涉及3个主要步骤： 文本被预处理为模型可以理解的格式。 预处理后的输入传递给模型。 模型的预测结果被后处理为人类可以理解的格式。 Pipeline将3个步骤组合在一起：预处理/Tokenizer、通过模型传递输入/Model和后处理/Post-Processing： Tokenizer/预处理Tokenizer的作用： 将输入拆分为称为token的单词、子词/subword或符号/symbols（如标点符号） 将每个token映射到一个整数 添加可能对模型有用的其他输入 Going Through Models/穿过模型模型实例化1234from transformers import AutoModelcheckpoint = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;model = AutoModel.from_pretrained(checkpoint) 在这段代码中，我们下载了在pipeline中使用的相同检查点（实际上已经缓存）并将模型实例化。 模型的输出：高维向量模型的输出向量通常有三个维度： Batch size: 一次处理的序列数 Sequence length: 序列向量的长度 Hidden size: 每个模型输入处理后的向量维度（hidden state vector） Model Heads：为了处理不同的任务Model heads:将隐藏状态的高维向量作为输入，并将它们投影到不同的维度上。它们通常由一个或几个线性层组成。如上图所示，紫色代表向量，粉色代表模组，Embeddings+layers表示Transformer的架构，经过这层架构后的输出送入Model Head进行处理，从而应用到不同的下游任务。🤗 Transformers 中有许多不同的Head架构可用，每一种架构都围绕着处理特定任务而设计。 下面列举了部分Model heads： *Model (retrieve the hidden states) *ForCausalLM *ForMaskedLM *ForMultipleChoice *ForQuestionAnswering *ForSequenceClassification *ForTokenClassification and others 🤗 Post-processing/后处理从模型中获得的作为输出的值本身并不一定有意义。要转换为概率，它们需要经过一个 SoftMax 层。 微调一个预训练模型数据处理在本节中，我们将使用MRPC（Microsoft Research Praphrase Corpus）数据集作为示例。该DataSet由5,801对句子组成，标签指示它们是否是同义句（即两个句子是否表示相同的意思）。 我们选择它是因为它是一个小型数据集，因此可以轻松训练。 从Hub上加载数据集Hub不仅包含模型，还含有多种语言的datasets。例如，MRPC数据集是构成 GLUE benchmark的 10 个数据集之一。GLUE（General Language Understanding Evaluation）是一个多任务的自然语言理解基准和分析平台。GLUE包含九项NLU任务，语言均为英语。GLUE九项任务涉及到自然语言推断、文本蕴含、情感分析、语义相似等多个任务。像BERT、XLNet、RoBERTa、ERINE、T5等知名模型都会在此基准上进行测试。 🤗 Datasets库提供了一个非常简单的命令来下载和缓存Hub上的dataset。 我们可以像这样下载 MRPC 数据集： 1234&gt;&gt;&gt; from datasets import load_dataset&gt;&gt;&gt; raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)&gt;&gt;&gt; raw_datasets 输出如下： 1234567891011121314DatasetDict(&#123; train: Dataset(&#123; features: [&#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;], num_rows: 3668 &#125;) validation: Dataset(&#123; features: [&#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;], num_rows: 408 &#125;) test: Dataset(&#123; features: [&#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;], num_rows: 1725 &#125;)&#125;) 这样就得到一个DatasetDict对象，包含训练集、验证集和测试集，训练集中有3,668 个句子对，验证集中有408对，测试集中有1,725 对。每个句子对包含四个字段：’sentence1’, ‘sentence2’, ‘label’和 ‘idx’。 我们可以通过索引访问raw_datasets 的句子对： 12&gt;&gt;&gt; raw_train_dataset = raw_datasets[&quot;train&quot;]&gt;&gt;&gt; raw_train_dataset[0] 输出如下： 1234&#123;&#x27;sentence1&#x27;: &#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;, &#x27;sentence2&#x27;: &#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;, &#x27;label&#x27;: 1, &#x27;idx&#x27;: 0&#125; 我们可以通过features获得数据集的字段类型： 1&gt;&gt;&gt; raw_train_dataset.features 输出如下： 1234&#123;&#x27;sentence1&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;sentence2&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;not_equivalent&#x27;, &#x27;equivalent&#x27;], names_file=None, id=None), &#x27;idx&#x27;: Value(dtype=&#x27;int32&#x27;, id=None)&#125; TIPS： 没有数据集的话首先安装一下：pip install datasets 这里很容易出现连接错误，解决方法如下：https://blog.csdn.net/qq_20849045/article/details/117462846?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link 数据集预处理通过数据集预处理，我们将文本转换成模型能理解的向量。这个过程通过Tokenizer实现： 123456&gt;&gt;&gt; from transformers import AutoTokenizer&gt;&gt;&gt; checkpoint = &quot;bert-base-uncased&quot;&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(checkpoint)&gt;&gt;&gt; tokenized_sentences_1 = tokenizer(raw_datasets[&quot;train&quot;][&quot;sentence1&quot;])&gt;&gt;&gt; tokenized_sentences_2 = tokenizer(raw_datasets[&quot;train&quot;][&quot;sentence2&quot;]) （TODO） 使用Trainer API微调一个模型训练评估函数补充部分为什么4中用Trainer来微调模型？Training Arguments主要参数不同模型的加载方式Dynamic Padding——动态填充技术参考资料 基于transformers的自然语言处理(NLP)入门–在线阅读 https://datawhalechina.github.io/learn-nlp-with-transformers/#/ Huggingface官方教程 https://huggingface.co/course/chapter1","categories":[{"name":"04 组队学习","slug":"04-组队学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"第29期 基于transformer的NLP","slug":"04-组队学习/第29期-基于transformer的NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"组队学习","slug":"组队学习","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"预训练模型","slug":"预训练模型","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"BERT","slug":"BERT","permalink":"http://example.com/tags/BERT/"},{"name":"transfomer","slug":"transfomer","permalink":"http://example.com/tags/transfomer/"}]},{"title":"Task05 编写BERT模型","slug":"nlp-transformer-task05","date":"2021-09-20T19:01:35.000Z","updated":"2021-10-02T09:21:40.680Z","comments":true,"path":"nlp-transformer-task05/","link":"","permalink":"http://example.com/nlp-transformer-task05/","excerpt":"","text":"Overview本部分是BERT源码的解读，来自HuggingFace/transfomers/BERT[1]。 如图所示，代码结构和作用如下： BertTokenizer 预处理和切词 BertModel BertEmbeddings 词嵌入 BertEncoder BertAttention 注意力机制 BertIntermediate 全连接和激活函数 BertOutput 全连接、残差链接和正则化 BertPooler 取出[CLS]对应的向量，然后通过全连接层和激活函数后输出结果 BERT的实现BertConfig1234567classtransformers.BertConfig(vocab_size=30522, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act=&#x27;gelu&#x27;, hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1 , max_position_embeddings=512, type_vocab_size=2, initializer_range=0.02, layer_norm_eps=1e-12, pad_token_id=0, gradient_checkpointing=False, position_embedding_type=&#x27;absolute&#x27;, use_cache=True, classifier_dropout=None, **kwargs) 这是存储BertModel（Torch.nn.Module的子类）或TFBertModel（tf.keras.Model的子类）配置的配置类。它用于根据指定的参数来实例化BERT模型，定义模型架构。 配置对象从PretrainedConfig继承，可用于控制模型输出。 参数： vocab_size: BERT模型的词汇量，定义了能被inputs_ids表示的token数量。 hidden_size: BertTokenizerBertModelBERT的应用BertForPreTrainingBertForNextSentencePredictionBertForSequenceClassificationBertForMultipleChoiceBertForTokenClassificationBertForQuestionAnsweringBERT的训练和优化Pre-TrainingFine-Tuning参考资料 HuggingFace/transfomers/BERT https://huggingface.co/transformers/model_doc/bert.html# 基于transformers的自然语言处理(NLP)入门–在线阅读 https://datawhalechina.github.io/learn-nlp-with-transformers/#/","categories":[{"name":"04 组队学习","slug":"04-组队学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"第29期 基于transformer的NLP","slug":"04-组队学习/第29期-基于transformer的NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"组队学习","slug":"组队学习","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"预训练模型","slug":"预训练模型","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"BERT","slug":"BERT","permalink":"http://example.com/tags/BERT/"},{"name":"transfomer","slug":"transfomer","permalink":"http://example.com/tags/transfomer/"}]},{"title":"Task04 学习GPT","slug":"nlp-transformer-task04","date":"2021-09-19T12:02:17.000Z","updated":"2021-10-02T09:21:36.582Z","comments":true,"path":"nlp-transformer-task04/","link":"","permalink":"http://example.com/nlp-transformer-task04/","excerpt":"","text":"从语言模型说起自编码语言模型（auto-encoder）自编码语言模型通过随机Mask输入的部分单词，然后预训练的目标是预测被Mask的单词，不仅可以融入上文信息，还可以自然的融入下文信息。ex. BERT. 优点：自然地融入双向语言模型，同时看到被预测单词的上文和下文 缺点：训练和预测不一致。训练的时候输入引入了[Mask]标记，但是在预测阶段往往没有这个[Mask]标记，导致预训练阶段和Fine-tuning阶段不一致。 自回归语言模型（auto-regressive）语言模型根据输入句子的一部分文本来预测下一个词。ex. GPT-2 优点：对于生成类的NLP任务，比如文本摘要，机器翻译等，从左向右的生成内容，天然和自回归语言模型契合。 缺点：由于一般是从左到右（当然也可能从右到左），所以只能利用上文或者下文的信息，不能同时利用上文和下文的信息。 Transformer, BERT, GPT-2的关系Transformer的Encoder进化成了BERT，Decoder进化成了GPT2。 如果要使用Transformer来解决语言模型任务，并不需要完整的Encoder部分和Decoder部分，于是在原始Transformer之后的许多研究工作中，人们尝试只使用Transformer Encoder或者Decoder进行预训练。比如BERT只使用了Encoder部分进行masked language model（自编码）训练，GPT-2便是只使用了Decoder部分进行自回归（auto regressive）语言模型训练。 GPT-2概述模型的输入输入的处理分为两步：token embedding + position encoding。即: 在嵌入矩阵中查找输入的单词的对应的embedding向量 融入位置编码 Decoder层每一层decoder的组成：Masked Self-Attention + Feed Forward Neural Network Self-Attention所做的事情是：它通过对句子片段中每个词的相关性打分，并将这些词的表示向量根据相关性加权求和，从而让模型能够将词和其他相关词向量的信息融合起来。 Masked Self-Attention做的是：将mask位置对应的的attention score变成一个非常小的数字或者0，让其他单词再self attention的时候（加权求和的时候）不考虑这些单词。 模型的输出当模型顶部的Decoder层产生输出向量时，模型会将这个向量乘以一个巨大的嵌入矩阵（vocab size x embedding size）来计算该向量和所有单词embedding向量的相关得分。这个相乘的结果，被解释为模型词汇表中每个词的分数，经过softmax之后被转换成概率。 我们可以选择最高分数的 token（top_k=1），也可以同时考虑其他词（top k）。假设每个位置输出k个token，假设总共输出n个token，那么基于n个单词的联合概率选择的输出序列会更好。 模型完成一次迭代，输出一个单词。模型会继续迭代，直到所有的单词都已经生成，或者直到输出了表示句子末尾的token。 关于Self-Attention, Masked Self-AttentionSelf-AttentionSelf-Attention 主要通过 3 个步骤来实现： 为每个路径创建 Query、Key、Value 矩阵。 对于每个输入的token，使用它的Query向量为所有其他的Key向量进行打分。 将 Value 向量乘以它们对应的分数后求和。 Masked Self-Attention在Self-Attention的第2步，把未来的 token 评分设置为0，因此模型不能看到未来的词。 这个屏蔽（masking）经常用一个矩阵来实现，称为 attention mask矩阵。 GPT-2中的Self-Attention(skip) 自回归语言模型的应用应用在下游并取得不错效果的NLP任务有：机器翻译、摘要生成、音乐生成。（可见，主要是跟预训练任务相似的生成类任务。） 参考资料 基于transformers的自然语言处理(NLP)入门–在线阅读 https://datawhalechina.github.io/learn-nlp-with-transformers/#/","categories":[{"name":"04 组队学习","slug":"04-组队学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"第29期 基于transformer的NLP","slug":"04-组队学习/第29期-基于transformer的NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"组队学习","slug":"组队学习","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"预训练模型","slug":"预训练模型","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"GPT","slug":"GPT","permalink":"http://example.com/tags/GPT/"}]},{"title":"Chapter01-02 PyTorch的简介和安装、PyTorch基础知识","slug":"pytorch-chap01-02","date":"2021-09-18T02:26:03.000Z","updated":"2021-10-13T11:05:03.457Z","comments":true,"path":"pytorch-chap01-02/","link":"","permalink":"http://example.com/pytorch-chap01-02/","excerpt":"","text":"第一章 PyTorch的简介和安装PyTorch简介PyTorch是由Facebook人工智能研究小组开发的一种基于Lua编写的Torch库的Python实现的深度学习库，目前被广泛应用于学术界和工业界，而随着Caffe2项目并入Pytorch， Pytorch开始影响到TensorFlow在深度学习应用框架领域的地位。总的来说，PyTorch是当前难得的简洁优雅且高效快速的框架。因此本课程我们选择了PyTorch来进行开源学习。 PyTorch的安装PyTorch官网：https://pytorch.org/ PyTorch的发展和优势“All in Pytorch”. PyTorch VS TensorFlow 第二章 PyTorch的基础知识Tensor/张量张量是基于向量和矩阵的推广，比如我们可以将标量视为零阶张量，矢量可以视为一阶张量，矩阵就是二阶张量。 0维张量/标量 标量是1个数字 1维张量/向量 1维张量称为“向量” 2维张量 2维张量称为“矩阵” 3维张量 时间序列数据、股价、文本数据、彩色图片(RGB) 4维=图像 5维=视频 在PyTorch中， torch.Tensor 是存储和变换数据的主要工具。 tensor-构造创建一个随机初始化的矩阵： 123x = torch.rand(4, 3) # 构造张量print(x.size()) # 获取维度信息print(x.shape) # 获取维度信息 还有一些常见的构造Tensor的函数： PyTorch中的Tensor支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考官方文档。 tensor-squeeze 增加/删除一个维度 tensor-transpose 转置 tensor-cat concatenate多个tensor 自动求导/自动微分PyTorch中，所有神经网络的核心是autograd包。autograd包为张量上的所有操作提供了自动求导机制。 How to Calculate Gradient 1234567891011121314151617&gt;&gt;&gt; x = torch.tensor([[1., 0.], [-1., 1.]], requires_grad = True)&gt;&gt;&gt; xtensor([[ 1., 0.], [-1., 1.]], requires_grad=True)&gt;&gt;&gt; z = x.pow(2)&gt;&gt;&gt; ztensor([[1., 0.], [1., 1.]], grad_fn=&lt;PowBackward0&gt;)&gt;&gt;&gt; z = z.sum()&gt;&gt;&gt; ztensor(3., grad_fn=&lt;SumBackward0&gt;)&gt;&gt;&gt; z.backward()&gt;&gt;&gt; ztensor(3., grad_fn=&lt;SumBackward0&gt;)&gt;&gt;&gt; x.gradtensor([[ 2., 0.], [-2., 2.]]) 并行计算简介在利用PyTorch做深度学习的过程中，可能会遇到数据量较大无法在单块GPU上完成，或者需要提升计算速度的场景，这时就需要用到并行计算。GPU的出现让我们可以训练的更快，更好。PyTorch可以在编写完模型之后，让多个GPU来参与训练。 CUDA是我们使用GPU的提供商——NVIDIA提供的GPU并行计算框架。对于GPU本身的编程，使用的是CUDA语言来实现的。但是，在我们使用PyTorch编写深度学习代码时，使用的CUDA又是另一个意思。在PyTorch使用 CUDA表示要开始要求我们的模型或者数据开始使用GPU了。 在编写程序中，当我们使用了 cuda() 时，其功能是让我们的模型或者数据迁移到GPU当中，通过GPU开始计算。 不同的数据分布到不同的设备中，执行相同的任务(Data parallelism): 参考资料 Datawhale开源项目：深入浅出PyTorch https://github.com/datawhalechina/thorough-pytorch/ 李宏毅机器学习2021春-PyTorch Tutorial https://www.bilibili.com/video/BV1Wv411h7kN?p=5 What is a gpu and do you need one in deep learning https://towardsdatascience.com/what-is-a-gpu-and-do-you-need-one-in-deep-learning-718b9597aa0d 动手学深度学习pytorch版 https://zh-v2.d2l.ai/chapter_preface/index.html","categories":[{"name":"04 组队学习","slug":"04-组队学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"第30期 深入浅出PyTorch","slug":"04-组队学习/第30期-深入浅出PyTorch","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAPyTorch/"}],"tags":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"组队学习","slug":"组队学习","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://example.com/tags/PyTorch/"}]},{"title":"Task03 学习BERT","slug":"nlp-transformer-task03","date":"2021-09-17T01:44:18.000Z","updated":"2021-10-02T09:21:28.431Z","comments":true,"path":"nlp-transformer-task03/","link":"","permalink":"http://example.com/nlp-transformer-task03/","excerpt":"","text":"BERT简介BERT首先在大规模无监督语料上进行预训练，然后在预训练好的参数基础上增加一个与任务相关的神经网络层，并在该任务的数据上进行微调训，最终取得很好的效果。BERT的这个训练过程可以简述为：预训练（pre-train）+微调（fine-tune/fine-tuning），已经成为最近几年最流行的NLP解决方案的范式。 如何直接应用BERT 下载在无监督语料上预训练好的BERT模型，一般来说对应了3个文件：BERT模型配置文件（用来确定Transformer的层数，隐藏层大小等），BERT模型参数，BERT词表（BERT所能处理的所有token）。 针对特定任务需要，在BERT模型上增加一个任务相关的神经网络，比如一个简单的分类器，然后在特定任务监督数据上进行微调训练。（微调的一种理解：学习率较小，训练epoch数量较少，对模型整体参数进行轻微调整） BERT的结构BERT模型结构基本上就是Transformer的encoder部分。 BERT的输入和输出BERT模型输入有一点特殊的地方是在一句话最开始拼接了一个[CLS] token，如下图所示。这个特殊的[CLS] token经过BERT得到的向量表示通常被用作当前的句子表示。我们直接使用第1个位置的向量输出（对应的是[CLS]）传入classifier网络，然后进行分类任务。 BERT的预训练任务BERT是一个多任务模型，它的任务是由两个自监督任务组成。 Masked Language Model（MLM）MLM：将输入文本序列的部分（15%）单词随机Mask掉，让BERT来预测这些被Mask的词语。（可以说是完形填空） Masked Language Model（MLM）和核心思想取自Wilson Taylor在1953年发表的一篇论文《cloze procedure: A new tool for measuring readability》。所谓MLM是指在训练的时候随即从输入预料上mask掉一些单词，然后通过的上下文预测该单词，该任务非常像我们在中学时期经常做的完形填空。正如传统的语言模型算法和RNN匹配那样，MLM的这个性质和Transformer的结构是非常匹配的。 Next Sentence Prediction（NSP）NSP：判断两个句子是否是相邻句子。即，输入是sentence A和sentence B，经过BERT编码之后，使用CLS token的向量表示来预测两个句子是否是相邻句子。 Next Sentence Prediction（NSP）的任务是判断句子B是否是句子A的下文。如果是的话输出’IsNext‘，否则输出’NotNext‘。训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在[CLS]符号中。 BERT的应用特征提取由于BERT模型可以得到输入序列所对应的所有token的向量表示，因此不仅可以使用最后一程BERT的输出连接上任务网络进行微调，还可以直接使用这些token的向量当作特征。比如，可以直接提取每一层encoder的token表示当作特征，输入现有的特定任务神经网络中进行训练。 Pretrain + Fine tune参考资料 基于transformers的自然语言处理(NLP)入门–在线阅读 https://datawhalechina.github.io/learn-nlp-with-transformers/#/ 李宏毅机器学习2019-ELMO,BERT,GPT https:// www.bilibili.com/video/BV1Gb411n7dE?p=61","categories":[{"name":"04 组队学习","slug":"04-组队学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"第29期 基于transformer的NLP","slug":"04-组队学习/第29期-基于transformer的NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"组队学习","slug":"组队学习","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"预训练模型","slug":"预训练模型","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"BERT","slug":"BERT","permalink":"http://example.com/tags/BERT/"}]},{"title":"Task02 学习Attentioin和Transformer","slug":"nlp-transformer-task02","date":"2021-09-17T01:09:24.000Z","updated":"2021-10-02T09:21:22.586Z","comments":true,"path":"nlp-transformer-task02/","link":"","permalink":"http://example.com/nlp-transformer-task02/","excerpt":"","text":"Attentionseq2seqseq2seq是一种常见的NLP模型结构，全称是：sequence to sequence，翻译为“序列到序列”。顾名思义：从一个文本序列得到一个新的文本序列。典型的任务有：机器翻译任务，文本摘要任务。 seq2seq模型由编码器（encoder）和解码器（decoder）组成，编码器用来分析输入序列，解码器用来生成输出序列。编码器会处理输入序列中的每个元素，把这些信息转换成为一个背景向量（context vector）。当我们处理完整个输入序列后，编码器把背景向量发送给解码器，解码器通过背景向量中的信息，逐个元素输出新的序列。 在transformer模型之前，seq2seq中的编码器和解码器一般采用循环神经网络（RNN），虽然非常经典，但是局限性也非常大。最大的局限性就在于编码器和解码器之间的唯一联系就是一个固定长度的context向量。也就是说，编码器要将整个序列的信息压缩进一个固定长度的向量中。这样做存在两个弊端： 语义向量可能无法完全表示整个序列的信息 先输入到网络的内容携带的信息会被后输入的信息覆盖掉，输入序列越长，这个现象就越严重 Attention为了解决seq2seq模型中的两个弊端，Bahdanau等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》中提出使用Attention机制，使得seq2seq模型可以有区分度、有重点地关注输入序列，从而极大地提高了机器翻译的质量。 一个有注意力机制的seq2seq与经典的seq2seq主要有2点不同： 首先，编码器会把更多的数据传递给解码器。编码器把所有时间步的 hidden state（隐藏层状态）传递给解码器，而不是只传递最后一个 hidden state（隐藏层状态） 注意力模型的解码器在产生输出之前，做了一个额外的attention处理 Transformer模型架构transformer原论文的架构图： 一个更清晰的架构图： 从输入到输出拆开看就是： INPUT：input vector + position encoding ENCODERs（×6），and each encoder includes： input multi-head self-attention residual connection&amp;norm full-connected network residual connection&amp;norm output DECODERs（×6），and each decoder includes： input Masked multihead self-attention residual connection&amp;norm multi-head self-attention residual connection&amp;norm full-connected network residual connection&amp;norm output OUTPUT： output (decoder’s) linear layer softmax layer output 模型输入词向量和常见的NLP任务一样，我们首先会使用词嵌入算法（embedding），将输入文本序列的每个词转换为一个词向量。 位置向量Transformer模型对每个输入的词向量都加上了一个位置向量。这些向量有助于确定每个单词的位置特征，或者句子中不同单词之间的距离特征。词向量加上位置向量背后的直觉是：将这些表示位置的向量添加到词向量中，得到的新向量，可以为模型提供更多有意义的信息，比如词的位置，词之间的距离等。 （生成位置编码向量的方法有很多种） 编码器和解码器注：1. 编码器和解码器中有相似的模块和结构，所以合并到一起介绍。2. 本部分按照李宏毅老师的Attention，Transformer部分的课程PPT来，因为lee的课程对新手更友好。 Self-Attentionself-attention对于每个向量都会考虑整个sequence的信息后输出一个向量，self-attention结构如下：FC：Fully-connected network 全连接网络ai: 输入变量。可能是整个网络的输入，也可能是某个隐藏层的输出bi: 考虑整个sequence信息后的输出变量 矩阵计算：目标：根据输入向量矩阵I，计算输出向量矩阵O。矩阵运算过程： 矩阵I分别乘以Wq, Wk, Wv（参数矩阵，需要模型进行学习），得到矩阵Q, K, V。 矩阵K的转置乘以Q，得到注意力权重矩阵A，归一化得到矩阵A’。 矩阵V乘矩阵A‘，得到输出向量矩阵O。 Multi Head Self-Attention简单地说，多了几组Q，K，V。在Self-Attention中，我们是使用𝑞去寻找与之相关的𝑘，但是这个相关性并不一定有一种。那多种相关性体现到计算方式上就是有多个矩阵𝑞，不同的𝑞负责代表不同的相关性。 Transformer 的论文通过增加多头注意力机制（一组注意力称为一个 attention head），进一步完善了Self-Attention。这种机制从如下两个方面增强了attention层的能力： 它扩展了模型关注不同位置的能力。 多头注意力机制赋予attention层多个“子表示空间”。 残差链接和归一化残差链接：一种把input向量和output向量直接加起来的架构。归一化：把数据映射到0～1范围之内处理。 模型输出线性层和softmaxDecoder 最终的输出是一个向量，其中每个元素是浮点数。我们怎么把这个向量转换为单词呢？这是线性层和softmax完成的。 线性层就是一个普通的全连接神经网络，可以把解码器输出的向量，映射到一个更大的向量，这个向量称为 logits 向量：假设我们的模型有 10000 个英语单词（模型的输出词汇表），此 logits 向量便会有 10000 个数字，每个数表示一个单词的分数。 然后，Softmax 层会把这些分数转换为概率（把所有的分数转换为正数，并且加起来等于 1）。然后选择最高概率的那个数字对应的词，就是这个时间步的输出单词。 损失函数Transformer训练的时候，需要将解码器的输出和label一同送入损失函数，以获得loss，最终模型根据loss进行方向传播。 只要Transformer解码器预测了组概率，我们就可以把这组概率和正确的输出概率做对比，然后使用反向传播来调整模型的权重，使得输出的概率分布更加接近整数输出。 那我们要怎么比较两个概率分布呢？：我们可以简单的用两组概率向量的的空间距离作为loss（向量相减，然后求平方和，再开方），当然也可以使用交叉熵(cross-entropy)]和KL 散度(Kullback–Leibler divergence)。 参考资料理论部分[1] (强推)李宏毅2021春机器学习课程 https://www.bilibili.com/video/BV1Wv411h7kN?from=search&amp;seid=17090062977285779802&amp;spm_id_from=333.337.0.0[2] 基于transformers的自然语言处理(NLP)入门（涵盖了图解系列、annotated transformer、huggingface） https://github.com/datawhalechina/learn-nlp-with-transformers[3] 图解transformer|The Illustrated Transformer http://jalammar.github.io/illustrated-transformer/[4] 图解seq2seq, attention|Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ 代码部分[5] The Annotated Transformer http://nlp.seas.harvard.edu//2018/04/03/attention.html[6] Huggingface/transformers https://github.com/huggingface/transformers/blob/master/README_zh-hans.md 论文部分Attention is all “we” need. 其他不错的博客或教程[7] 基于transformers的自然语言处理(NLP)入门–在线阅读 https://datawhalechina.github.io/learn-nlp-with-transformers/#/[8] 李宏毅2021春机器学习课程笔记——自注意力机制 https://www.cnblogs.com/sykline/p/14730088.html[9] 李宏毅2021春机器学习课程笔记——Transformer模型 https://www.cnblogs.com/sykline/p/14785552.html[10] 李宏毅机器学习学习笔记——自注意力机制 https://blog.csdn.net/p_memory/article/details/116271274[11] 车万翔-自然语言处理新范式：基于预训练的方法【讲座+PPT】 https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true[12] 苏剑林-《Attention is All You Need》浅读（简介+代码）https://spaces.ac.cn/archives/4765","categories":[{"name":"04 组队学习","slug":"04-组队学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"第29期 基于transformer的NLP","slug":"04-组队学习/第29期-基于transformer的NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"组队学习","slug":"组队学习","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"预训练模型","slug":"预训练模型","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"attention","slug":"attention","permalink":"http://example.com/tags/attention/"},{"name":"transfomer","slug":"transfomer","permalink":"http://example.com/tags/transfomer/"}]},{"title":"Task01 NLP学习概览","slug":"nlp-transformer-task01","date":"2021-09-12T16:14:06.000Z","updated":"2021-10-02T09:21:16.372Z","comments":true,"path":"nlp-transformer-task01/","link":"","permalink":"http://example.com/nlp-transformer-task01/","excerpt":"","text":"NLP思维导图(最近更新日期：2021-09-13) 参考资料 Datawhale-基于transformers的自然语言处理(NLP)入门 https://github.com/datawhalechina/learn-nlp-with-transformers 《自然语言处理-基于预训练模型的方法》 https://item.jd.com/13344628.html 刘知远老师-NLP研究入门之道 https://github.com/zibuyu/research_tao","categories":[{"name":"04 组队学习","slug":"04-组队学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"第29期 基于transformer的NLP","slug":"04-组队学习/第29期-基于transformer的NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"组队学习","slug":"组队学习","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"预训练模型","slug":"预训练模型","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"}]},{"title":"CS61A Week1 Comupter_Science, Functions","slug":"cs61a-week1","date":"2021-09-01T08:46:52.000Z","updated":"2021-09-17T01:51:24.035Z","comments":true,"path":"cs61a-week1/","link":"","permalink":"http://example.com/cs61a-week1/","excerpt":"","text":"前言 CS61A作为61系列基础课程的第一门课程，是一门计算机入门导论课程，伯克利大一新生的第一门计算机课程。该课程主要使用Python语言，简要介绍了计算机的各种概念，范围广而涉猎不深，包括高阶函数，抽象，递归和树，OOP，简单的SQL语句，Scheme语法和解释器等概念。 目前推荐的课程是20年秋季学期(fa20)的课程。 ——名校公开课程评价网 名校公开课程评价网-cs61a 文档组织对应不同教学内容，文档组织如下： 0_课件：lecture 1_代码：lecture代码 2_笔记：学习笔记 使用markdown 内容包括：Weekx内容(x=week number), Lecture Notes, Lab Notes, Homework Notes 3_实验：lab 4_作业：homework 5_项目：project Week1内容 Week1主要内容： Lecture01 Computer Science; Lecture02 Functions 介绍计算机科学和函数基础知识 Lab00: Getting Started 安装Python3，终端的使用，常用命令行，文档测试（doctest）的使用，测试和提交使用OK系统 HW01: Variables &amp; Functions, Control。掌握函数特性 Lecture NotesWhat is Computer Science 计算机科学是一门定义和解决计算问题的方法和技术的学科。它的分支结构参考CSRanking的分类方式（ http://csrankings.org/#/index?all&amp;world），大概可以分为人工智能（计算机视觉、机器学习、自然语言处理、信息检索…）、系统（计算机结构、网络、安全、数据库、操作系统、分布式…）、理论（算法和复杂度、formal method…）、交叉（计算生物/生物计算、人机交互、机器人…）等方向。 Anatomy of a Call Expression: Operator, Operand 在程式語言中, 指示程式進行運算(計算、比較或連結) 的符號, 稱為operators (運算子), 被運算的資料稱為operands (運算元), 一句中有operators 及operands 就稱為expression。 Environment Diagrams Environment Diagrams Tools: http://pythontutor.com/composingprograms.html#mode=edit Defining Functions 这里涉及到全局变量（Global Variable）和局部变量（Local Variable）的问题，全局变量是整个程序都可访问的变量，生存期从程序开始到程序结束；局部变量存在于模块中(比如某个函数)，只有在模块中才可以访问，生存期从模块开始到模块结束。简单的说， 全局变量：在模块内、在所有函数的外面、在class外面 局部变量：在函数内、在class的方法内 Lab Notes常用命令: 1234ls: lists all files in the current directorycd &lt;path to directory&gt;: change into the specified directorymkdir &lt;directory name&gt;: make a new directory with the given namemv &lt;source path&gt; &lt;destination path&gt;: move the file at the given source to the given destination 1234python3 xxx.py # 运行程序python3 -i xxx.py # 运行程序并打开交互式会话python3 -m doctest xxx.py # 运行文档测试python3 -m doctest - xxx.py # 运行文档测试并显示样例 Homework NotesBugTypeError: &#39;int&#39; object is not callable 修改程序名就可以了。 QuestionQ5: If Function vs Statement while… 参考资料 cs61a 20fall 官网 https://inst.eecs.berkeley.edu/~cs61a/fa20/","categories":[{"name":"01 计算机基础","slug":"01-计算机基础","permalink":"http://example.com/categories/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"CS61A 计算机程序的构造与解释","slug":"01-计算机基础/CS61A-计算机程序的构造与解释","permalink":"http://example.com/categories/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/CS61A-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9E%84%E9%80%A0%E4%B8%8E%E8%A7%A3%E9%87%8A/"}],"tags":[{"name":"笔记","slug":"笔记","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"CS公开课","slug":"CS公开课","permalink":"http://example.com/tags/CS%E5%85%AC%E5%BC%80%E8%AF%BE/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"function","slug":"function","permalink":"http://example.com/tags/function/"}]},{"title":"公式之美-EVERYTHING IS EPHEMERAL BUT FORMULA IS ETERNAL","slug":"formula","date":"2021-08-04T12:27:54.000Z","updated":"2021-09-27T02:33:41.405Z","comments":true,"path":"formula/","link":"","permalink":"http://example.com/formula/","excerpt":"","text":"一个有点意思的科普书，尤其在不想写论文的时候，宁愿去看勾股定理的N种推导也不愿意碰论文。更好玩的是，这里面的插图要比内容更没有争议的获得一致好评。 1854年之前，欧洲数学家灿若星辰，笛卡儿、拉格朗日、牛顿、贝叶斯、拉普拉斯、柯西、傅里叶、伽罗瓦等，无一不是数学天才。1854—1935年，高斯、黎曼等人在数学界领袖群伦，德国取代英法成为世界的数学中心。1935年之后，希特勒给美国送上“科学大礼包”：哥德尔、爱因斯坦、德拜、冯.诺依曼、费米、冯.卡门、外尔……很多科学家逃至北美，数学大本营从德国转向美国，美国成为世界的数学中心。 古希腊几何学家阿波洛尼乌斯总结了圆锥曲线理论，一千多年后，德国天文学家开普勒才将其应用于行星轨道；高斯被认为最早发现非欧几何，半个世纪后，由他弟子创立的黎曼几何成为广义相对论的数学基础。伴随着杠杆原理、牛顿三大定律、麦克斯韦方程、香农公式、贝叶斯定理等，人类向蒸汽时代、电力时代、信息时代乃至人工智能时代徐徐迈进。 1+1=2：数学的溯源 勾股定理：数与形的结合 费马大定理：困扰人类358年 牛顿-莱布尼茨公式：无穷小的秘密 万有引力：从混沌到光明 欧拉公式：最美的等式 伽罗瓦理论：无解的方程 危险的黎曼猜想 熵增定律：寂灭是宇宙宿命？ 麦克斯韦方程组：让黑暗消失 质能方程：开启潘多拉的魔盒 薛定谔方程：猫与量子世界 狄拉克方程：反物质的“先知” 杨-米尔斯规范场论：大统一之路 香农公式：5G背后的主宰 布莱克-斯科尔斯方程：金融“巫师” 枪械：弹道里的“技术哲学” 胡克定律：机械表的心脏 混沌理论：一只蝴蝶引发的思考 凯利公式：赌场上的最大赢家 贝叶斯定理：AI如何思考？ 三体问题：挥之不去的乌云 椭圆曲线方程：比特币的基石 参考资料 《公式之美》https://book.douban.com/subject/35218287/","categories":[{"name":"沉思录","slug":"沉思录","permalink":"http://example.com/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"}],"tags":[{"name":"阅读","slug":"阅读","permalink":"http://example.com/tags/%E9%98%85%E8%AF%BB/"},{"name":"math","slug":"math","permalink":"http://example.com/tags/math/"}]},{"title":"NLP中的文本表示方法","slug":"nlp-text-representation","date":"2021-07-31T11:48:54.000Z","updated":"2021-09-17T01:53:35.231Z","comments":true,"path":"nlp-text-representation/","link":"","permalink":"http://example.com/nlp-text-representation/","excerpt":"","text":"TODO 参考资料","categories":[{"name":"02 人工智能","slug":"02-人工智能","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"自然语言处理","slug":"02-人工智能/自然语言处理","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"文本表示","slug":"文本表示","permalink":"http://example.com/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"}]},{"title":"SQL表连接&聚合函数&窗口函数","slug":"SQL重点","date":"2021-07-26T19:33:19.000Z","updated":"2021-09-17T01:53:20.789Z","comments":true,"path":"SQL重点/","link":"","permalink":"http://example.com/SQL%E9%87%8D%E7%82%B9/","excerpt":"","text":"表连接 joinjoin: 以字段（列）为单位进行多表连接。 1234Inner join # 只保留两个表中同时存在的记录。Left join # 保留左表所有的记录，无论其是否能够在右表中匹配到对应的记录。若无匹配记录，则需要用NULL填补。Right join # 保留右表所有的记录，无论其是否能够在左表中匹配到对应的记录。若无匹配记录，则需要用NULL填补。Full join # 左表和右表所有的记录都会保留，没有匹配记录的用NULL填补。 聚合函数1234sum() # 返回分组后组内所有记录的和avg() # 返回分组后组内所有记录的均值count() # 返回分组后组内所有记录的计数max()/min() # 返回分组后组内所有记录的最大值、最小值 窗口函数窗口函数对记录分组之后进行聚合计算，为分组中的每条记录返回特定值。 窗口函数的基本结构是： 12&lt;窗口函数&gt;() over (partition by &lt;col1, col2&gt; order by &lt;col3 desc/asc, col4 asc/desc&gt;) 窗口函数 介绍 rank() over() 返回记录在同一分组内的排序，如果有并列名次的行，会占用下一名次的位置 dense_rank() over() 返回记录在同一分组内的排序，如果有并列名次的行，不占用下一名次的位置 row_number() over() 返回记录在同一分组内的排序，不考虑并列名次的情况 percent_rank() over() 返回记录在同一分组内排序的分位数，为0~1 sum(col) over() 返回同一分组内所有记录col值的和，同一分组内记录的返回值相同 avg(col) over() 返回同一分组内所有记录col值的平均值，同一分组内记录的返回值相同 max/min(col) over() 返回同一分组内所有记录col值的最大值/最小值，同一分组内记录的返回值相同 聚合函数在窗口函数中，是对自身记录、及位于自身记录以上的数据进行运算的结果。聚合函数作为窗口函数，可以在每一行的数据里直观的看到，截止到本行数据，统计数据是多少（最大值、最小值等）。同时可以看出每一行数据，对整体统计数据的影响。 索引索引用来排序数据以加快搜索和排序操作的速度。可以在一个或多个列上定义索引，使DBMS保存其内容的一个排过序的列表。在定义了索引后，DBMS以使用书的索引类似的方法使用它。DBMS 搜索排过序的索引，找出匹配的位置，然后检索这些行。 索引是关系数据库中对某一列或多个列的值进行预排序的数据结构。通过使用索引，可以让数据库系统不必扫描整个表，而是直接定位到符合条件的记录，这样就大大加快了查询速度。 索引用CREATE INDEX 语句创建（不同DBMS创建索引的语句变化很大）。 参考资料[1]通俗易懂的学会：SQL窗口函数 https://zhuanlan.zhihu.com/p/92654574[2]《拿下Offer:数据分析师求职面试指南》 https://item.jd.com/12686131.html[3]《SQL必知必会》 https://book.douban.com/subject/24250054/[4]廖雪峰的官方网站-SQL教程 https://www.liaoxuefeng.com/wiki/1177760294764384","categories":[{"name":"02 人工智能","slug":"02-人工智能","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"数据分析","slug":"02-人工智能/数据分析","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://example.com/tags/SQL/"}]},{"title":"5h打通Git全套教程","slug":"git","date":"2021-07-23T13:01:25.000Z","updated":"2021-09-17T01:53:30.399Z","comments":true,"path":"git/","link":"","permalink":"http://example.com/git/","excerpt":"","text":"本文是以下课程的笔记： 【尚硅谷】5h打通Git全套教程丨2021最新IDEA版（涵盖GitHub\\Gitee码云\\GitLab）https://www.bilibili.com/video/BV1vy4y1s7k6?p=41 课程结构本套视频从基础的常用命令开始讲起，到开发工具集成Git 、GitHub如何进行团队协作、国内代码托管中心Gitee码云的使用、局域网自建代码托管平台GitLab服务器的部署。（本文主要是P1-P26的笔记。） P1-P2 Git P3-P6 Git概述 P7-P14 Git命令 P15-P18 Git分支 P19 Git团队协作 P20-P26 Git&amp;GitHub P27-P37 IDEA集成GitHub P38-P40 Gitee码云 P41-P44 GitLab P45 课程总结 Git介绍GitGit是一个免费的、开源的分布式版本控制系统，可以快速高效地处理从小型到大型的各种项目。 版本控制版本控制是一种记录文件内容变化，以便将来查阅特定版本修订情况的系统。版本控制其实最重要的是可以记录文件修改历史记录，从而让用户能够查看历史版本，方便版本切换。 版本控制工具 集中式版本控制工具：CVS、SVN(Subversion)、VSS…… 分布式版本控制工具：Git、 Mercurial、 Bazaar、 Darcs…… Git简史Git是Linus大神写的，所以和Linux一套命令 Git机制在工作区写代码，通过git add添加到暂存区，再通过git commit提交到本地库，生成历史版本。本地库可以push代码到远程库，也可以从远程库pull拉取代码，不同版本的代码可以进行merge。 代码托管中心-远程库代码托管中心是基于网络服务器的远程代码仓库，一般我们简单称为远程库。 局域网：GitLab 互联网：GitHub(国外)，Gitee（国内） Git常用命令一图以蔽之， Git分支分支在版本控制过程中，同时推进多个任务，为每个任务，我们就可以创建每个任务的单独分支。使用分支意味着程序员可以把自己的工作从开发主线上分离开来，开发自己分支的时候，不会影响主线分支的运行。对于初学者而言，分支可以简单理解为副本，一个分支就是一个单独的副本。（分支底层其实也是指针的引用） 同时并行推进多个功能开发，可以提高开发效率。各个分支在开发过程中，如果某一个分支开发失败，不会对其他分支有任何影响。失败的分支删除重新开始即可。 Git分支命令1234git branch -v # 查看分支git branch 分支名 # 创建分支git checkout 分支名 # 切换分支git merge 分支名 # 合并分支 合并冲突合并分支时，两个分支在同一个文件的同一个位置有两套完全不同的修改。Git无法替我们决定使用哪一个。必须人为决定新代码内容。 Git团队合作两个非常形象化的图和生动的例子！ 团队内合作 跨团队合作 GitHub远程仓库操作 clone会做如下操作： 拉取代码。 初始化本地仓库。 创建别名（origin） 邀请合作者 SSH登录 Git与其他环境集成官方文档-Git与各种IDE的集成：https://git-scm.com/book/en/v2 码云Gitee众所周知，GitHub服务器在国外，使用GitHub作为项目托管网站，如果网速不好的话，严重影响使用体验，甚至会出现登录不上的情况。针对这个情况，大家也可以使用国内的项目托管网站-码云 https://gitee.com/。 GitLabGitLab https://about.gitlab.com/是由GitLabInc.开发，使用MIT许可证的基于网络的Git仓库管理工具，且具有wiki和issue跟踪功能。使用Git作为代码管理工具，并在此基础上搭建起来的web服务。GitLab由乌克兰程序员DmitriyZaporozhets和ValerySizov开发，它使用Ruby语言写成。后来,一些部分用Go语言重写。截止2018年5月，该公司约有290名团队成员，以及2000多名开源贡献者。GitLab被IBM, Sony, JülichResearchCenter, NASA, Alibaba, Invincea, O’Reilly Media, Leibniz-Rechenzentrum, CERN, SpaceX等组织使用。","categories":[{"name":"03 工具箱","slug":"03-工具箱","permalink":"http://example.com/categories/03-%E5%B7%A5%E5%85%B7%E7%AE%B1/"}],"tags":[{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"},{"name":"笔记","slug":"笔记","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"}]},{"title":"大理-在民宿打工的日子","slug":"大理","date":"2021-07-21T14:49:49.000Z","updated":"2021-09-27T02:33:17.823Z","comments":true,"path":"大理/","link":"","permalink":"http://example.com/%E5%A4%A7%E7%90%86/","excerpt":"","text":"生活就是换个地方打工。 然而田园的生活也不是那么美好的，有很多想象不到的问题。比如，随时随地出现的各种蚊虫、大蜘蛛、蛇，花花草草都要每天浇水和定时剪枝，下雨天泳池会有很多脏东西需要处理，垃圾必须每天清空不然会有老鼠光顾，厨房排水做不好的话会倒流没法用（怪不得要专业疏通下水道），还有电路也时不时的出点毛病。 由于每天都在处理这种琐碎的事情，这段时间最常去的地方就是仓库了。仓库就像哆啦A梦的口袋一样，变出我以前不知道的各种工具来，有些我即使认识也不知道名字，但是这些小玩意儿在生活中如此有用，比如玻璃胶、磨砂纸、扎带种种。有一次，洗衣机旁的电源开关的接触不好，电工来修，把电线直接接到了一个插排上，虽然我对这个做法的安全性和易用性保持怀疑，但足够让物理一向不好的我很是佩服——毕竟我连火线零线都分不清，高中物理连接小灯泡的题也得想半天。 后来，就匆匆离开了。就像Harry说的，It is not better or worse than any place else - just different.","categories":[{"name":"沉思录","slug":"沉思录","permalink":"http://example.com/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"}],"tags":[{"name":"大理","slug":"大理","permalink":"http://example.com/tags/%E5%A4%A7%E7%90%86/"}]},{"title":"校准对世界的预期","slug":"校准对世界的预期","date":"2021-07-15T09:34:54.000Z","updated":"2021-10-12T05:50:20.430Z","comments":true,"path":"校准对世界的预期/","link":"","permalink":"http://example.com/%E6%A0%A1%E5%87%86%E5%AF%B9%E4%B8%96%E7%95%8C%E7%9A%84%E9%A2%84%E6%9C%9F/","excerpt":"","text":"前言校准对世界的预期，这个题目出自于播客里听到的一段话： 从学校步入职场，要做的第一件事是重新校准你对世界的预期。这世界就是很糟糕的（或者说，这世界是 okay 的，但是你对它有不切实际的预期）。 很早的时候我经常怀疑，老师讲的东西有什么用？总有那么些课程，既不能直接用到工作中，又不能锻炼思维和逻辑能力。最后不得不承认一个危险的真相：上课太耽误学习了。而自己之所以为此感到痛苦，是因为抱有不合理的假设和预期。比如，当我产生怀疑的时候，其实我的假设就是老师教的就应该是有用的。可是凭什么呢？明明是我一厢情愿的这样认为，然后发现事实与预期不符，于是就不断痛苦。这样的假设还包括： H1. 学校教的是对工作有用的(×)学校是用来筛选的。学校有一套自己的游戏规则，按照这个规则走，就会得到老师和同学的赞赏、拿奖学金、保研、发论文、获得一系列荣誉奖项等——最终筛选出一个乖巧、听话、能干活、而且学习能力不错的人（或者至少愿意伪装成这样的人）。但把这个规则玩好，并不等同于拥有了工作需要的实力和能力，因为工作是另一套规则和玩法。由于从小一直灌输的是学校的规则，所以很难意识到游戏已经切换了，这就是常说的学生思维吧。 同时，这个世界上有无数个行业和领域，就有无数游戏和规则。诚然底层的东西是可以迁移，例如努力、负责等品质在哪里都是被认可的，但仍然要保持开放的心态，不能做出一个选择就否定其他选择，比如，一些马上想到但是不可描述的例子。这种心态的形成至少有3个因素：1.人的心理天然倾向于找各种理由支持自己的选择，而忽略对自己选择不利的方面。2.即使知道自己的选择有不好的一面，但出于虚荣坚决不能承认。3.即使知道自己的选择有不好的一面，但忽悠更多人进来才方便找人接盘。 最终通过身边的现象来看，乖乖听老师话的找不到工作只能考公（不是说公务员不好，而是说这种情况下并没有选择权），早早翘课去实习的拿到了令人羡慕的offer。用朋友圈里看到的一段话概括就是：主修LeetCode，辅修bilibili，旁听cs公开课，最后混一下学位必修课以达到毕业要求。这才是找到好工作的正确姿势。 H2. 世界是公平的(×)世界本来是不公平的，也不天然应该是公平的，因为人类文明的出现于是变好了一点点。所以那些声称只能接受这个糟糕的世界的说法也不对，这简直是在否定人类文明，没有什么是理所当然的。 尝试放下所有假设和预期我们对于很多东西的判断是来自于书籍、电视、网络、和他人的只言片语、自己的经验。通过经验学习当然很有效，但问题是这些经验不都是客观理性的，如果我们放下这些先验知识，感受事物本身，通过自己的独立思考得到自己的结论，那么被忽悠的概率会大大降低吧。 参考播客 BYM 职场12: 生活就是不公平的，你打算怎么办 收听链接 BYM 职场14: 认清世界之残酷，你才可能成为那 5% 拥有自己事业的人 收听链接","categories":[{"name":"沉思录","slug":"沉思录","permalink":"http://example.com/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"}],"tags":[{"name":"思考","slug":"思考","permalink":"http://example.com/tags/%E6%80%9D%E8%80%83/"},{"name":"播客","slug":"播客","permalink":"http://example.com/tags/%E6%92%AD%E5%AE%A2/"}]},{"title":"一些好用的中英文LaTeX简历模板","slug":"一些好用的中英文LaTeX简历模板","date":"2021-07-15T08:23:20.000Z","updated":"2021-09-17T01:53:59.711Z","comments":true,"path":"一些好用的中英文LaTeX简历模板/","link":"","permalink":"http://example.com/%E4%B8%80%E4%BA%9B%E5%A5%BD%E7%94%A8%E7%9A%84%E4%B8%AD%E8%8B%B1%E6%96%87LaTeX%E7%AE%80%E5%8E%86%E6%A8%A1%E6%9D%BF/","excerpt":"","text":"总结简历的结构主要包括： 标题 Heading（姓名、联系方式等） 教育背景 Education 经历 Experience 实习 Work 科研/论文 Publications 项目 Projects 个人技能 Skills GitHub模板英文简历 https://github.com/sb2nov/resume 中文简历 https://github.com/hijiangtao/resume 中英文兼容的简历 https://github.com/billryan/resume/tree/zh_CN 怎么写简历的参考文章师妹看了都说好的简历长啥样https://mp.weixin.qq.com/s/ea2Pq3ZbJ30lTV3etaECoQ 一个简洁优雅的 XeLaTeX 简历模板https://tiankuizhang.github.io/files/00CV_CN/README/","categories":[{"name":"03 工具箱","slug":"03-工具箱","permalink":"http://example.com/categories/03-%E5%B7%A5%E5%85%B7%E7%AE%B1/"}],"tags":[{"name":"laTeX","slug":"laTeX","permalink":"http://example.com/tags/laTeX/"},{"name":"简历","slug":"简历","permalink":"http://example.com/tags/%E7%AE%80%E5%8E%86/"}]},{"title":"SQL学习资料","slug":"SQL学习资料","date":"2021-07-14T07:32:58.000Z","updated":"2021-09-17T01:53:15.370Z","comments":true,"path":"SQL学习资料/","link":"","permalink":"http://example.com/SQL%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/","excerpt":"","text":"&lt;&lt;SQL必知必会&gt;&gt; https://book.douban.com/subject/35167240/ SQLZOO https://sqlzoo.net/ 牛客网-SQL https://www.nowcoder.com/activity/oj?tab=1 LeetCode-数据库 https://leetcode-cn.com/problemset/database/ Datafrog-SQL经典45题 https://www.bilibili.com/video/BV1pp4y1Q7Yv 尚硅谷-MySQL基础教程 https://www.bilibili.com/video/BV1xW411u7ax","categories":[{"name":"02 人工智能","slug":"02-人工智能","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"数据分析","slug":"02-人工智能/数据分析","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://example.com/tags/SQL/"}]},{"title":"概率论与数理统计","slug":"概率论与数理统计","date":"2021-07-14T03:03:00.000Z","updated":"2021-09-17T01:53:40.027Z","comments":true,"path":"概率论与数理统计/","link":"","permalink":"http://example.com/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/","excerpt":"","text":"概率统计的基础概念随机试验；随机变量；概率分布；概率分布函数；概率密度函数；累积分布函数；样本和总体。 离散型随机变量及其分布根据随机试验的结果数量是否可数，分为离散型随机变量和连续型随机变量。 0-1分布/伯努利分布 定义：伯努利分布指的是对于随机变量X有参数为p（0&lt;1&lt;P），它分别以概率p和1-p取1和0为值。ex. 令X表示抛硬币的结果。 表示：X ~ b( p) 公式：P(X=1) = p, P(X=0) = 1-p, where p in [0,1] 期望与方差：E(X)=p, D(X)=p(1-p) 二项分布/n个重复独立的伯努利分布 在n次独立重复的伯努利试验中，设每次试验中事件A发生的概率为p。用X表示n重伯努利试验中事件A发生的次数，则X的可能取值为0，1，…，n,且对每一个k（0≤k≤n）,事件{X=k}即为“n次试验中事件A恰好发生k次”，随机变量X的离散概率分布即为二项分布（Binomial Distribution）。 表示：X ~ b(n, p) 概率函数： 期望与方差：E(X)=np, D(X)=np(1-p) 几何分布 定义：几何分布（Geometric distribution）是离散型概率分布。其中一种定义为：在n次伯努利试验中，试验k次才得到第一次成功的机率。详细地说，是：前k-1次皆失败，第k次成功的概率。 表示：X ~ g( p) 概率函数： 期望与方差：E(X)=1/p, D(X)=(1-p)/p^2 泊松分布 定义：Poisson分布，是一种统计与概率学里常见到的离散概率分布。泊松分布的参数λ是单位时间(或单位面积)内随机事件的平均发生次数。 泊松分布适合于描述单位时间内随机事件发生的次数。 表示：X ~ p(λ) 概率函数： 期望与方差：E(X)=λ, D(X)=λ ex. 某一服务设施在一定时间内到达的人数、某网站或APP在单位时间内的访问人数。 泊松分布与二项分布：当二项分布的n很大而p很小时，泊松分布可作为二项分布的近似，其中λ为np。通常当n≧20,p≦0.05时，就可以用泊松公式近似得计算。 连续型随机变量及其分布均匀分布 定义：均匀分布也叫矩形分布，它是对称概率分布，在相同长度间隔的分布概率是等可能的。 均匀分布由两个参数a和b定义，它们是数轴上的最小值和最大值，通常缩写为U（a，b）。 表示：X ~ u(a,b) 概率密度函数： 累积分布函数： 期望和方差：E(X)=(a+b)/2, D(X)=(a-b)^2/12 正态分布/高斯分布 定义：若随机变量X服从一个数学期望为μ、方差为σ^2的正态分布，记为N(μ，σ^2)。其概率密度函数为正态分布的期望值μ决定了其位置，其标准差σ决定了分布的幅度。当μ = 0,σ = 1时的正态分布是标准正态分布。 表示：N(μ，σ^2) 标准化变换： 概率分布函数： 期望和方差：E(X)=μ，D(X)=σ^2 性质： 指数分布 定义：在概率理论和统计学中，指数分布（也称为负指数分布）是描述泊松过程中的事件之间的时间的概率分布，即事件以恒定平均速率连续且独立地发生的过程。 X ~ E(λ) 其中λ &gt; 0是分布的一个参数，常被称为率参数（rate parameter）。即每单位时间内发生某事件的次数。指数分布的区间是[0,∞)。 概率密度函数： 累积分布函数： 期望和方差：E(X)=1/λ, D(X)=1/λ^2 随机变量的分布特征与统计量数据的分布特征与统计量 重要随机变量的期望和方差总结 随机变量 表示 期望 方差 0-1分布 X ~ b( p) E(X)=p D(X)=p(1-p) 二项分布 X ~ b(n, p) E(X)=np D(X)=np(1-p) 几何分布 X ~ g( p) E(X)=1/p D(X)=(1-p)/p^2 泊松分布 X ~ p(λ) E(X)=λ D(X)=λ 均匀分布 X ~ u(a,b) E(X)=(a+b)/2 D(X)=(a-b)^2/12 正态分布 N(μ，σ^2) E(X)=μ D(X)=σ^2 指数分布 X ~ E(λ) E(X)=1/λ D(X)=1/λ^2 协方差和相关系数协方差协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。协方差表示的是两个变量的总体的误差。https://baike.baidu.com/item/协方差 相关系数相关关系是一种非确定性的关系，相关系数是研究变量之间线性相关程度的量。相关系数是最早由统计学家卡尔·皮尔逊设计的统计指标，是研究变量之间线性相关程度的量，一般用字母 r 表示。由于研究对象的不同，相关系数有多种定义方式，较为常用的是皮尔逊相关系数。https://baike.baidu.com/item/相关系数 独立事件、条件概率、贝叶斯定理独立事件独立事件：两个事件不论哪一个事件发生都不影响另一个发生的概率。A和B是独立的, 当且仅当: P(AB) = P(A)P(B) 条件概率条件概率：当事件B已经发生时，事件A发生的概率。事件B发生条件下事件A发生的概率为: P(A|B) = P(AB)/P(B)概率的乘法公式: P(AB) = P(A|B)P(B) = P(B|A)P(A) 全概率公式P(B) = ∑P(B|Ai)P(Ai)，i=1,2,...,k 贝叶斯定理通常P(Ai)为A的先验概率，P(Ai|B)为A的后验概率。 大数定理与中心极限定理大数定律大数定律：将随机变量X所对应的随机试验重复多次，随着试验次数的增加，X的均值会愈发趋近于E(X)。或当样本数据无限大时，样本均值趋于总体均值。 中心极限定理中心极限定理（central limit theorem）:设从均值为μ、方差为o2 （有限）的任意一个总体中抽取样本量为n的样本，当n充分大时，样本均值X的抽样分布近似服从均值为μ、方差为o2/n的正态分布。 参数估计置信区间、置信度在区间估计中，由样本统计量所构造的总体参数的估计区间称为置信区间（confidence interval），其中区间的最小值称为置信下限，最大值称为置信上限。由于统计学家在某种程度上确信这个区间会包含真正的总体参数，所以给它取名为置信区间。原因是，如果抽取了许多不同的样本，比如说抽取100个样本，根据每一个样本构造一个置信区间，这样，由100个样本构造的总体参数的100个置信区间中，有95%的区间包含了总体参数的真值，而5%则没包含，则95%这个值称为置信水平。一 般地，如果将构造置信区间的步骤重复多次，置信区间中包含总体参数真值的次数所占的比例称为置信水平（ confidence level），也称为置信度或置信系数（ confidence coefficient）。 评价估计量的标准：1.无偏性 2.有效性 3.一致性 一个总体参数的区间估计 两个总体参数的区间估计 假设检验H0: 原假设H1: 备择假设 检验统计量：用于假设检验计算的统计量，基于样本检验统计量的值来接受或者拒绝原假设。在原假设成立的情况下，检验统计量服从一个特定的分布；而在备择假设成立的情况下，则不服从该分布。常用的检验统计量有t统计量、z统计量等。 假设检验的基本思想通过证明在原假设成立的前提下，检验统计量出现当前值或者更为极端的值属于“小概率”事件，以此推翻原假设，接受备择假设。 p-value“检验统计量出现当前值或者更为极端的值”的概率就是p-value.将p值与预先设定的显著性水平α进行对比，如果p值小于α，就可以推翻原假设，接受备择假设。 两类错误 项目 没有拒绝H0 拒绝H0 H0为真 1-α (正确决策) α (弃真错误/第i类错误) H0为伪 β (取伪错误/第ii类错误) 1-β (正确决策) 抽样分布：z分布、卡方分布、t分布、F分布z分布z分布：正态分布（Normal distribution）又名高斯分布（Gaussiandistribution），若随机变量X服从一个数学期望为μ、方差为σ2的高斯分布，记为N(μ，σ2)。 卡方分布若n个相互独立的随机变量ξ₁，ξ₂，…,ξn ，均服从标准正态分布（也称独立同分布于标准正态分布），则这n个服从标准正态分布的随机变量的平方和构成一新的随机变量，其分布规律称为卡方分布（chi-square distribution）。其中参数n称为自由度。卡方分布是由正态分布构造而成的一个新的分布，当自由度很大时，卡方分布近似为正态分布。E = n, D = 2n t分布t分布:首先要提一句u分布，正态分布（normal distribution）是许多统计方法的理论基础。正态分布的两个参数μ和σ决定了正态分布的位置和形态。为了应用方便，常将一般的正态变 量X通过u变换(X一μ)/σ转化成标准正态变量u， 以使原来各种形态的正态分布都转换为μ=0，σ= 1的标准正态分布（standard normaldistribution） ，亦称u分布。根据中心极限定理，通过抽样模拟试验表明，在正态分布总体中以固定n抽取若干个样本时，样本均数的分布仍服从正态分布，即N (μ， σ)。所以，对样本均数的分布进行u变换，也可变换为标准正态分布N(0,1)由于在实际工作中，往往σ（总体方差）是未知的，常用s （样本方差）作为o的估计值，为了与u变换区别，称为t变换，统计量t值的分布称为t分布。 F分布 一个总体参数的检验 两个总体参数的检验 参考资料[1]《统计学完全教程》第一版 L.沃塞曼[2]《统计学》第六版 贾俊平","categories":[{"name":"02 人工智能","slug":"02-人工智能","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"数据分析","slug":"02-人工智能/数据分析","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"统计","slug":"统计","permalink":"http://example.com/tags/%E7%BB%9F%E8%AE%A1/"}]}],"categories":[{"name":"04 组队学习","slug":"04-组队学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"第30期 李宏毅机器学习","slug":"04-组队学习/第30期-李宏毅机器学习","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"第30期 深入浅出PyTorch","slug":"04-组队学习/第30期-深入浅出PyTorch","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAPyTorch/"},{"name":"第29期 基于transformer的NLP","slug":"04-组队学习/第29期-基于transformer的NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"},{"name":"01 计算机基础","slug":"01-计算机基础","permalink":"http://example.com/categories/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"CS61A 计算机程序的构造与解释","slug":"01-计算机基础/CS61A-计算机程序的构造与解释","permalink":"http://example.com/categories/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/CS61A-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9E%84%E9%80%A0%E4%B8%8E%E8%A7%A3%E9%87%8A/"},{"name":"沉思录","slug":"沉思录","permalink":"http://example.com/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"},{"name":"02 人工智能","slug":"02-人工智能","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"自然语言处理","slug":"02-人工智能/自然语言处理","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"数据分析","slug":"02-人工智能/数据分析","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"03 工具箱","slug":"03-工具箱","permalink":"http://example.com/categories/03-%E5%B7%A5%E5%85%B7%E7%AE%B1/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"http://example.com/tags/machine-learning/"},{"name":"team leaning","slug":"team-leaning","permalink":"http://example.com/tags/team-leaning/"},{"name":"back propagation","slug":"back-propagation","permalink":"http://example.com/tags/back-propagation/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://example.com/tags/PyTorch/"},{"name":"team learning","slug":"team-learning","permalink":"http://example.com/tags/team-learning/"},{"name":"note","slug":"note","permalink":"http://example.com/tags/note/"},{"name":"组队学习","slug":"组队学习","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"regression","slug":"regression","permalink":"http://example.com/tags/regression/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"预训练模型","slug":"预训练模型","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"transfomer","slug":"transfomer","permalink":"http://example.com/tags/transfomer/"},{"name":"总结","slug":"总结","permalink":"http://example.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"笔记","slug":"笔记","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"BERT","slug":"BERT","permalink":"http://example.com/tags/BERT/"},{"name":"文本分类","slug":"文本分类","permalink":"http://example.com/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"},{"name":"GPT","slug":"GPT","permalink":"http://example.com/tags/GPT/"},{"name":"attention","slug":"attention","permalink":"http://example.com/tags/attention/"},{"name":"CS公开课","slug":"CS公开课","permalink":"http://example.com/tags/CS%E5%85%AC%E5%BC%80%E8%AF%BE/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"function","slug":"function","permalink":"http://example.com/tags/function/"},{"name":"阅读","slug":"阅读","permalink":"http://example.com/tags/%E9%98%85%E8%AF%BB/"},{"name":"math","slug":"math","permalink":"http://example.com/tags/math/"},{"name":"文本表示","slug":"文本表示","permalink":"http://example.com/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"},{"name":"SQL","slug":"SQL","permalink":"http://example.com/tags/SQL/"},{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"},{"name":"大理","slug":"大理","permalink":"http://example.com/tags/%E5%A4%A7%E7%90%86/"},{"name":"思考","slug":"思考","permalink":"http://example.com/tags/%E6%80%9D%E8%80%83/"},{"name":"播客","slug":"播客","permalink":"http://example.com/tags/%E6%92%AD%E5%AE%A2/"},{"name":"laTeX","slug":"laTeX","permalink":"http://example.com/tags/laTeX/"},{"name":"简历","slug":"简历","permalink":"http://example.com/tags/%E7%AE%80%E5%8E%86/"},{"name":"统计","slug":"统计","permalink":"http://example.com/tags/%E7%BB%9F%E8%AE%A1/"}]}