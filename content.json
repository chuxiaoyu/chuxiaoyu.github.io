{"meta":{"title":"Memex","subtitle":"","description":"","author":"Xiaoyu CHU","url":"http://example.com","root":"/"},"pages":[{"title":"Categories","date":"2021-07-14T02:32:38.665Z","updated":"2021-07-14T02:32:38.665Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"About","date":"2021-10-13T11:27:05.806Z","updated":"2021-10-13T11:27:05.806Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"What I Believe Be the change you wish to see in the world. To be who you want to become, let go of who you think you are. Not everyone can become a great artist, but a great artist can come from anywhere. Donâ€™t let schooling interfere with your education. Talk is cheap, show me the code. Make it your ambition to lead a quiet life. å‘é„™æ˜¯å‘é„™è€…çš„é€šè¡Œè¯ï¼Œé«˜å°šæ˜¯é«˜å°šè€…çš„å¢“å¿—é“­ã€‚"},{"title":"Tags","date":"2021-07-14T02:32:38.667Z","updated":"2021-07-14T02:32:38.666Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Lecture01(elective) åå‘ä¼ æ’­ç®—æ³•","slug":"leeml-bp","date":"2021-10-15T06:55:13.000Z","updated":"2021-10-15T07:01:39.603Z","comments":true,"path":"leeml-bp/","link":"","permalink":"http://example.com/leeml-bp/","excerpt":"","text":"ï¼ˆTODOï¼‰ å‚è€ƒèµ„æ–™","categories":[{"name":"04 ç»„é˜Ÿå­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬30æœŸ æå®æ¯…æœºå™¨å­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬30æœŸ-æå®æ¯…æœºå™¨å­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"http://example.com/tags/machine-learning/"},{"name":"team leaning","slug":"team-leaning","permalink":"http://example.com/tags/team-leaning/"},{"name":"back propagation","slug":"back-propagation","permalink":"http://example.com/tags/back-propagation/"}]},{"title":"Chapter04 PyTorchåŸºç¡€å®æˆ˜â€”â€”FashionMNISTå›¾åƒåˆ†ç±»","slug":"pytorch-chap04","date":"2021-10-14T10:02:55.000Z","updated":"2021-10-14T10:09:21.110Z","comments":true,"path":"pytorch-chap04/","link":"","permalink":"http://example.com/pytorch-chap04/","excerpt":"","text":"(TODO) å‚è€ƒèµ„æ–™ Datawhaleå¼€æºé¡¹ç›®ï¼šæ·±å…¥æµ…å‡ºPyTorch https://github.com/datawhalechina/thorough-pytorch/ æå®æ¯…æœºå™¨å­¦ä¹ 2021æ˜¥-PyTorch Tutorial https://www.bilibili.com/video/BV1Wv411h7kN?p=5 åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ pytorchç‰ˆ https://zh-v2.d2l.ai/chapter_preface/index.html PyTorchå®˜æ–¹æ•™ç¨‹ä¸­æ–‡ç‰ˆ https://pytorch123.com/SecondSection/training_a_classifier/","categories":[{"name":"04 ç»„é˜Ÿå­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬30æœŸ æ·±å…¥æµ…å‡ºPyTorch","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬30æœŸ-æ·±å…¥æµ…å‡ºPyTorch","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAPyTorch/"}],"tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://example.com/tags/PyTorch/"},{"name":"team learning","slug":"team-learning","permalink":"http://example.com/tags/team-learning/"},{"name":"note","slug":"note","permalink":"http://example.com/tags/note/"}]},{"title":"Lecture01 æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç®€ä»‹","slug":"leeml-lec01-introduction","date":"2021-10-12T05:40:12.000Z","updated":"2021-10-15T06:58:56.176Z","comments":true,"path":"leeml-lec01-introduction/","link":"","permalink":"http://example.com/leeml-lec01-introduction/","excerpt":"","text":"å‰è¨€æœ¬æ–‡å­¦ä¹ çš„è¯¾ç¨‹æ˜¯ï¼šæå®æ¯…2021æ˜¥æœºå™¨å­¦ä¹ è¯¾ç¨‹ https://www.bilibili.com/video/BV1Wv411h7kN?p=2 è¯¾ç¨‹å¤§çº²ç¬¬ä¸€èŠ‚ Introduction ä½œä¸š HW1: Regressionç¬¬äºŒèŠ‚ Deep Learning ä½œä¸š HW2: Classificationç¬¬ä¸‰èŠ‚ Self-Attention ä½œä¸š HW3: CNN HW4: Self-Attentionç¬¬å››èŠ‚ Theory of MLç¬¬äº”èŠ‚ Transformer ä½œä¸š HW5: Transformerç¬¬å…­èŠ‚ Generative Model ä½œä¸š HW6: GANç¬¬ä¸ƒèŠ‚ Self-Supervised Learning ä½œä¸š HW7: BERT HW8: Autoencoderç¬¬å…«èŠ‚ Explainable AI / Adversarial Attack ä½œä¸š HW9: Explainable AI HW10: Adversarial Attackç¬¬ä¹èŠ‚ Domain Adaptation/ RL ä½œä¸š HW11: Adaptationç¬¬åèŠ‚ RL ä½œä¸š HW12: RLç¬¬åä¸€èŠ‚ Privacy v.s. MLç¬¬åäºŒèŠ‚ Quantum MLç¬¬åä¸‰èŠ‚ Life-Long/Compression ä½œä¸š HW13: Life-Long HW14: Compressionç¬¬åå››èŠ‚ Meta Learning ä½œä¸š HW15: Meta Learning Buffet Style Learning/è‡ªåŠ©å¼å­¦ä¹  Everyone can take this course. You decide how much you want to learn. You decide how deep you want to learn. æœºå™¨å­¦ä¹ åŸºæœ¬æ¦‚å¿µç®€ä»‹Machine Learning çº¦ç­‰äº Looking for Function ä¸åŒç±»å‹çš„å‡½æ•°Regression/å›å½’ï¼šå‡½æ•°çš„è¾“å‡ºæ˜¯æ•°å€¼ã€‚ Classification/åˆ†ç±»ï¼šç»™å®šé€‰é¡¹ï¼ˆç±»åˆ«ï¼‰ï¼Œè¾“å‡ºæ­£ç¡®çš„ç±»åˆ«ã€‚ Structured Learningï¼šcreate something with structure (image, document etc.) How to find a function: A caseCase: Predict YouTobe views æ¨¡å‹çš„è®­ç»ƒï¼š Step1. Function with Unknown ParametersåŸºäºé¢†åŸŸçŸ¥è¯†å†™å‡ºä¸€ä¸ªå¸¦æœ‰ä½ç½®å‚æ•°çš„å‡½æ•° y = b + wx1 w: weight, b: bias Step2. Define Loss from Training DataLoss is a function of parameters. L(b, w) Loss is how good a set of values is. Step3. Optimizationw*, b* = arg min L(w, b) Gradient Descent/æ¢¯åº¦ä¸‹é™ï¼ˆ1ä¸ªå‚æ•°ï¼Œå¤šå‚æ•°åŒç†ï¼‰ éšæœºé€‰å–ä¸€ä¸ªåˆå§‹ç‚¹w0 è®¡ç®—w0å¯¹Lçš„å¾®åˆ†ã€‚learning rate/å­¦ä¹ ç‡ã€‚ Negative: increase w Positive: decrease w Update w iteratively/è¿­ä»£æ›´æ–°w Training: Step1-3æ·±åº¦å­¦ä¹ åŸºæœ¬æ¦‚å¿µç®€ä»‹Regression/å›å½’ï¼Œæ¢¦å¼€å§‹çš„åœ°æ–¹å¦‚ä½•é€¼è¿‘å¤æ‚çš„æ›²çº¿ï¼Ÿâ€”â€”Sigmoid Function ç”¨å¤šæ®µsigmoidé€¼è¿‘å¤æ‚å‡½æ•°ï¼š ç»™sigmoidåŠ æ›´å¤šçš„featureï¼ˆå°±æ˜¯å¤šæ®µ y=b+wxç»„åˆèµ·æ¥ï¼‰ï¼š sigmoidçš„çŸ©é˜µè¿ç®—ï¼š Loss/æŸå¤±å‡½æ•°ä»€ä¹ˆæ˜¯æŸå¤±å‡½æ•°ï¼š ä¸€ä¸ªå…³äºå‚æ•°çš„å‡½æ•°L(ğœƒ) è¡¡é‡è¿™ä¸€ç»„æ¨¡å‹å‚æ•°çš„æ•ˆæœ Optimization/ä¼˜åŒ–å™¨ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°ï¼š ä¸€äº›æ¦‚å¿µï¼š Batch: æ•°æ®åŒ… Updateï¼šæ¯æ¬¡æ›´æ–°ä¸€æ¬¡å‚æ•° Epochï¼šæ‰€æœ‰batchè¿‡ä¸€é Activation function/æ¿€æ´»å‡½æ•° sigmoid ReLU Deep LearningA fancy name: Deep Learning. It is just so deep, right? Go deeper and deeper: Think: Why deep network, not fat network? (é€‰ä¿®)æ·±åº¦å­¦ä¹ ç®€ä»‹Deep Learningçš„å‘å±•å†ç¨‹ Deep Learningçš„ä¸‰ä¸ªæ­¥éª¤ Full Connect Feedforward Network/å…¨è¿æ¥å‰é¦ˆç¥ç»ç½‘ç»œ Matrix Operation/çŸ©é˜µè¿ç®—å‚æ•°çŸ©é˜µï¼š Output Layer Example Applicationæ‰‹å†™è¯†åˆ«æ¡ˆä¾‹ï¼š You need to decide the network structure to let a good function in your function set. Lossè®¡ç®—æŸå¤±å‡½æ•°ï¼š æ‰¾ä¸€ä¸ªæœ€å°åŒ–æŸå¤±å‡½æ•°çš„å‡½æ•°ï¼Œå³æ‰¾ä¸€ç»„å‚æ•°å¯ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼š Gradient Descentå¦‚ä½•å¯»æ‰¾è¿™æ ·çš„å‚æ•°ï¼Ÿâ€”â€”æ¢¯åº¦ä¸‹é™ã€‚ Backpropagation/åå‘ä¼ æ’­å¦‚ä½•è®¡ç®—æ¢¯åº¦ï¼Ÿâ€”â€”åå‘ä¼ æ’­ç®—æ³•ã€‚ å‚è€ƒèµ„æ–™ æå®æ¯…2021æ˜¥æœºå™¨å­¦ä¹ è¯¾ç¨‹ https://www.bilibili.com/video/BV1Wv411h7kN?p=2","categories":[{"name":"04 ç»„é˜Ÿå­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬30æœŸ æå®æ¯…æœºå™¨å­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬30æœŸ-æå®æ¯…æœºå™¨å­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"ç»„é˜Ÿå­¦ä¹ ","slug":"ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://example.com/tags/machine-learning/"},{"name":"regression","slug":"regression","permalink":"http://example.com/tags/regression/"}]},{"title":"Summary Transformerè¯¾ç¨‹æ€»ç»“","slug":"nlp-transformer-summary","date":"2021-09-30T07:44:01.000Z","updated":"2021-10-15T07:00:47.781Z","comments":true,"path":"nlp-transformer-summary/","link":"","permalink":"http://example.com/nlp-transformer-summary/","excerpt":"","text":"æˆ‘çš„èƒŒæ™¯ç¬¬ä¸€æ¬¡å‚åŠ Datawhaleç»„é˜Ÿå­¦ä¹ è¯¾ç¨‹ï¼Œæˆ‘çš„ç›¸å…³çŸ¥è¯†èƒŒæ™¯æ˜¯ï¼š Transformerï¼š0åŸºç¡€ PyTorchï¼š0åŸºç¡€ NLPï¼š0.1åŸºç¡€ Pythonï¼š0åŸºç¡€ ä½œä¸ºNLPæƒ…æ„Ÿåˆ†æçš„é¢†èˆªå‘˜å’ŒTransformersçš„å­¦å‘˜ï¼Œæˆ‘å°†ä»è¯¾ç¨‹å†…å®¹å’Œè¿è¥ä¸¤æ–¹é¢å†™ä¸€ä¸‹è‡ªå·±çš„æ„Ÿå—å’Œæƒ³æ³•ã€‚ Transformerè¯¾ç¨‹å¤§çº² è¯¾ç¨‹å†…å®¹æ–¹é¢ è¯¾ç¨‹å†…å®¹å¯¹é›¶åŸºç¡€å…¥é—¨çš„äººæ˜¯æ¯”è¾ƒå‹å¥½çš„ã€‚æ¯”å¦‚æˆ‘æ˜¯ç¬¬ä¸€æ¬¡å­¦ä¹ Transformerï¼Œä½†æ˜¯å›¾è§£ç³»åˆ—å¾ˆå®¹æ˜“ç†è§£ã€‚ ä½†æ˜¯æ¯ä¸ªtaskçš„å†…å®¹éš¾åº¦å·®åˆ«è¿‡å¤§ã€‚æ¯”å¦‚Task02çš„TransformeråŸè®ºæ–‡ä»£ç æ ‡æ³¨ ï¼Œå’ŒTask05 transformersæºç è®²è§£ã€‚ æ¯ä¸ªtaskçš„å·¥ä½œé‡ä¸å¹³è¡¡ï¼Œæœ‰çš„ç‰¹åˆ«å¤šï¼Œæœ‰çš„ç›¸å¯¹å°‘ã€‚ ç¬¬å››ç« å¯ä»¥ä»»é€‰ä¸€ä¸ªä»»åŠ¡åº”ç”¨ï¼ŒæŠŠæ›´å¤šæ—¶é—´ç•™ç»™å­¦ä¹ å¦‚ä½•ä½¿ç”¨huggingfaceçš„transformersã€‚ï¼ˆå°±æ˜¯é‚£ä¸ªå®˜æ–¹è¯¾ç¨‹ï¼‰ è¯¾ç¨‹è¿è¥æ–¹é¢ ä¼˜ç§€é˜Ÿå‘˜å’Œä¼˜ç§€é˜Ÿé•¿è¯„é€‰æ ‡å‡†éœ€è¦ç»Ÿä¸€ è¡¥å¡è§„åˆ™éœ€è¦ç»Ÿä¸€ é€æ­¥å®Œå–„è¯¾ç¨‹ä½“ç³» å°ç¨‹åºï¼Œç”¨èµ·æ¥ä¸æ˜¯å¾ˆæ–¹ä¾¿ ï¼ˆè„‘æ´1ï¼‰æ¯æ¬¡æ‰“å¡ä¹‹åé©¬ä¸Šè¿›è¡Œä½œä¸šè¯„å®¡å’Œåé¦ˆï¼ˆè¿‡äºè€—è´¹åŠ©æ•™ç²¾åŠ›ï¼‰ ï¼ˆè„‘æ´2ï¼‰ç»™æ¯ä¸ªå°ç»„æˆ–ä¸ªäººå®‰æ’ä¸€ä¸ªæœŸæœ«å¤§projectï¼Œæˆ–è€…å¸ƒç½®å¹³æ—¶ä½œä¸šï¼ˆè¿‡äºè€—è´¹å­¦å‘˜ç²¾åŠ›ï¼‰ ï¼ˆè„‘æ´3ï¼‰ç›´æ¥ç”¨ä¸€ä¸ªè¯¾ç¨‹ç®¡ç†ç³»ç»Ÿè¿›è¡Œç®¡ç†ï¼ˆç±»ä¼¼äºcanvas,é›¨è¯¾å ‚ï¼‰(é€æ¸å­¦é™¢åŒ–) å‚è€ƒèµ„æ–™æ¸…å•ï¼ˆæ€»ï¼‰Transformeråœ¨ç½‘ä¸Šæœ‰å¾ˆå¤šå¾ˆå¤šæ•™ç¨‹ï¼Œå…¶ä¸­å…¬è®¤çš„ã€æ™®éæ€§çš„æ¯”è¾ƒå¥½çš„èµ„æ–™å¦‚ä¸‹ï¼š ç†è®ºéƒ¨åˆ†[1] (å¼ºæ¨)æå®æ¯…2021æ˜¥æœºå™¨å­¦ä¹ è¯¾ç¨‹ https://www.bilibili.com/video/BV1Wv411h7kN?from=search&amp;seid=17090062977285779802&amp;spm_id_from=333.337.0.0[2] åŸºäºtransformersçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨ï¼ˆæ¶µç›–äº†å›¾è§£ç³»åˆ—ã€annotated transformerã€huggingfaceï¼‰ https://github.com/datawhalechina/learn-nlp-with-transformers[3] å›¾è§£transformer|The Illustrated Transformer http://jalammar.github.io/illustrated-transformer/[4] å›¾è§£seq2seq, attention|Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ ä»£ç éƒ¨åˆ†[5] The Annotated Transformer http://nlp.seas.harvard.edu//2018/04/03/attention.html[6] Huggingface/transformers https://github.com/huggingface/transformers/blob/master/README_zh-hans.md è®ºæ–‡éƒ¨åˆ†Attention is all â€œweâ€ need. å…¶ä»–ä¸é”™çš„åšå®¢æˆ–æ•™ç¨‹[7] åŸºäºtransformersçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨â€“åœ¨çº¿é˜…è¯» https://datawhalechina.github.io/learn-nlp-with-transformers/#/[8] æå®æ¯…2021æ˜¥æœºå™¨å­¦ä¹ è¯¾ç¨‹ç¬”è®°â€”â€”è‡ªæ³¨æ„åŠ›æœºåˆ¶ https://www.cnblogs.com/sykline/p/14730088.html[9] æå®æ¯…2021æ˜¥æœºå™¨å­¦ä¹ è¯¾ç¨‹ç¬”è®°â€”â€”Transformeræ¨¡å‹ https://www.cnblogs.com/sykline/p/14785552.html[10] æå®æ¯…æœºå™¨å­¦ä¹ å­¦ä¹ ç¬”è®°â€”â€”è‡ªæ³¨æ„åŠ›æœºåˆ¶ https://blog.csdn.net/p_memory/article/details/116271274[11] è½¦ä¸‡ç¿”-è‡ªç„¶è¯­è¨€å¤„ç†æ–°èŒƒå¼ï¼šåŸºäºé¢„è®­ç»ƒçš„æ–¹æ³•ã€è®²åº§+PPTã€‘ https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true[12] è‹å‰‘æ—-ã€ŠAttention is All You Needã€‹æµ…è¯»ï¼ˆç®€ä»‹+ä»£ç ï¼‰https://spaces.ac.cn/archives/4765","categories":[{"name":"04 ç»„é˜Ÿå­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬29æœŸ åŸºäºtransformerçš„NLP","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬29æœŸ-åŸºäºtransformerçš„NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"ç»„é˜Ÿå­¦ä¹ ","slug":"ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"é¢„è®­ç»ƒæ¨¡å‹","slug":"é¢„è®­ç»ƒæ¨¡å‹","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"transfomer","slug":"transfomer","permalink":"http://example.com/tags/transfomer/"},{"name":"æ€»ç»“","slug":"æ€»ç»“","permalink":"http://example.com/tags/%E6%80%BB%E7%BB%93/"}]},{"title":"Chapter03 PyTorchçš„ä¸»è¦ç»„æˆæ¨¡å—","slug":"pytorch-chap03","date":"2021-09-28T01:31:42.000Z","updated":"2021-10-14T10:04:01.506Z","comments":true,"path":"pytorch-chap03/","link":"","permalink":"http://example.com/pytorch-chap03/","excerpt":"","text":"ç¬¬ä¸‰ç«  PyTorchçš„ä¸»è¦ç»„æˆæ¨¡å—å®Œæˆæ·±åº¦å­¦ä¹ çš„å¿…è¦éƒ¨åˆ†æœºå™¨å­¦ä¹ ï¼š æ•°æ®é¢„å¤„ç†ï¼ˆæ•°æ®æ ¼å¼ã€æ•°æ®è½¬æ¢ã€åˆ’åˆ†æ•°æ®é›†ï¼‰ é€‰æ‹©æ¨¡å‹ï¼Œè®¾å®šæŸå¤±å’Œä¼˜åŒ–å‡½æ•°ï¼Œè®¾ç½®è¶…å‚æ•° è®­ç»ƒæ¨¡å‹ï¼Œæ‹Ÿåˆè®­ç»ƒé›† è¯„ä¼°æ¨¡å‹ï¼Œåœ¨å¹¶åœ¨éªŒè¯é›†/æµ‹è¯•é›†ä¸Šè®¡ç®—æ¨¡å‹è¡¨ç° æ·±åº¦å­¦ä¹ çš„æ³¨æ„äº‹é¡¹ï¼š æ•°æ®é¢„å¤„ç†ï¼ˆæ•°æ®åŠ è½½ã€æ‰¹å¤„ç†ï¼‰ é€å±‚æ­å»ºæ¨¡å‹ï¼Œç»„è£…ä¸åŒæ¨¡å— GPUçš„é…ç½®å’Œæ“ä½œ åŸºæœ¬é…ç½®å¯¼å…¥å¿…é¡»çš„åŒ…ï¼š 123456import osimport numpy as npimport torchimport torch.nn as nnfrom torch.utils.data import Dataset, DataLoaderimport torch.optim as optimizer è¶…å‚æ•°è®¾ç½®ï¼š 123batch_size = 16 # batch sizelr = 1e-4 # åˆå§‹å­¦ä¹ ç‡max_epochs = 100 # è®­ç»ƒæ¬¡æ•° GPUçš„è®¾ç½®ï¼š 12345# æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨os.environï¼Œè¿™ç§æƒ…å†µå¦‚æœä½¿ç”¨GPUä¸éœ€è¦è®¾ç½®os.environ[&#x27;CUDA_VISIBLE_DEVICES&#x27;] = &#x27;0,1&#x27;# æ–¹æ¡ˆäºŒï¼šä½¿ç”¨â€œdeviceâ€ï¼Œåç»­å¯¹è¦ä½¿ç”¨GPUçš„å˜é‡ç”¨.to(device)å³å¯device = torch.device(&quot;cuda:1&quot; if torch.cuda.is_available() else &quot;cpu&quot;) æ•°æ®åŠ è½½å’Œå¤„ç†PyTorchæ•°æ®è¯»å…¥æ˜¯é€šè¿‡Dataset+Dataloaderçš„æ–¹å¼å®Œæˆçš„ï¼ŒDatasetå®šä¹‰å¥½æ•°æ®çš„æ ¼å¼å’Œæ•°æ®å˜æ¢å½¢å¼ï¼ŒDataloaderç”¨iterativeçš„æ–¹å¼ä¸æ–­è¯»å…¥æ‰¹æ¬¡æ•°æ®ã€‚ æˆ‘ä»¬å¯ä»¥å®šä¹‰è‡ªå·±çš„Datasetç±»æ¥å®ç°çµæ´»çš„æ•°æ®è¯»å–ï¼Œå®šä¹‰çš„ç±»éœ€è¦ç»§æ‰¿PyTorchè‡ªèº«çš„Datasetç±»ã€‚ä¸»è¦åŒ…å«ä¸‰ä¸ªå‡½æ•°ï¼š __init__: ç”¨äºå‘ç±»ä¸­ä¼ å…¥å¤–éƒ¨å‚æ•°ï¼ŒåŒæ—¶å®šä¹‰æ ·æœ¬é›† __getitem__: ç”¨äºé€ä¸ªè¯»å–æ ·æœ¬é›†åˆä¸­çš„å…ƒç´ ï¼Œå¯ä»¥è¿›è¡Œä¸€å®šçš„å˜æ¢ï¼Œå¹¶å°†è¿”å›è®­ç»ƒ/éªŒè¯æ‰€éœ€çš„æ•°æ® __len__: ç”¨äºè¿”å›æ•°æ®é›†çš„æ ·æœ¬æ•° batch_sizeï¼šæ ·æœ¬æ˜¯æŒ‰â€œæ‰¹â€è¯»å…¥çš„ï¼Œbatch_sizeå°±æ˜¯æ¯æ¬¡è¯»å…¥çš„æ ·æœ¬æ•° num_workersï¼šæœ‰å¤šå°‘ä¸ªè¿›ç¨‹ç”¨äºè¯»å–æ•°æ® shuffleï¼šæ˜¯å¦å°†è¯»å…¥çš„æ•°æ®æ‰“ä¹± drop_lastï¼šå¯¹äºæ ·æœ¬æœ€åä¸€éƒ¨åˆ†æ²¡æœ‰è¾¾åˆ°æ‰¹æ¬¡æ•°çš„æ ·æœ¬ï¼Œä¸å†å‚ä¸è®­ç»ƒ ä¸‹é¢æ˜¯æœ¬éƒ¨åˆ†ä»£ç åœ¨notebookä¸­çš„è¿è¡Œæƒ…å†µã€‚ä¸»è¦å‚è€ƒ PyTorchå®˜æ–¹æ•™ç¨‹ä¸­æ–‡ç‰ˆ https://pytorch123.com/SecondSection/training_a_classifier/ æ¨¡å‹æ„å»ºç¥ç»ç½‘ç»œçš„æ„é€ PyTorchä¸­ç¥ç»ç½‘ç»œæ„é€ ä¸€èˆ¬æ˜¯åŸºäº Module ç±»çš„æ¨¡å‹æ¥å®Œæˆçš„ã€‚Module ç±»æ˜¯ nn æ¨¡å—ï§©æä¾›çš„ä¸€ä¸ªæ¨¡å‹æ„é€ ç±»ï¼Œæ˜¯æ‰€æœ‰ç¥ç»â½¹ç½‘ç»œæ¨¡å—çš„åŸºç±»ï¼Œæˆ‘ä»¬å¯ä»¥ç»§æ‰¿å®ƒæ¥å®šä¹‰æˆ‘ä»¬æƒ³è¦çš„æ¨¡å‹ã€‚ä¸‹é¢ç»§æ‰¿ Module ç±»æ„é€ å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ã€‚ 12345678910111213141516import torchfrom torch import nnclass MLP(nn.Module): # å£°æ˜å¸¦æœ‰æ¨¡å‹å‚æ•°çš„å±‚ï¼Œè¿™é‡Œå£°æ˜äº†ä¸¤ä¸ªå…¨è¿æ¥å±‚ def __init__(self, **kwargs): # è°ƒç”¨MLPçˆ¶ç±»Blockçš„æ„é€ å‡½æ•°æ¥è¿›è¡Œå¿…è¦çš„åˆå§‹åŒ–ã€‚è¿™æ ·åœ¨æ„é€ å®ä¾‹ä¾‹æ—¶è¿˜å¯ä»¥æŒ‡å®šå…¶ä»–å‡½æ•° super(MLP, self).__init__(**kwargs) self.hidden = nn.Linear(784, 256) self.act = nn.ReLU() self.output = nn.Linear(256,10) # å®šä¹‰æ¨¡å‹çš„å‰å‘è®¡ç®—ï¼Œå³å¦‚ä½•æ ¹æ®è¾“å…¥xè®¡ç®—è¿”å›æ‰€éœ€è¦çš„æ¨¡å‹è¾“å‡º def forward(self, x): o = self.act(self.hidden(x)) return self.output(o) æˆ‘ä»¬å¯ä»¥å®ä¾‹åŒ– MLP ç±»å¾—åˆ°æ¨¡å‹å˜ï¥¾ net ã€‚ä¸‹â¾¯çš„ä»£ç åˆå§‹åŒ– net å¹¶ä¼ å…¥è¾“â¼Šæ•°æ® X åšä¸€æ¬¡å‰å‘è®¡ç®—ã€‚å…¶ä¸­ï¼Œ net(X) ä¼šè°ƒç”¨ MLP ç»§æ‰¿â¾ƒè‡ª Module ç±»çš„ call å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°å°†è°ƒâ½¤ç”¨ MLP ç±»å®šä¹‰çš„forward å‡½æ•°æ¥å®Œæˆå‰å‘è®¡ç®—ã€‚ 1234567891011121314151617181920212223242526272829303132333435&gt;&gt;&gt; import torch&gt;&gt;&gt; X = torch.rand(2, 784)&gt;&gt;&gt; Xtensor([[0.3277, 0.2204, 0.5239, ..., 0.4333, 0.1906, 0.1318], [0.9850, 0.2121, 0.8405, ..., 0.3796, 0.2717, 0.5553]])&gt;&gt;&gt; from torch import nn&gt;&gt;&gt; &gt;&gt;&gt; class MLP(nn.Module):... # å£°æ˜å¸¦æœ‰æ¨¡å‹å‚æ•°çš„å±‚ï¼Œè¿™é‡Œå£°æ˜äº†ä¸¤ä¸ªå…¨è¿æ¥å±‚... def __init__(self, **kwargs):... # è°ƒç”¨MLPçˆ¶ç±»Blockçš„æ„é€ å‡½æ•°æ¥è¿›è¡Œå¿…è¦çš„åˆå§‹åŒ–ã€‚è¿™æ ·åœ¨æ„é€ å®ä¾‹ä¾‹æ—¶è¿˜å¯ä»¥æŒ‡å®šå…¶ä»–å‡½æ•°... super(MLP, self).__init__(**kwargs)... self.hidden = nn.Linear(784, 256)... self.act = nn.ReLU()... self.output = nn.Linear(256,10)... ... # å®šä¹‰æ¨¡å‹çš„å‰å‘è®¡ç®—ï¼Œå³å¦‚ä½•æ ¹æ®è¾“å…¥xè®¡ç®—è¿”å›æ‰€éœ€è¦çš„æ¨¡å‹è¾“å‡º... def forward(self, x):... o = self.act(self.hidden(x))... return self.output(o)... &gt;&gt;&gt; net = MLP()&gt;&gt;&gt; netMLP( (hidden): Linear(in_features=784, out_features=256, bias=True) (act): ReLU() (output): Linear(in_features=256, out_features=10, bias=True))&gt;&gt;&gt; net(X)tensor([[ 0.1317, 0.0702, 0.1707, -0.0081, -0.2730, 0.2837, 0.0700, 0.1718, 0.0299, 0.2082], [ 0.1094, 0.0936, 0.2474, -0.0139, -0.1861, 0.1846, 0.1658, 0.2051, 0.2609, 0.2227]], grad_fn=&lt;AddmmBackward&gt;)&gt;&gt;&gt; ç¥ç»ç½‘ç»œä¸­å¸¸è§çš„å±‚ä¸å«æ¨¡å‹å‚æ•°çš„å±‚ä¸‹â¾¯æ„é€ çš„ MyLayer ç±»é€šè¿‡ç»§æ‰¿ Module ç±»è‡ªå®šä¹‰ï¦ºä¸€ä¸ªå°†è¾“å…¥å‡æ‰å‡å€¼åè¾“å‡ºçš„å±‚ã€‚è¿™ä¸ªå±‚ï§©ï¥§å«æ¨¡å‹å‚æ•°ã€‚ 1234567891011121314&gt;&gt;&gt; import torch&gt;&gt;&gt; from torch import nn&gt;&gt;&gt; &gt;&gt;&gt; class MyLayer(nn.Module):... def __init__(self, **kwargs):... super(MyLayer, self).__init__(**kwargs)... def forward(self, x):... return x - x.mean() ... &gt;&gt;&gt; layer = MyLayer() # å®ä¾‹åŒ–è¯¥å±‚&gt;&gt;&gt; layerMyLayer()&gt;&gt;&gt; layer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))tensor([-2., -1., 0., 1., 2.]) å«æ¨¡å‹å‚æ•°çš„å±‚æˆ‘ä»¬è¿˜å¯ä»¥è‡ªå®šä¹‰å«æ¨¡å‹å‚æ•°çš„è‡ªå®šä¹‰å±‚ã€‚å…¶ä¸­çš„æ¨¡å‹å‚æ•°å¯ä»¥é€šè¿‡è®­ç»ƒå­¦å‡ºã€‚ Parameter ç±»å…¶å®æ˜¯ Tensor çš„å­ç±»ï¼Œå¦‚æœä¸€ ä¸ª Tensor æ˜¯ Parameter ï¼Œé‚£ä¹ˆå®ƒä¼šâ¾ƒåŠ¨è¢«æ·»åŠ åˆ°æ¨¡å‹çš„å‚æ•°ï¦œè¡¨ï§©ã€‚æ‰€ä»¥åœ¨â¾ƒå®šä¹‰å«æ¨¡å‹å‚æ•°çš„å±‚æ—¶ï¼Œæˆ‘ä»¬åº”è¯¥å°†å‚æ•°å®šä¹‰æˆ Parameter ï¼Œé™¤äº†ç›´æ¥å®šä¹‰æˆ Parameter ç±»å¤–ï¼Œè¿˜å¯ä»¥ä½¿â½¤ ParameterList å’Œ ParameterDict åˆ†åˆ«å®šä¹‰å‚æ•°çš„ï¦œè¡¨å’Œå­—å…¸ã€‚ 123456789101112131415161718192021class MyListDense(nn.Module): def __init__(self): super(MyListDense, self).__init__() self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)]) self.params.append(nn.Parameter(torch.randn(4, 1))) def forward(self, x): for i in range(len(self.params)): x = torch.mm(x, self.params[i]) return x &gt;&gt;&gt; net = MyListDense()&gt;&gt;&gt; print(net)MyListDense( (params): ParameterList( (0): Parameter containing: [torch.FloatTensor of size 4x4] (1): Parameter containing: [torch.FloatTensor of size 4x4] (2): Parameter containing: [torch.FloatTensor of size 4x4] (3): Parameter containing: [torch.FloatTensor of size 4x1] )) 123456789101112131415161718192021class MyDictDense(nn.Module): def __init__(self): super(MyDictDense, self).__init__() self.params = nn.ParameterDict(&#123; &#x27;linear1&#x27;: nn.Parameter(torch.randn(4, 4)), &#x27;linear2&#x27;: nn.Parameter(torch.randn(4, 1)) &#125;) self.params.update(&#123;&#x27;linear3&#x27;: nn.Parameter(torch.randn(4, 2))&#125;) # æ–°å¢ def forward(self, x, choice=&#x27;linear1&#x27;): return torch.mm(x, self.params[choice])&gt;&gt;&gt; net = MyDictDense()&gt;&gt;&gt; print(net)MyDictDense( (params): ParameterDict( (linear1): Parameter containing: [torch.FloatTensor of size 4x4] (linear2): Parameter containing: [torch.FloatTensor of size 4x1] (linear3): Parameter containing: [torch.FloatTensor of size 4x2] )) ä¸‹é¢ç»™å‡ºå¸¸è§çš„ç¥ç»ç½‘ç»œçš„ä¸€äº›å±‚ï¼Œæ¯”å¦‚å·ç§¯å±‚ã€æ± åŒ–å±‚ï¼Œä»¥åŠè¾ƒä¸ºåŸºç¡€çš„AlexNetï¼ŒLeNetç­‰ã€‚ äºŒç»´å·ç§¯å±‚äºŒç»´å·ç§¯å±‚å°†è¾“å…¥å’Œå·ç§¯æ ¸åšäº’ç›¸å…³è¿ç®—ï¼Œå¹¶åŠ ä¸Šä¸€ä¸ªæ ‡ï¥¾åå·®æ¥å¾—åˆ°è¾“å‡ºã€‚ 12345678910111213141516171819202122import torchfrom torch import nn# å·ç§¯è¿ç®—ï¼ˆäºŒç»´äº’ç›¸å…³ï¼‰def corr2d(X, K): h, w = K.shape X, K = X.float(), K.float() Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i, j] = (X[i: i + h, j: j + w] * K).sum() return Y# äºŒç»´å·ç§¯å±‚class Conv2D(nn.Module): def __init__(self, kernel_size): super(Conv2D, self).__init__() self.weight = nn.Parameter(torch.randn(kernel_size)) self.bias = nn.Parameter(torch.randn(1)) def forward(self, x): return corr2d(x, self.weight) + self.bias å¡«å……(padding)æ˜¯æŒ‡åœ¨è¾“â¼Šå…¥â¾¼é«˜å’Œå®½çš„ä¸¤ä¾§å¡«å……å…ƒç´ (é€šå¸¸æ˜¯0å…ƒç´ )ã€‚ åœ¨äºŒç»´äº’ç›¸å…³è¿ç®—ä¸­ï¼Œå·ç§¯çª—å£ä»è¾“å…¥æ•°ç»„çš„æœ€å·¦ä¸Šæ–¹å¼€å§‹ï¼ŒæŒ‰ä»å·¦å¾€å³ã€ä»ä¸Šå¾€ä¸‹ çš„é¡ºåºï¼Œä¾æ¬¡åœ¨è¾“â¼Šæ•°ç»„ä¸Šæ»‘åŠ¨ã€‚æˆ‘ä»¬å°†æ¯æ¬¡æ»‘åŠ¨çš„ï¨ˆæ•°å’Œï¦œæ•°ç§°ä¸ºæ­¥å¹…(stride)ã€‚ ï¼ˆskipï¼‰ æ± åŒ–å±‚æ± åŒ–å±‚æ¯æ¬¡å¯¹è¾“å…¥æ•°æ®çš„ä¸€ä¸ªå›ºå®šå½¢çŠ¶çª—å£(â¼œç§°æ± åŒ–çª—å£)ä¸­çš„å…ƒç´ è®¡ç®—è¾“å‡ºã€‚ï¥§åŒäºå·ç§¯å±‚ï§©è®¡ç®—è¾“â¼Šå’Œæ ¸çš„äº’ç›¸å…³æ€§ï¼Œæ± åŒ–å±‚ç›´æ¥è®¡ç®—æ± åŒ–çª—å£å†…å…ƒç´ çš„æœ€å¤§å€¼æˆ–è€…å¹³å‡å€¼ã€‚è¯¥è¿ç®—ä¹Ÿ åˆ†åˆ«å«åšæœ€å¤§æ± åŒ–æˆ–å¹³å‡æ± åŒ–ã€‚ 12345678910111213141516171819&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; import torch&gt;&gt;&gt; from torch import nn&gt;&gt;&gt; &gt;&gt;&gt; def pool2d(X, pool_size, mode=&#x27;max&#x27;):... p_h, p_w = pool_size... Y = np.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))... for i in range(Y.shape[0]):... for j in range(Y.shape[1]):... if mode == &#x27;max&#x27;:... Y[i, j] = X[i: i + p_h, j: j + p_w].max()... elif mode == &#x27;avg&#x27;:... Y[i, j] = X[i: i + p_h, j: j + p_w].mean()... return Y... &gt;&gt;&gt; X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])&gt;&gt;&gt; pool2d(X, (2, 2))array([[4., 5.], [7., 8.]]) æ¨¡å‹ç¤ºä¾‹ï¼šLeNetï¼ˆå¾…è¡¥å……ï¼‰ æ¨¡å‹ç¤ºä¾‹ï¼šAlexNetï¼ˆå¾…è¡¥å……ï¼‰ æŸå¤±å‡½æ•°ä¸€ä¸ªå¥½çš„è®­ç»ƒç¦»ä¸å¼€ä¼˜è´¨çš„è´Ÿåé¦ˆï¼Œè¿™é‡Œçš„æŸå¤±å‡½æ•°å°±æ˜¯æ¨¡å‹çš„è´Ÿåé¦ˆã€‚ è¿™é‡Œå°†åˆ—å‡ºPyTorchä¸­å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼ˆä¸€èˆ¬é€šè¿‡torch.nnè°ƒç”¨ï¼‰ï¼Œå¹¶è¯¦ç»†ä»‹ç»æ¯ä¸ªæŸå¤±å‡½æ•°çš„åŠŸèƒ½ä»‹ç»ã€æ•°å­¦å…¬å¼å’Œè°ƒç”¨ä»£ç ã€‚ äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±å‡½æ•°torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;) åŠŸèƒ½ï¼šè®¡ç®—äºŒåˆ†ç±»ä»»åŠ¡æ—¶çš„äº¤å‰ç†µï¼ˆCross Entropyï¼‰å‡½æ•°ã€‚åœ¨äºŒåˆ†ç±»ä¸­ï¼Œlabelæ˜¯{0,1}ã€‚å¯¹äºè¿›å…¥äº¤å‰ç†µå‡½æ•°çš„inputä¸ºæ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œinputä¸ºsigmoidæ¿€æ´»å±‚çš„è¾“å‡ºï¼Œæˆ–è€…softmaxçš„è¾“å‡ºã€‚ ä¸»è¦å‚æ•°ï¼š weight:æ¯ä¸ªç±»åˆ«çš„lossè®¾ç½®æƒå€¼ size_average:æ•°æ®ä¸ºboolï¼Œä¸ºTrueæ—¶ï¼Œè¿”å›çš„lossä¸ºå¹³å‡å€¼ï¼›ä¸ºFalseæ—¶ï¼Œè¿”å›çš„å„æ ·æœ¬çš„lossä¹‹å’Œ. reduce:æ•°æ®ç±»å‹ä¸ºboolï¼Œä¸ºTrueæ—¶ï¼Œlossçš„è¿”å›æ˜¯æ ‡é‡ã€‚ å…¶ä»–æŸå¤±å‡½æ•°äº¤å‰ç†µæŸå¤±å‡½æ•° L1æŸå¤±å‡½æ•° MSEæŸå¤±å‡½æ•° å¹³æ»‘L1 (Smooth L1)æŸå¤±å‡½æ•° ç›®æ ‡æ³Šæ¾åˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± KLæ•£åº¦ ä¼˜åŒ–å™¨ä»€ä¹ˆæ˜¯ä¼˜åŒ–å™¨æ·±åº¦å­¦ä¹ çš„ç›®æ ‡æ˜¯é€šè¿‡ä¸æ–­æ”¹å˜ç½‘ç»œå‚æ•°ï¼Œä½¿å¾—å‚æ•°èƒ½å¤Ÿå¯¹è¾“å…¥åšå„ç§éçº¿æ€§å˜æ¢æ‹Ÿåˆè¾“å‡ºï¼Œæœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªå‡½æ•°å»å¯»æ‰¾æœ€ä¼˜è§£ï¼Œåªä¸è¿‡è¿™ä¸ªæœ€ä¼˜è§£ä½¿ä¸€ä¸ªçŸ©é˜µã€‚é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•è®¡ç®—å‡ºæ¥è¿™ä¹ˆå¤šçš„ç³»æ•°ï¼Œæœ‰ä»¥ä¸‹ä¸¤ç§æ–¹æ³•ï¼š ç¬¬ä¸€ç§æ˜¯æœ€ç›´æ¥çš„æš´åŠ›ç©·ä¸¾ä¸€éå‚æ•°ï¼Œè¿™ç§æ–¹æ³•çš„å®æ–½å¯èƒ½æ€§åŸºæœ¬ä¸º0ï¼Œå ªæ¯”æ„šå…¬ç§»å±±plusçš„éš¾åº¦ã€‚ ä¸ºäº†ä½¿æ±‚è§£å‚æ•°è¿‡ç¨‹æ›´åŠ å¿«ï¼Œäººä»¬æå‡ºäº†ç¬¬äºŒç§åŠæ³•ï¼Œå³å°±æ˜¯æ˜¯BP+ä¼˜åŒ–å™¨é€¼è¿‘æ±‚è§£ã€‚ å› æ­¤ï¼Œä¼˜åŒ–å™¨å°±æ˜¯æ ¹æ®ç½‘ç»œåå‘ä¼ æ’­çš„æ¢¯åº¦ä¿¡æ¯æ¥æ›´æ–°ç½‘ç»œçš„å‚æ•°ï¼Œä»¥èµ·åˆ°é™ä½losså‡½æ•°è®¡ç®—å€¼ï¼Œä½¿å¾—æ¨¡å‹è¾“å‡ºæ›´åŠ æ¥è¿‘çœŸå®æ ‡ç­¾ã€‚ PyTorchæä¾›çš„ä¼˜åŒ–å™¨Pytorchå¾ˆäººæ€§åŒ–çš„ç»™æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä¼˜åŒ–å™¨çš„åº“torch.optimï¼Œåœ¨è¿™é‡Œé¢ç»™æˆ‘ä»¬æä¾›äº†åç§ä¼˜åŒ–å™¨ã€‚ torch.optim.ASGD torch.optim.Adadelta torch.optim.Adagrad torch.optim.Adam torch.optim.AdamW torch.optim.Adamax torch.optim.LBFGS torch.optim.RMSprop torch.optim.Rprop torch.optim.SGD torch.optim.SparseAdam è®­ç»ƒä¸è¯„ä¼°å®Œæˆäº†ä¸Šè¿°è®¾å®šåå°±å¯ä»¥åŠ è½½æ•°æ®å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ã€‚é¦–å…ˆåº”è¯¥è®¾ç½®æ¨¡å‹çš„çŠ¶æ€ï¼šå¦‚æœæ˜¯è®­ç»ƒçŠ¶æ€ï¼Œé‚£ä¹ˆæ¨¡å‹çš„å‚æ•°åº”è¯¥æ”¯æŒåå‘ä¼ æ’­çš„ä¿®æ”¹ï¼›å¦‚æœæ˜¯éªŒè¯/æµ‹è¯•çŠ¶æ€ï¼Œåˆ™ä¸åº”è¯¥ä¿®æ”¹æ¨¡å‹å‚æ•°ã€‚ 12model.train() # è®­ç»ƒçŠ¶æ€model.eval() # éªŒè¯/æµ‹è¯•çŠ¶æ€ è®­ç»ƒè¿‡ç¨‹ï¼š 12345678910111213def train(epoch): model.train() train_loss = 0 for data, label in train_loader: # æ­¤æ—¶è¦ç”¨forå¾ªç¯è¯»å–DataLoaderä¸­çš„å…¨éƒ¨æ•°æ®ã€‚ data, label = data.cuda(), label.cuda() # ä¹‹åå°†æ•°æ®æ”¾åˆ°GPUä¸Šç”¨äºåç»­è®¡ç®—ï¼Œæ­¤å¤„ä»¥.cuda()ä¸ºä¾‹ optimizer.zero_grad() # å¼€å§‹ç”¨å½“å‰æ‰¹æ¬¡æ•°æ®åšè®­ç»ƒæ—¶ï¼Œåº”å½“å…ˆå°†ä¼˜åŒ–å™¨çš„æ¢¯åº¦ç½®é›¶ output = model(data) # ä¹‹åå°†dataé€å…¥æ¨¡å‹ä¸­è®­ç»ƒ loss = criterion(label, output) # æ ¹æ®é¢„å…ˆå®šä¹‰çš„criterionè®¡ç®—æŸå¤±å‡½æ•° loss.backward() # å°†lossåå‘ä¼ æ’­å›ç½‘ç»œ optimizer.step() # ä½¿ç”¨ä¼˜åŒ–å™¨æ›´æ–°æ¨¡å‹å‚æ•° train_loss += loss.item()*data.size(0) train_loss = train_loss/len(train_loader.dataset) print(&#x27;Epoch: &#123;&#125; \\tTraining Loss: &#123;:.6f&#125;&#x27;.format(epoch, train_loss)) éªŒè¯/æµ‹è¯•çš„æµç¨‹åŸºæœ¬ä¸è®­ç»ƒè¿‡ç¨‹ä¸€è‡´ï¼Œä¸åŒç‚¹åœ¨äºï¼š éœ€è¦é¢„å…ˆè®¾ç½®torch.no_gradï¼Œä»¥åŠå°†modelè°ƒè‡³evalæ¨¡å¼ ä¸éœ€è¦å°†ä¼˜åŒ–å™¨çš„æ¢¯åº¦ç½®é›¶ ä¸éœ€è¦å°†lossåå‘å›ä¼ åˆ°ç½‘ç»œ ä¸éœ€è¦æ›´æ–°optimizer éªŒè¯/æµ‹è¯•è¿‡ç¨‹ï¼š 12345678910111213def val(epoch): model.eval() val_loss = 0 with torch.no_grad(): for data, label in val_loader: data, label = data.cuda(), label.cuda() output = model(data) preds = torch.argmax(output, 1) loss = criterion(output, label) val_loss += loss.item()*data.size(0) running_accu += torch.sum(preds == label.data) val_loss = val_loss/len(val_loader.dataset) print(&#x27;Epoch: &#123;&#125; \\tTraining Loss: &#123;:.6f&#125;&#x27;.format(epoch, val_loss)) å¯è§†åŒ–åœ¨PyTorchæ·±åº¦å­¦ä¹ ä¸­ï¼Œå¯è§†åŒ–æ˜¯ä¸€ä¸ªå¯é€‰é¡¹ï¼ŒæŒ‡çš„æ˜¯æŸäº›ä»»åŠ¡åœ¨è®­ç»ƒå®Œæˆåï¼Œéœ€è¦å¯¹ä¸€äº›å¿…è¦çš„å†…å®¹è¿›è¡Œå¯è§†åŒ–ï¼Œæ¯”å¦‚åˆ†ç±»çš„ROCæ›²çº¿ï¼Œå·ç§¯ç½‘ç»œä¸­çš„å·ç§¯æ ¸ï¼Œä»¥åŠè®­ç»ƒ/éªŒè¯è¿‡ç¨‹çš„æŸå¤±å‡½æ•°æ›²çº¿ç­‰ç­‰ã€‚ å‚è€ƒèµ„æ–™ Datawhaleå¼€æºé¡¹ç›®ï¼šæ·±å…¥æµ…å‡ºPyTorch https://github.com/datawhalechina/thorough-pytorch/ æå®æ¯…æœºå™¨å­¦ä¹ 2021æ˜¥-PyTorch Tutorial https://www.bilibili.com/video/BV1Wv411h7kN?p=5 åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ pytorchç‰ˆ https://zh-v2.d2l.ai/chapter_preface/index.html PyTorchå®˜æ–¹æ•™ç¨‹ä¸­æ–‡ç‰ˆ https://pytorch123.com/SecondSection/training_a_classifier/","categories":[{"name":"04 ç»„é˜Ÿå­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬30æœŸ æ·±å…¥æµ…å‡ºPyTorch","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬30æœŸ-æ·±å…¥æµ…å‡ºPyTorch","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAPyTorch/"}],"tags":[{"name":"ç¬”è®°","slug":"ç¬”è®°","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"ç»„é˜Ÿå­¦ä¹ ","slug":"ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://example.com/tags/PyTorch/"}]},{"title":"Task07 ä½¿ç”¨Transformersè§£å†³æ–‡æœ¬åˆ†ç±»ä»»åŠ¡","slug":"nlp-transformer-task07","date":"2021-09-25T08:57:45.000Z","updated":"2021-10-02T09:21:50.190Z","comments":true,"path":"nlp-transformer-task07/","link":"","permalink":"http://example.com/nlp-transformer-task07/","excerpt":"","text":"è¯¥éƒ¨åˆ†çš„å†…å®¹ç¿»è¯‘è‡ªğŸ¤—HuggingFace/notebooks https://github.com/huggingface/notebooks/tree/master/examplesä¸­æ–‡ç¿»è¯‘ï¼šDatawhale/learn-nlp-with-transformers/4.1-æ–‡æœ¬åˆ†ç±» Datawhale/learn-nlp-with-transformers/4.1-æ–‡æœ¬åˆ†ç±» å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»æˆ‘ä»¬å°†ä½¿ç”¨ ğŸ¤— Transformersä»£ç åº“ä¸­çš„æ¨¡å‹æ¥è§£å†³æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œä»»åŠ¡æ¥æºäºGLUE Benchmark.GLUEæ¦œå•åŒ…å«äº†9ä¸ªå¥å­çº§åˆ«çš„åˆ†ç±»ä»»åŠ¡ï¼Œåˆ†åˆ«æ˜¯ï¼š CoLA (Corpus of Linguistic Acceptability) é‰´åˆ«ä¸€ä¸ªå¥å­æ˜¯å¦è¯­æ³•æ­£ç¡®. MNLI (Multi-Genre Natural Language Inference) ç»™å®šä¸€ä¸ªå‡è®¾ï¼Œåˆ¤æ–­å¦ä¸€ä¸ªå¥å­ä¸è¯¥å‡è®¾çš„å…³ç³»ï¼šentails, contradicts æˆ–è€… unrelatedã€‚ MRPC (Microsoft Research Paraphrase Corpus) åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦äº’ä¸ºparaphrases. QNLI (Question-answering Natural Language Inference) åˆ¤æ–­ç¬¬2å¥æ˜¯å¦åŒ…å«ç¬¬1å¥é—®é¢˜çš„ç­”æ¡ˆã€‚ QQP (Quora Question Pairs2) åˆ¤æ–­ä¸¤ä¸ªé—®å¥æ˜¯å¦è¯­ä¹‰ç›¸åŒã€‚ RTE (Recognizing Textual Entailment)åˆ¤æ–­ä¸€ä¸ªå¥å­æ˜¯å¦ä¸å‡è®¾æˆentailå…³ç³»ã€‚ SST-2 (Stanford Sentiment Treebank) åˆ¤æ–­ä¸€ä¸ªå¥å­çš„æƒ…æ„Ÿæ­£è´Ÿå‘. STS-B (Semantic Textual Similarity Benchmark) åˆ¤æ–­ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼æ€§ï¼ˆåˆ†æ•°ä¸º1-5åˆ†ï¼‰ã€‚ WNLI (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. å¯¹äºä»¥ä¸Šä»»åŠ¡ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ç®€å•çš„Datasetåº“åŠ è½½æ•°æ®é›†ï¼ŒåŒæ—¶ä½¿ç”¨transformerä¸­çš„Traineræ¥å£å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ 1GLUE_TASKS = [&quot;cola&quot;, &quot;mnli&quot;, &quot;mnli-mm&quot;, &quot;mrpc&quot;, &quot;qnli&quot;, &quot;qqp&quot;, &quot;rte&quot;, &quot;sst2&quot;, &quot;stsb&quot;, &quot;wnli&quot;] This notebook is built to run on any of the tasks in the list above, with any model checkpoint from the Model Hub as long as that model has a version with a classification head. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:æœ¬notebookç†è®ºä¸Šå¯ä»¥ä½¿ç”¨å„ç§å„æ ·çš„transformeræ¨¡å‹ï¼ˆæ¨¡å‹é¢æ¿ï¼‰ï¼Œè§£å†³ä»»ä½•æ–‡æœ¬åˆ†ç±»åˆ†ç±»ä»»åŠ¡ã€‚å¦‚æœæ‚¨æ‰€å¤„ç†çš„ä»»åŠ¡æœ‰æ‰€ä¸åŒï¼Œå¤§æ¦‚ç‡åªéœ€è¦å¾ˆå°çš„æ”¹åŠ¨ä¾¿å¯ä»¥ä½¿ç”¨æœ¬notebookè¿›è¡Œå¤„ç†ã€‚åŒæ—¶ï¼Œæ‚¨åº”è¯¥æ ¹æ®æ‚¨çš„GPUæ˜¾å­˜æ¥è°ƒæ•´å¾®è°ƒè®­ç»ƒæ‰€éœ€è¦çš„btach sizeå¤§å°ï¼Œé¿å…æ˜¾å­˜æº¢å‡ºã€‚ 123task = &quot;cola&quot;model_checkpoint = &quot;distilbert-base-uncased&quot;batch_size = 16 åŠ è½½æ•°æ®é›†We will use the ğŸ¤— Datasets library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions load_dataset and load_metric.æˆ‘ä»¬å°†ä¼šä½¿ç”¨ğŸ¤— Datasetsåº“æ¥åŠ è½½æ•°æ®å’Œå¯¹åº”çš„è¯„æµ‹æ–¹å¼ã€‚æ•°æ®åŠ è½½å’Œè¯„æµ‹æ–¹å¼åŠ è½½åªéœ€è¦ç®€å•ä½¿ç”¨load_datasetå’Œload_metricå³å¯ã€‚ 1from datasets import load_dataset, load_metric Apart from mnli-mm being a special code, we can directly pass our task name to those functions. load_dataset will cache the dataset to avoid downloading it again the next time you run this cell.é™¤äº†mnli-mmä»¥å¤–ï¼Œå…¶ä»–ä»»åŠ¡éƒ½å¯ä»¥ç›´æ¥é€šè¿‡ä»»åŠ¡åå­—è¿›è¡ŒåŠ è½½ã€‚æ•°æ®åŠ è½½ä¹‹åä¼šè‡ªåŠ¨ç¼“å­˜ã€‚ 123actual_task = &quot;mnli&quot; if task == &quot;mnli-mm&quot; else taskdataset = load_dataset(&quot;glue&quot;, actual_task)metric = load_metric(&#x27;glue&#x27;, actual_task) ä¸ŠèŠ‚è®²è¿‡ï¼Œè¿™é‡Œï¼Œæœ€å¥½æ‰‹åŠ¨ä¸‹è½½glue.pyå’Œgule_metric.pyï¼Œä¸ä¸‹è½½åˆ°æœ¬åœ°çš„è¯ï¼Œå®¹æ˜“å‡ºç°è¿æ¥é”™è¯¯ã€‚ The dataset object itself is DatasetDict, which contains one key for the training, validation and test set (with more keys for the mismatched validation and test set in the special case of mnli).è¿™ä¸ªdatasetså¯¹è±¡æœ¬èº«æ˜¯ä¸€ç§DatasetDictæ•°æ®ç»“æ„.å¯¹äºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œåªéœ€è¦ä½¿ç”¨å¯¹åº”çš„keyï¼ˆtrainï¼Œvalidationï¼Œtestï¼‰å³å¯å¾—åˆ°ç›¸åº”çš„æ•°æ®ã€‚ 123456789101112131415&gt;&gt;&gt; datasetDatasetDict(&#123; train: Dataset(&#123; features: [&#x27;sentence&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;], num_rows: 8551 &#125;) validation: Dataset(&#123; features: [&#x27;sentence&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;], num_rows: 1043 &#125;) test: Dataset(&#123; features: [&#x27;sentence&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;], num_rows: 1063 &#125;)&#125;) 1234&gt;&gt;&gt; dataset[&quot;train&quot;][0]&#123;&#x27;sentence&#x27;: &quot;Our friends won&#x27;t buy this analysis, let alone the next one we propose.&quot;,&#x27;label&#x27;: 1, &#x27;idx&#x27;: 0&#125; To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset.ä¸ºäº†èƒ½å¤Ÿè¿›ä¸€æ­¥ç†è§£æ•°æ®é•¿ä»€ä¹ˆæ ·å­ï¼Œä¸‹é¢çš„å‡½æ•°å°†ä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤ºã€‚ 1234567891011121314151617181920import datasetsimport randomimport pandas as pdfrom IPython.display import display, HTMLdef show_random_elements(dataset, num_examples=10): assert num_examples &lt;= len(dataset), &quot;Can&#x27;t pick more elements than there are in the dataset.&quot; picks = [] for _ in range(num_examples): pick = random.randint(0, len(dataset)-1) while pick in picks: pick = random.randint(0, len(dataset)-1) picks.append(pick) df = pd.DataFrame(dataset[picks]) for column, typ in dataset.features.items(): if isinstance(typ, datasets.ClassLabel): df[column] = df[column].transform(lambda i: typ.names[i]) display(HTML(df.to_html())) 1show_random_elements(dataset[&quot;train&quot;]) The metric is an instance of datasets.Metric: 1pass You can call its compute method with your predictions and labels directly and it will return a dictionary with the metric(s) value:ç›´æ¥è°ƒç”¨metricçš„computeæ–¹æ³•ï¼Œä¼ å…¥labelså’Œpredictionså³å¯å¾—åˆ°metricçš„å€¼ï¼š 12345import numpy as npfake_preds = np.random.randint(0, 2, size=(64,))fake_labels = np.random.randint(0, 2, size=(64,))metric.compute(predictions=fake_preds, references=fake_labels) Note that load_metric has loaded the proper metric associated to your task, which is:æ¯ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»ä»»åŠ¡æ‰€å¯¹åº”çš„meticæœ‰æ‰€ä¸åŒï¼Œå…·ä½“å¦‚ä¸‹: for CoLA: Matthews Correlation Coefficient for MNLI (matched or mismatched): Accuracy for MRPC: Accuracy and F1 score for QNLI: Accuracy for QQP: Accuracy and F1 score for RTE: Accuracy for SST-2: Accuracy for STS-B: Pearson Correlation Coefficient and Spearmanâ€™s_Rank_Correlation_Coefficient for WNLI: Accuracy so the metric object only computes the one(s) needed for your task. æ•°æ®é¢„å¤„ç†Before we can feed those texts to our model, we need to preprocess them. This is done by a ğŸ¤— Transformers Tokenizer which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.åœ¨å°†æ•°æ®å–‚å…¥æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚é¢„å¤„ç†çš„å·¥å…·å«Tokenizerã€‚Tokenizeré¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œtokenizeï¼Œç„¶åå°†tokensè½¬åŒ–ä¸ºé¢„æ¨¡å‹ä¸­éœ€è¦å¯¹åº”çš„token IDï¼Œå†è½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚ To do all of this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which will ensure: we get a tokenizer that corresponds to the model architecture we want to use, we download the vocabulary used when pretraining this specific checkpoint. ä¸ºäº†è¾¾åˆ°æ•°æ®é¢„å¤„ç†çš„ç›®çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨AutoTokenizer.from_pretrainedæ–¹æ³•å®ä¾‹åŒ–æˆ‘ä»¬çš„tokenizerï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿ï¼š æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªä¸é¢„è®­ç»ƒæ¨¡å‹ä¸€ä¸€å¯¹åº”çš„tokenizerã€‚ ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹checkpointå¯¹åº”çš„tokenizerçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¹Ÿä¸‹è½½äº†æ¨¡å‹éœ€è¦çš„è¯è¡¨åº“vocabularyï¼Œå‡†ç¡®æ¥è¯´æ˜¯tokens vocabularyã€‚ That vocabulary will be cached, so itâ€™s not downloaded again the next time we run the cell.è¿™ä¸ªè¢«ä¸‹è½½çš„tokens vocabularyä¼šè¢«ç¼“å­˜èµ·æ¥ï¼Œä»è€Œå†æ¬¡ä½¿ç”¨çš„æ—¶å€™ä¸ä¼šé‡æ–°ä¸‹è½½ã€‚ 123from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True) You can directly call this tokenizer on one sentence or a pair of sentences: 1tokenizer(&quot;Hello, this one sentence!&quot;, &quot;And this sentence goes with it.&quot;) è¾“å‡ºä¸ºï¼špass To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names: 123456789101112task_to_keys = &#123; &quot;cola&quot;: (&quot;sentence&quot;, None), &quot;mnli&quot;: (&quot;premise&quot;, &quot;hypothesis&quot;), &quot;mnli-mm&quot;: (&quot;premise&quot;, &quot;hypothesis&quot;), &quot;mrpc&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;), &quot;qnli&quot;: (&quot;question&quot;, &quot;sentence&quot;), &quot;qqp&quot;: (&quot;question1&quot;, &quot;question2&quot;), &quot;rte&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;), &quot;sst2&quot;: (&quot;sentence&quot;, None), &quot;stsb&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;), &quot;wnli&quot;: (&quot;sentence1&quot;, &quot;sentence2&quot;),&#125; We can double check it does work on our current dataset: 123456sentence1_key, sentence2_key = task_to_keys[task]if sentence2_key is None: print(f&quot;Sentence: &#123;dataset[&#x27;train&#x27;][0][sentence1_key]&#125;&quot;)else: print(f&quot;Sentence 1: &#123;dataset[&#x27;train&#x27;][0][sentence1_key]&#125;&quot;) print(f&quot;Sentence 2: &#123;dataset[&#x27;train&#x27;][0][sentence2_key]&#125;&quot;) è¾“å‡ºä¸ºï¼š 1Sentence: Our friends won&#x27;t buy this analysis, let alone the next one we propose. We can them write the function that will preprocess our samples. We just feed them to the tokenizer with the argument truncation=True. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. 1234def preprocess_function(examples): if sentence2_key is None: return tokenizer(examples[sentence1_key], truncation=True) return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True) 12&gt;&gt;&gt; preprocess_function(dataset[&#x27;train&#x27;][:5])&#123;&#x27;input_ids&#x27;: [[101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 1998, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 2030, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 1996, 2062, 2057, 2817, 16025, 1010, 1996, 13675, 16103, 2121, 2027, 2131, 1012, 102], [101, 2154, 2011, 2154, 1996, 8866, 2024, 2893, 14163, 8024, 3771, 1012, 102]], &#x27;attention_mask&#x27;: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]&#125; To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the map method of our dataset object we created earlier. This will apply the function on all the elements of all the splits in dataset, so our training, validation and testing data will be preprocessed in one single command.æ¥ä¸‹æ¥å¯¹æ•°æ®é›†datasetsé‡Œé¢çš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå¤„ç†çš„æ–¹å¼æ˜¯ä½¿ç”¨mapå‡½æ•°ï¼Œå°†é¢„å¤„ç†å‡½æ•°prepare_train_featuresåº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚ 1encoded_dataset = dataset.map(preprocess_function, batched=True) å¾®è°ƒæ¨¡å‹Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about sentence classification, we use the AutoModelForSequenceClassification class. Like with the tokenizer, the from_pretrained method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which is always 2, except for STS-B which is a regression problem and MNLI where we have 3 labels):æ—¢ç„¶æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶åŠ è½½æˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚æ—¢ç„¶æˆ‘ä»¬æ˜¯åšseq2seqä»»åŠ¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨AutoModelForSequenceClassification è¿™ä¸ªç±»ã€‚å’Œtokenizerç›¸ä¼¼ï¼Œfrom_pretrainedæ–¹æ³•åŒæ ·å¯ä»¥å¸®åŠ©æˆ‘ä»¬ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¹æ¨¡å‹è¿›è¡Œç¼“å­˜ï¼Œå°±ä¸ä¼šé‡å¤ä¸‹è½½æ¨¡å‹å•¦ã€‚ 1234from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainernum_labels = 3 if task.startswith(&quot;mnli&quot;) else 1 if task==&quot;stsb&quot; else 2model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels) è¾“å‡ºä¸ºï¼š 12345Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: [&#x27;vocab_transform.bias&#x27;, &#x27;vocab_projector.weight&#x27;, &#x27;vocab_layer_norm.bias&#x27;, &#x27;vocab_layer_norm.weight&#x27;, &#x27;vocab_projector.bias&#x27;, &#x27;vocab_transform.weight&#x27;]- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: [&#x27;classifier.weight&#x27;, &#x27;classifier.bias&#x27;, &#x27;pre_classifier.bias&#x27;, &#x27;pre_classifier.weight&#x27;]You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. The warning is telling us we are throwing away some weights (the vocab_transform and vocab_layer_norm layers) and randomly initializing some other (the pre_classifier and classifier layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we donâ€™t have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.ç”±äºæˆ‘ä»¬å¾®è°ƒçš„ä»»åŠ¡æ˜¯æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œè€Œæˆ‘ä»¬åŠ è½½çš„æ˜¯é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œæ‰€ä»¥ä¼šæç¤ºæˆ‘ä»¬åŠ è½½æ¨¡å‹çš„æ—¶å€™æ‰”æ‰äº†ä¸€äº›ä¸åŒ¹é…çš„ç¥ç»ç½‘ç»œå‚æ•°ï¼ˆæ¯”å¦‚ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç¥ç»ç½‘ç»œheadè¢«æ‰”æ‰äº†ï¼ŒåŒæ—¶éšæœºåˆå§‹åŒ–äº†æ–‡æœ¬åˆ†ç±»çš„ç¥ç»ç½‘ç»œheadï¼‰ã€‚ To instantiate a Trainer, we will need to define two more things. The most important is the TrainingArguments, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªTrainerè®­ç»ƒå·¥å…·ï¼Œæˆ‘ä»¬è¿˜éœ€è¦3ä¸ªè¦ç´ ï¼Œå…¶ä¸­æœ€é‡è¦çš„æ˜¯è®­ç»ƒçš„è®¾å®š/å‚æ•° TrainingArgumentsã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§ã€‚ 1234567891011121314151617metric_name = &quot;pearson&quot; if task == &quot;stsb&quot; else &quot;matthews_correlation&quot; if task == &quot;cola&quot; else &quot;accuracy&quot;model_name = model_checkpoint.split(&quot;/&quot;)[-1]args = TrainingArguments( &quot;test-glue&quot;, evaluation_strategy = &quot;epoch&quot;, save_strategy = &quot;epoch&quot;, learning_rate=2e-5, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, num_train_epochs=5, weight_decay=0.01, load_best_model_at_end=True, metric_for_best_model=metric_name, push_to_hub=False, push_to_hub_model_id=f&quot;&#123;model_name&#125;-finetuned-&#123;task&#125;&quot;,) Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the batch_size defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay. Since the best model might not be the one at the end of training, we ask the Trainer to load the best model it saved (according to metric_name) at the end of training.ä¸Šé¢evaluation_strategy = â€œepochâ€å‚æ•°å‘Šè¯‰è®­ç»ƒä»£ç ï¼šæˆ‘ä»¬æ¯ä¸ªepcohä¼šåšä¸€æ¬¡éªŒè¯è¯„ä¼°ã€‚ä¸Šé¢batch_sizeåœ¨è¿™ä¸ªnotebookä¹‹å‰å®šä¹‰å¥½äº†ã€‚ The last two arguments are to setup everything so we can push the model to the Hub at the end of training. Remove the two of them if you didnâ€™t follow the installation steps at the top of the notebook, otherwise you can change the value of push_to_hub_model_id to something you would prefer.(åé¢éœ€è¦è¿æ¥åˆ°hubå®¢æˆ·ç«¯ï¼Œå¤ªéº»çƒ¦ï¼Œæ‰€ä»¥å…ˆè®¾ä¸ºFalse) The last thing to define for our Trainer is how to compute the metrics from the predictions. We need to define a function for this, which will just use the metric we loaded earlier, the only preprocessing we have to do is to take the argmax of our predicted logits (our just squeeze the last axis in the case of STS-B):æœ€åï¼Œç”±äºä¸åŒçš„ä»»åŠ¡éœ€è¦ä¸åŒçš„è¯„æµ‹æŒ‡æ ‡ï¼Œæˆ‘ä»¬å®šä¸€ä¸ªå‡½æ•°æ¥æ ¹æ®ä»»åŠ¡åå­—å¾—åˆ°è¯„ä»·æ–¹æ³•: 1234567def compute_metrics(eval_pred): predictions, labels = eval_pred if task != &quot;stsb&quot;: predictions = np.argmax(predictions, axis=1) else: predictions = predictions[:, 0] return metric.compute(predictions=predictions, references=labels) Then we just need to pass all of this along with our datasets to the Trainer: 123456789validation_key = &quot;validation_mismatched&quot; if task == &quot;mnli-mm&quot; else &quot;validation_matched&quot; if task == &quot;mnli&quot; else &quot;validation&quot;trainer = Trainer( model, args, train_dataset=encoded_dataset[&quot;train&quot;], eval_dataset=encoded_dataset[validation_key], tokenizer=tokenizer, compute_metrics=compute_metrics) BUG:ValueError: You must login to the Hugging Face hub on this computer by typing transformers-cli login and entering your credentials to use use_auth_token=True. Alternatively, you can pass your own token as the use_auth_token argument.æŠŠargsé‡Œé¢çš„è¯¥é¡¹å‚æ•°æ”¹ä¸ºFalse push_to_hub=False,ã€‚ We can now finetune our model by just calling the train method: 1trainer.train() è¾“å‡ºä¸ºï¼š 1pass We can check with the evaluate method that our Trainer did reload the best model properly (if it was not the last one): 1trainer.evaluate() è¾“å‡ºä¸ºï¼š 1pass è¶…å‚æœç´¢The Trainer supports hyperparameter search using optuna or Ray Tune. 12pip install optunapip install ray[tune] During hyperparameter search, the Trainer will run several trainings, so it needs to have the model defined via a function (so it can be reinitialized at each new run) instead of just having it passed. We jsut use the same function as before:è¶…å‚æœç´¢æ—¶ï¼ŒTrainerå°†ä¼šè¿”å›å¤šä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ‰€ä»¥éœ€è¦ä¼ å…¥ä¸€ä¸ªå®šä¹‰å¥½çš„æ¨¡å‹ä»è€Œè®©Trainerå¯ä»¥ä¸æ–­é‡æ–°åˆå§‹åŒ–è¯¥ä¼ å…¥çš„æ¨¡å‹ï¼š 12def model_init(): return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels) And we can instantiate our Trainer like before: 12345678trainer = Trainer( model_init=model_init, args=args, train_dataset=encoded_dataset[&quot;train&quot;], eval_dataset=encoded_dataset[validation_key], tokenizer=tokenizer, compute_metrics=compute_metrics) The method we call this time is hyperparameter_search. Note that it can take a long time to run on the full dataset for some of the tasks. You can try to find some good hyperparameter on a portion of the training dataset by replacing the train_dataset line above by:train_dataset = encoded_dataset[&quot;train&quot;].shard(index=1, num_shards=10) for 1/10th of the dataset. Then you can run a full training on the best hyperparameters picked by the search.è°ƒç”¨æ–¹æ³•hyperparameter_searchã€‚æ³¨æ„ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯èƒ½å¾ˆä¹…ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆç”¨éƒ¨åˆ†æ•°æ®é›†è¿›è¡Œè¶…å‚æœç´¢ï¼Œå†è¿›è¡Œå…¨é‡è®­ç»ƒã€‚ æ¯”å¦‚ä½¿ç”¨1/10çš„æ•°æ®è¿›è¡Œæœç´¢ï¼š 1best_run = trainer.hyperparameter_search(n_trials=10, direction=&quot;maximize&quot;) The hyperparameter_search method returns a BestRun objects, which contains the value of the objective maximized (by default the sum of all metrics) and the hyperparameters it used for that run.hyperparameter_searchä¼šè¿”å›æ•ˆæœæœ€å¥½çš„æ¨¡å‹ç›¸å…³çš„å‚æ•°ï¼š 12&gt;&gt;&gt; best_run To reproduce the best training, just set the hyperparameters in your TrainingArgument before creating a Trainer:å°†Trainnerè®¾ç½®ä¸ºæœç´¢åˆ°çš„æœ€å¥½å‚æ•°ï¼Œè¿›è¡Œè®­ç»ƒï¼š 1234for n, v in best_run.hyperparameters.items(): setattr(trainer.args, n, v)trainer.train() å‚è€ƒèµ„æ–™ HuggingFace/transfomers/BERT https://huggingface.co/transformers/model_doc/bert.html# åŸºäºtransformersçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨â€“åœ¨çº¿é˜…è¯» https://datawhalechina.github.io/learn-nlp-with-transformers/#/","categories":[{"name":"04 ç»„é˜Ÿå­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬29æœŸ åŸºäºtransformerçš„NLP","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬29æœŸ-åŸºäºtransformerçš„NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"ç¬”è®°","slug":"ç¬”è®°","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"ç»„é˜Ÿå­¦ä¹ ","slug":"ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"é¢„è®­ç»ƒæ¨¡å‹","slug":"é¢„è®­ç»ƒæ¨¡å‹","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"BERT","slug":"BERT","permalink":"http://example.com/tags/BERT/"},{"name":"transfomer","slug":"transfomer","permalink":"http://example.com/tags/transfomer/"},{"name":"æ–‡æœ¬åˆ†ç±»","slug":"æ–‡æœ¬åˆ†ç±»","permalink":"http://example.com/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"}]},{"title":"Task06 BERTåº”ç”¨ã€è®­ç»ƒå’Œä¼˜åŒ–","slug":"nlp-transformer-task06","date":"2021-09-24T07:18:42.000Z","updated":"2021-10-02T09:21:45.032Z","comments":true,"path":"nlp-transformer-task06/","link":"","permalink":"http://example.com/nlp-transformer-task06/","excerpt":"","text":"è¯¥éƒ¨åˆ†çš„å†…å®¹ç¿»è¯‘è‡ªğŸ¤—HuggingFaceå®˜ç½‘æ•™ç¨‹ç¬¬1éƒ¨åˆ†ï¼ˆ1-4ç« ï¼‰ï¼Œè§ https://huggingface.co/course/chapter1ã€‚è¯¥ç³»åˆ—æ•™ç¨‹ç”±3å¤§éƒ¨åˆ†å…±12ç« ç»„æˆï¼ˆå¦‚å›¾ï¼‰ï¼Œå…¶ä¸­ç¬¬1éƒ¨åˆ†ä»‹ç»transformersåº“çš„ä¸»è¦æ¦‚å¿µã€æ¨¡å‹çš„å·¥ä½œåŸç†å’Œä½¿ç”¨æ–¹æ³•ã€æ€æ ·åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå¾®è°ƒç­‰å†…å®¹ã€‚ ç¯å¢ƒæ­å»ºç®€å•çš„è¯´ï¼Œæœ‰ä¸¤ç§å¯ä»¥è·‘æ¨¡å‹ä»£ç çš„æ–¹å¼ï¼š Google Colab æœ¬åœ°è™šæ‹Ÿç¯å¢ƒ pip install transformers è¯¦è§ https://huggingface.co/course/chapter0?fw=pt Transformeræ¨¡å‹æ¦‚è¿°Transformers, å¯ä»¥åšä»€ä¹ˆï¼Ÿç›®å‰å¯ç”¨çš„ä¸€äº›pipelineæ˜¯ï¼š feature-extraction è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º fill-mask å®Œå½¢å¡«ç©º ner (named entity recognition) å‘½åå®ä½“è¯†åˆ« question-answering é—®ç­” sentiment-analysis æƒ…æ„Ÿåˆ†æ summarization æ‘˜è¦ç”Ÿæˆ text-generation æ–‡æœ¬ç”Ÿæˆ translation ç¿»è¯‘ zero-shot-classification é›¶æ ·æœ¬åˆ†ç±» pipeline: ç›´è¯‘ç®¡é“/æµæ°´çº¿ï¼Œå¯ä»¥ç†è§£ä¸ºæµç¨‹ã€‚ Transformers, å¦‚ä½•å·¥ä½œï¼ŸTransformerç®€å²Transformer æ¶æ„äº 2017 å¹´ 6 æœˆæ¨å‡ºã€‚åŸå§‹ç ”ç©¶çš„é‡ç‚¹æ˜¯ç¿»è¯‘ä»»åŠ¡ã€‚éšåæ¨å‡ºäº†å‡ ä¸ªæœ‰å½±å“åŠ›çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š 2018 å¹´ 6 æœˆï¼šGPTï¼Œç¬¬ä¸€ä¸ªé¢„è®­ç»ƒçš„ Transformer æ¨¡å‹ï¼Œç”¨äºå„ç§ NLP ä»»åŠ¡çš„å¾®è°ƒå¹¶è·å¾—æœ€å…ˆè¿›çš„ç»“æœ 2018 å¹´ 10 æœˆï¼šBERTï¼Œå¦ä¸€ä¸ªå¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¨åœ¨ç”Ÿæˆæ›´å¥½çš„å¥å­æ‘˜è¦ 2019 å¹´ 2 æœˆï¼šGPT-2ï¼ŒGPT çš„æ”¹è¿›ï¼ˆå’Œæ›´å¤§ï¼‰ç‰ˆæœ¬ 2019 å¹´ 10 æœˆï¼šDistilBERTï¼ŒBERT çš„è’¸é¦ç‰ˆæœ¬ï¼Œé€Ÿåº¦æé«˜ 60%ï¼Œå†…å­˜å‡è½» 40%ï¼Œä½†ä»ä¿ç•™ BERT 97% çš„æ€§èƒ½ 2019 å¹´ 10 æœˆï¼šBART å’Œ T5ï¼Œä¸¤ä¸ªä½¿ç”¨ä¸åŸå§‹ Transformer æ¨¡å‹ç›¸åŒæ¶æ„çš„å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼ˆç¬¬ä¸€ä¸ªè¿™æ ·åšï¼‰ 2020 å¹´ 5 æœˆï¼ŒGPT-3ï¼ŒGPT-2 çš„æ›´å¤§ç‰ˆæœ¬ï¼Œæ— éœ€å¾®è°ƒå³å¯åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼ˆç§°ä¸ºé›¶æ ·æœ¬å­¦ä¹ zero-shot learningï¼‰ å¤§ä½“ä¸Šï¼Œå®ƒä»¬å¯ä»¥åˆ†ä¸ºä¸‰ç±»ï¼š GPTç±»ï¼ˆåˆç§°ä¸ºè‡ªå›å½’ Transformer æ¨¡å‹ï¼‰ï¼šåªä½¿ç”¨transformer-decoderéƒ¨åˆ† BERTç±»ï¼ˆåˆç§°ä¸ºè‡ªç¼–ç  Transformer æ¨¡å‹ï¼‰ï¼šåªä½¿ç”¨transformer-encoderéƒ¨åˆ† BART/T5ç±»ï¼ˆåˆç§°ä¸ºåºåˆ—åˆ°åºåˆ— Transformer æ¨¡å‹ï¼‰ï¼šä½¿ç”¨Transformer-encoder-decoderéƒ¨åˆ† å®ƒä»¬çš„åˆ†ç±»ã€å…·ä½“æ¨¡å‹ã€ä¸»è¦åº”ç”¨ä»»åŠ¡å¦‚ä¸‹ï¼š å…¶ä»–éœ€è¦çŸ¥é“çš„ï¼š Transformersæ˜¯è¯­è¨€æ¨¡å‹ Transformersæ˜¯å¤§æ¨¡å‹ Transformersçš„åº”ç”¨é€šè¿‡é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªè¿‡ç¨‹ åè¯è§£é‡Šï¼šArchitectureå’ŒCheckpointsArchitecture/æ¶æ„ï¼šå®šä¹‰äº†æ¨¡å‹çš„åŸºæœ¬ç»“æ„å’ŒåŸºæœ¬è¿ç®—ã€‚Checkpoints/æ£€æŸ¥ç‚¹ï¼šæ¨¡å‹çš„æŸä¸ªè®­ç»ƒçŠ¶æ€ï¼ŒåŠ è½½æ­¤checkpointä¼šåŠ è½½æ­¤æ—¶çš„æƒé‡ã€‚è®­ç»ƒæ—¶å¯ä»¥é€‰æ‹©è‡ªåŠ¨ä¿å­˜checkpointã€‚æ¨¡å‹åœ¨è®­ç»ƒæ—¶å¯ä»¥è®¾ç½®è‡ªåŠ¨ä¿å­˜äºæŸä¸ªæ—¶é—´ç‚¹ï¼ˆæ¯”å¦‚æ¨¡å‹è®­ç»ƒäº†ä¸€è½®epochï¼Œæ›´æ–°äº†å‚æ•°ï¼Œå°†è¿™ä¸ªçŠ¶æ€çš„æ¨¡å‹ä¿å­˜ä¸‹æ¥ï¼Œä¸ºä¸€ä¸ªcheckpointã€‚ï¼‰ æ‰€ä»¥æ¯ä¸ªcheckpointå¯¹åº”æ¨¡å‹çš„ä¸€ä¸ªçŠ¶æ€ï¼Œä¸€ç»„æƒé‡ã€‚ ä½¿ç”¨Transformers3ä¸ªå¤„ç†æ­¥éª¤å°†ä¸€äº›æ–‡æœ¬ä¼ é€’åˆ°pipelineæ—¶æ¶‰åŠ3ä¸ªä¸»è¦æ­¥éª¤ï¼š æ–‡æœ¬è¢«é¢„å¤„ç†ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼ã€‚ é¢„å¤„ç†åçš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹ã€‚ æ¨¡å‹çš„é¢„æµ‹ç»“æœè¢«åå¤„ç†ä¸ºäººç±»å¯ä»¥ç†è§£çš„æ ¼å¼ã€‚ Pipelineå°†3ä¸ªæ­¥éª¤ç»„åˆåœ¨ä¸€èµ·ï¼šé¢„å¤„ç†/Tokenizerã€é€šè¿‡æ¨¡å‹ä¼ é€’è¾“å…¥/Modelå’Œåå¤„ç†/Post-Processingï¼š Tokenizer/é¢„å¤„ç†Tokenizerçš„ä½œç”¨ï¼š å°†è¾“å…¥æ‹†åˆ†ä¸ºç§°ä¸ºtokençš„å•è¯ã€å­è¯/subwordæˆ–ç¬¦å·/symbolsï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ï¼‰ å°†æ¯ä¸ªtokenæ˜ å°„åˆ°ä¸€ä¸ªæ•´æ•° æ·»åŠ å¯èƒ½å¯¹æ¨¡å‹æœ‰ç”¨çš„å…¶ä»–è¾“å…¥ Going Through Models/ç©¿è¿‡æ¨¡å‹æ¨¡å‹å®ä¾‹åŒ–1234from transformers import AutoModelcheckpoint = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;model = AutoModel.from_pretrained(checkpoint) åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬ä¸‹è½½äº†åœ¨pipelineä¸­ä½¿ç”¨çš„ç›¸åŒæ£€æŸ¥ç‚¹ï¼ˆå®é™…ä¸Šå·²ç»ç¼“å­˜ï¼‰å¹¶å°†æ¨¡å‹å®ä¾‹åŒ–ã€‚ æ¨¡å‹çš„è¾“å‡ºï¼šé«˜ç»´å‘é‡æ¨¡å‹çš„è¾“å‡ºå‘é‡é€šå¸¸æœ‰ä¸‰ä¸ªç»´åº¦ï¼š Batch size: ä¸€æ¬¡å¤„ç†çš„åºåˆ—æ•° Sequence length: åºåˆ—å‘é‡çš„é•¿åº¦ Hidden size: æ¯ä¸ªæ¨¡å‹è¾“å…¥å¤„ç†åçš„å‘é‡ç»´åº¦ï¼ˆhidden state vectorï¼‰ Model Headsï¼šä¸ºäº†å¤„ç†ä¸åŒçš„ä»»åŠ¡Model heads:å°†éšè—çŠ¶æ€çš„é«˜ç»´å‘é‡ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å®ƒä»¬æŠ•å½±åˆ°ä¸åŒçš„ç»´åº¦ä¸Šã€‚å®ƒä»¬é€šå¸¸ç”±ä¸€ä¸ªæˆ–å‡ ä¸ªçº¿æ€§å±‚ç»„æˆã€‚å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œç´«è‰²ä»£è¡¨å‘é‡ï¼Œç²‰è‰²ä»£è¡¨æ¨¡ç»„ï¼ŒEmbeddings+layersè¡¨ç¤ºTransformerçš„æ¶æ„ï¼Œç»è¿‡è¿™å±‚æ¶æ„åçš„è¾“å‡ºé€å…¥Model Headè¿›è¡Œå¤„ç†ï¼Œä»è€Œåº”ç”¨åˆ°ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚ğŸ¤— Transformers ä¸­æœ‰è®¸å¤šä¸åŒçš„Headæ¶æ„å¯ç”¨ï¼Œæ¯ä¸€ç§æ¶æ„éƒ½å›´ç»•ç€å¤„ç†ç‰¹å®šä»»åŠ¡è€Œè®¾è®¡ã€‚ ä¸‹é¢åˆ—ä¸¾äº†éƒ¨åˆ†Model headsï¼š *Model (retrieve the hidden states) *ForCausalLM *ForMaskedLM *ForMultipleChoice *ForQuestionAnswering *ForSequenceClassification *ForTokenClassification and others ğŸ¤— Post-processing/åå¤„ç†ä»æ¨¡å‹ä¸­è·å¾—çš„ä½œä¸ºè¾“å‡ºçš„å€¼æœ¬èº«å¹¶ä¸ä¸€å®šæœ‰æ„ä¹‰ã€‚è¦è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œå®ƒä»¬éœ€è¦ç»è¿‡ä¸€ä¸ª SoftMax å±‚ã€‚ å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹æ•°æ®å¤„ç†åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨MRPCï¼ˆMicrosoft Research Praphrase Corpusï¼‰æ•°æ®é›†ä½œä¸ºç¤ºä¾‹ã€‚è¯¥DataSetç”±5,801å¯¹å¥å­ç»„æˆï¼Œæ ‡ç­¾æŒ‡ç¤ºå®ƒä»¬æ˜¯å¦æ˜¯åŒä¹‰å¥ï¼ˆå³ä¸¤ä¸ªå¥å­æ˜¯å¦è¡¨ç¤ºç›¸åŒçš„æ„æ€ï¼‰ã€‚ æˆ‘ä»¬é€‰æ‹©å®ƒæ˜¯å› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå°å‹æ•°æ®é›†ï¼Œå› æ­¤å¯ä»¥è½»æ¾è®­ç»ƒã€‚ ä»Hubä¸ŠåŠ è½½æ•°æ®é›†Hubä¸ä»…åŒ…å«æ¨¡å‹ï¼Œè¿˜å«æœ‰å¤šç§è¯­è¨€çš„datasetsã€‚ä¾‹å¦‚ï¼ŒMRPCæ•°æ®é›†æ˜¯æ„æˆ GLUE benchmarkçš„ 10 ä¸ªæ•°æ®é›†ä¹‹ä¸€ã€‚GLUEï¼ˆGeneral Language Understanding Evaluationï¼‰æ˜¯ä¸€ä¸ªå¤šä»»åŠ¡çš„è‡ªç„¶è¯­è¨€ç†è§£åŸºå‡†å’Œåˆ†æå¹³å°ã€‚GLUEåŒ…å«ä¹é¡¹NLUä»»åŠ¡ï¼Œè¯­è¨€å‡ä¸ºè‹±è¯­ã€‚GLUEä¹é¡¹ä»»åŠ¡æ¶‰åŠåˆ°è‡ªç„¶è¯­è¨€æ¨æ–­ã€æ–‡æœ¬è•´å«ã€æƒ…æ„Ÿåˆ†æã€è¯­ä¹‰ç›¸ä¼¼ç­‰å¤šä¸ªä»»åŠ¡ã€‚åƒBERTã€XLNetã€RoBERTaã€ERINEã€T5ç­‰çŸ¥åæ¨¡å‹éƒ½ä¼šåœ¨æ­¤åŸºå‡†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚ ğŸ¤— Datasetsåº“æä¾›äº†ä¸€ä¸ªéå¸¸ç®€å•çš„å‘½ä»¤æ¥ä¸‹è½½å’Œç¼“å­˜Hubä¸Šçš„datasetã€‚ æˆ‘ä»¬å¯ä»¥åƒè¿™æ ·ä¸‹è½½ MRPC æ•°æ®é›†ï¼š 1234&gt;&gt;&gt; from datasets import load_dataset&gt;&gt;&gt; raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)&gt;&gt;&gt; raw_datasets è¾“å‡ºå¦‚ä¸‹ï¼š 1234567891011121314DatasetDict(&#123; train: Dataset(&#123; features: [&#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;], num_rows: 3668 &#125;) validation: Dataset(&#123; features: [&#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;], num_rows: 408 &#125;) test: Dataset(&#123; features: [&#x27;sentence1&#x27;, &#x27;sentence2&#x27;, &#x27;label&#x27;, &#x27;idx&#x27;], num_rows: 1725 &#125;)&#125;) è¿™æ ·å°±å¾—åˆ°ä¸€ä¸ªDatasetDictå¯¹è±¡ï¼ŒåŒ…å«è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œè®­ç»ƒé›†ä¸­æœ‰3,668 ä¸ªå¥å­å¯¹ï¼ŒéªŒè¯é›†ä¸­æœ‰408å¯¹ï¼Œæµ‹è¯•é›†ä¸­æœ‰1,725 å¯¹ã€‚æ¯ä¸ªå¥å­å¯¹åŒ…å«å››ä¸ªå­—æ®µï¼šâ€™sentence1â€™, â€˜sentence2â€™, â€˜labelâ€™å’Œ â€˜idxâ€™ã€‚ æˆ‘ä»¬å¯ä»¥é€šè¿‡ç´¢å¼•è®¿é—®raw_datasets çš„å¥å­å¯¹ï¼š 12&gt;&gt;&gt; raw_train_dataset = raw_datasets[&quot;train&quot;]&gt;&gt;&gt; raw_train_dataset[0] è¾“å‡ºå¦‚ä¸‹ï¼š 1234&#123;&#x27;sentence1&#x27;: &#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;, &#x27;sentence2&#x27;: &#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;, &#x27;label&#x27;: 1, &#x27;idx&#x27;: 0&#125; æˆ‘ä»¬å¯ä»¥é€šè¿‡featuresè·å¾—æ•°æ®é›†çš„å­—æ®µç±»å‹ï¼š 1&gt;&gt;&gt; raw_train_dataset.features è¾“å‡ºå¦‚ä¸‹ï¼š 1234&#123;&#x27;sentence1&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;sentence2&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;not_equivalent&#x27;, &#x27;equivalent&#x27;], names_file=None, id=None), &#x27;idx&#x27;: Value(dtype=&#x27;int32&#x27;, id=None)&#125; TIPSï¼š æ²¡æœ‰æ•°æ®é›†çš„è¯é¦–å…ˆå®‰è£…ä¸€ä¸‹ï¼špip install datasets è¿™é‡Œå¾ˆå®¹æ˜“å‡ºç°è¿æ¥é”™è¯¯ï¼Œè§£å†³æ–¹æ³•å¦‚ä¸‹ï¼šhttps://blog.csdn.net/qq_20849045/article/details/117462846?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link æ•°æ®é›†é¢„å¤„ç†é€šè¿‡æ•°æ®é›†é¢„å¤„ç†ï¼Œæˆ‘ä»¬å°†æ–‡æœ¬è½¬æ¢æˆæ¨¡å‹èƒ½ç†è§£çš„å‘é‡ã€‚è¿™ä¸ªè¿‡ç¨‹é€šè¿‡Tokenizerå®ç°ï¼š 123456&gt;&gt;&gt; from transformers import AutoTokenizer&gt;&gt;&gt; checkpoint = &quot;bert-base-uncased&quot;&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(checkpoint)&gt;&gt;&gt; tokenized_sentences_1 = tokenizer(raw_datasets[&quot;train&quot;][&quot;sentence1&quot;])&gt;&gt;&gt; tokenized_sentences_2 = tokenizer(raw_datasets[&quot;train&quot;][&quot;sentence2&quot;]) ï¼ˆTODOï¼‰ ä½¿ç”¨Trainer APIå¾®è°ƒä¸€ä¸ªæ¨¡å‹è®­ç»ƒè¯„ä¼°å‡½æ•°è¡¥å……éƒ¨åˆ†ä¸ºä»€ä¹ˆ4ä¸­ç”¨Traineræ¥å¾®è°ƒæ¨¡å‹ï¼ŸTraining Argumentsä¸»è¦å‚æ•°ä¸åŒæ¨¡å‹çš„åŠ è½½æ–¹å¼Dynamic Paddingâ€”â€”åŠ¨æ€å¡«å……æŠ€æœ¯å‚è€ƒèµ„æ–™ åŸºäºtransformersçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨â€“åœ¨çº¿é˜…è¯» https://datawhalechina.github.io/learn-nlp-with-transformers/#/ Huggingfaceå®˜æ–¹æ•™ç¨‹ https://huggingface.co/course/chapter1","categories":[{"name":"04 ç»„é˜Ÿå­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬29æœŸ åŸºäºtransformerçš„NLP","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬29æœŸ-åŸºäºtransformerçš„NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"ç¬”è®°","slug":"ç¬”è®°","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"ç»„é˜Ÿå­¦ä¹ ","slug":"ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"é¢„è®­ç»ƒæ¨¡å‹","slug":"é¢„è®­ç»ƒæ¨¡å‹","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"BERT","slug":"BERT","permalink":"http://example.com/tags/BERT/"},{"name":"transfomer","slug":"transfomer","permalink":"http://example.com/tags/transfomer/"}]},{"title":"Task05 ç¼–å†™BERTæ¨¡å‹","slug":"nlp-transformer-task05","date":"2021-09-20T19:01:35.000Z","updated":"2021-10-02T09:21:40.680Z","comments":true,"path":"nlp-transformer-task05/","link":"","permalink":"http://example.com/nlp-transformer-task05/","excerpt":"","text":"Overviewæœ¬éƒ¨åˆ†æ˜¯BERTæºç çš„è§£è¯»ï¼Œæ¥è‡ªHuggingFace/transfomers/BERT[1]ã€‚ å¦‚å›¾æ‰€ç¤ºï¼Œä»£ç ç»“æ„å’Œä½œç”¨å¦‚ä¸‹ï¼š BertTokenizer é¢„å¤„ç†å’Œåˆ‡è¯ BertModel BertEmbeddings è¯åµŒå…¥ BertEncoder BertAttention æ³¨æ„åŠ›æœºåˆ¶ BertIntermediate å…¨è¿æ¥å’Œæ¿€æ´»å‡½æ•° BertOutput å…¨è¿æ¥ã€æ®‹å·®é“¾æ¥å’Œæ­£åˆ™åŒ– BertPooler å–å‡º[CLS]å¯¹åº”çš„å‘é‡ï¼Œç„¶åé€šè¿‡å…¨è¿æ¥å±‚å’Œæ¿€æ´»å‡½æ•°åè¾“å‡ºç»“æœ BERTçš„å®ç°BertConfig1234567classtransformers.BertConfig(vocab_size=30522, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act=&#x27;gelu&#x27;, hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1 , max_position_embeddings=512, type_vocab_size=2, initializer_range=0.02, layer_norm_eps=1e-12, pad_token_id=0, gradient_checkpointing=False, position_embedding_type=&#x27;absolute&#x27;, use_cache=True, classifier_dropout=None, **kwargs) è¿™æ˜¯å­˜å‚¨BertModelï¼ˆTorch.nn.Moduleçš„å­ç±»ï¼‰æˆ–TFBertModelï¼ˆtf.keras.Modelçš„å­ç±»ï¼‰é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°æ¥å®ä¾‹åŒ–BERTæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ é…ç½®å¯¹è±¡ä»PretrainedConfigç»§æ‰¿ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚ å‚æ•°ï¼š vocab_size: BERTæ¨¡å‹çš„è¯æ±‡é‡ï¼Œå®šä¹‰äº†èƒ½è¢«inputs_idsè¡¨ç¤ºçš„tokenæ•°é‡ã€‚ hidden_size: BertTokenizerBertModelBERTçš„åº”ç”¨BertForPreTrainingBertForNextSentencePredictionBertForSequenceClassificationBertForMultipleChoiceBertForTokenClassificationBertForQuestionAnsweringBERTçš„è®­ç»ƒå’Œä¼˜åŒ–Pre-TrainingFine-Tuningå‚è€ƒèµ„æ–™ HuggingFace/transfomers/BERT https://huggingface.co/transformers/model_doc/bert.html# åŸºäºtransformersçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨â€“åœ¨çº¿é˜…è¯» https://datawhalechina.github.io/learn-nlp-with-transformers/#/","categories":[{"name":"04 ç»„é˜Ÿå­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬29æœŸ åŸºäºtransformerçš„NLP","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬29æœŸ-åŸºäºtransformerçš„NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"ç¬”è®°","slug":"ç¬”è®°","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"ç»„é˜Ÿå­¦ä¹ ","slug":"ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"é¢„è®­ç»ƒæ¨¡å‹","slug":"é¢„è®­ç»ƒæ¨¡å‹","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"BERT","slug":"BERT","permalink":"http://example.com/tags/BERT/"},{"name":"transfomer","slug":"transfomer","permalink":"http://example.com/tags/transfomer/"}]},{"title":"Task04 å­¦ä¹ GPT","slug":"nlp-transformer-task04","date":"2021-09-19T12:02:17.000Z","updated":"2021-10-02T09:21:36.582Z","comments":true,"path":"nlp-transformer-task04/","link":"","permalink":"http://example.com/nlp-transformer-task04/","excerpt":"","text":"ä»è¯­è¨€æ¨¡å‹è¯´èµ·è‡ªç¼–ç è¯­è¨€æ¨¡å‹ï¼ˆauto-encoderï¼‰è‡ªç¼–ç è¯­è¨€æ¨¡å‹é€šè¿‡éšæœºMaskè¾“å…¥çš„éƒ¨åˆ†å•è¯ï¼Œç„¶åé¢„è®­ç»ƒçš„ç›®æ ‡æ˜¯é¢„æµ‹è¢«Maskçš„å•è¯ï¼Œä¸ä»…å¯ä»¥èå…¥ä¸Šæ–‡ä¿¡æ¯ï¼Œè¿˜å¯ä»¥è‡ªç„¶çš„èå…¥ä¸‹æ–‡ä¿¡æ¯ã€‚ex. BERT. ä¼˜ç‚¹ï¼šè‡ªç„¶åœ°èå…¥åŒå‘è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶çœ‹åˆ°è¢«é¢„æµ‹å•è¯çš„ä¸Šæ–‡å’Œä¸‹æ–‡ ç¼ºç‚¹ï¼šè®­ç»ƒå’Œé¢„æµ‹ä¸ä¸€è‡´ã€‚è®­ç»ƒçš„æ—¶å€™è¾“å…¥å¼•å…¥äº†[Mask]æ ‡è®°ï¼Œä½†æ˜¯åœ¨é¢„æµ‹é˜¶æ®µå¾€å¾€æ²¡æœ‰è¿™ä¸ª[Mask]æ ‡è®°ï¼Œå¯¼è‡´é¢„è®­ç»ƒé˜¶æ®µå’ŒFine-tuningé˜¶æ®µä¸ä¸€è‡´ã€‚ è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆauto-regressiveï¼‰è¯­è¨€æ¨¡å‹æ ¹æ®è¾“å…¥å¥å­çš„ä¸€éƒ¨åˆ†æ–‡æœ¬æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚ex. GPT-2 ä¼˜ç‚¹ï¼šå¯¹äºç”Ÿæˆç±»çš„NLPä»»åŠ¡ï¼Œæ¯”å¦‚æ–‡æœ¬æ‘˜è¦ï¼Œæœºå™¨ç¿»è¯‘ç­‰ï¼Œä»å·¦å‘å³çš„ç”Ÿæˆå†…å®¹ï¼Œå¤©ç„¶å’Œè‡ªå›å½’è¯­è¨€æ¨¡å‹å¥‘åˆã€‚ ç¼ºç‚¹ï¼šç”±äºä¸€èˆ¬æ˜¯ä»å·¦åˆ°å³ï¼ˆå½“ç„¶ä¹Ÿå¯èƒ½ä»å³åˆ°å·¦ï¼‰ï¼Œæ‰€ä»¥åªèƒ½åˆ©ç”¨ä¸Šæ–‡æˆ–è€…ä¸‹æ–‡çš„ä¿¡æ¯ï¼Œä¸èƒ½åŒæ—¶åˆ©ç”¨ä¸Šæ–‡å’Œä¸‹æ–‡çš„ä¿¡æ¯ã€‚ Transformer, BERT, GPT-2çš„å…³ç³»Transformerçš„Encoderè¿›åŒ–æˆäº†BERTï¼ŒDecoderè¿›åŒ–æˆäº†GPT2ã€‚ å¦‚æœè¦ä½¿ç”¨Transformeræ¥è§£å†³è¯­è¨€æ¨¡å‹ä»»åŠ¡ï¼Œå¹¶ä¸éœ€è¦å®Œæ•´çš„Encoderéƒ¨åˆ†å’ŒDecoderéƒ¨åˆ†ï¼Œäºæ˜¯åœ¨åŸå§‹Transformerä¹‹åçš„è®¸å¤šç ”ç©¶å·¥ä½œä¸­ï¼Œäººä»¬å°è¯•åªä½¿ç”¨Transformer Encoderæˆ–è€…Decoderè¿›è¡Œé¢„è®­ç»ƒã€‚æ¯”å¦‚BERTåªä½¿ç”¨äº†Encoderéƒ¨åˆ†è¿›è¡Œmasked language modelï¼ˆè‡ªç¼–ç ï¼‰è®­ç»ƒï¼ŒGPT-2ä¾¿æ˜¯åªä½¿ç”¨äº†Decoderéƒ¨åˆ†è¿›è¡Œè‡ªå›å½’ï¼ˆauto regressiveï¼‰è¯­è¨€æ¨¡å‹è®­ç»ƒã€‚ GPT-2æ¦‚è¿°æ¨¡å‹çš„è¾“å…¥è¾“å…¥çš„å¤„ç†åˆ†ä¸ºä¸¤æ­¥ï¼štoken embedding + position encodingã€‚å³: åœ¨åµŒå…¥çŸ©é˜µä¸­æŸ¥æ‰¾è¾“å…¥çš„å•è¯çš„å¯¹åº”çš„embeddingå‘é‡ èå…¥ä½ç½®ç¼–ç  Decoderå±‚æ¯ä¸€å±‚decoderçš„ç»„æˆï¼šMasked Self-Attention + Feed Forward Neural Network Self-Attentionæ‰€åšçš„äº‹æƒ…æ˜¯ï¼šå®ƒé€šè¿‡å¯¹å¥å­ç‰‡æ®µä¸­æ¯ä¸ªè¯çš„ç›¸å…³æ€§æ‰“åˆ†ï¼Œå¹¶å°†è¿™äº›è¯çš„è¡¨ç¤ºå‘é‡æ ¹æ®ç›¸å…³æ€§åŠ æƒæ±‚å’Œï¼Œä»è€Œè®©æ¨¡å‹èƒ½å¤Ÿå°†è¯å’Œå…¶ä»–ç›¸å…³è¯å‘é‡çš„ä¿¡æ¯èåˆèµ·æ¥ã€‚ Masked Self-Attentionåšçš„æ˜¯ï¼šå°†maskä½ç½®å¯¹åº”çš„çš„attention scoreå˜æˆä¸€ä¸ªéå¸¸å°çš„æ•°å­—æˆ–è€…0ï¼Œè®©å…¶ä»–å•è¯å†self attentionçš„æ—¶å€™ï¼ˆåŠ æƒæ±‚å’Œçš„æ—¶å€™ï¼‰ä¸è€ƒè™‘è¿™äº›å•è¯ã€‚ æ¨¡å‹çš„è¾“å‡ºå½“æ¨¡å‹é¡¶éƒ¨çš„Decoderå±‚äº§ç”Ÿè¾“å‡ºå‘é‡æ—¶ï¼Œæ¨¡å‹ä¼šå°†è¿™ä¸ªå‘é‡ä¹˜ä»¥ä¸€ä¸ªå·¨å¤§çš„åµŒå…¥çŸ©é˜µï¼ˆvocab size x embedding sizeï¼‰æ¥è®¡ç®—è¯¥å‘é‡å’Œæ‰€æœ‰å•è¯embeddingå‘é‡çš„ç›¸å…³å¾—åˆ†ã€‚è¿™ä¸ªç›¸ä¹˜çš„ç»“æœï¼Œè¢«è§£é‡Šä¸ºæ¨¡å‹è¯æ±‡è¡¨ä¸­æ¯ä¸ªè¯çš„åˆ†æ•°ï¼Œç»è¿‡softmaxä¹‹åè¢«è½¬æ¢æˆæ¦‚ç‡ã€‚ æˆ‘ä»¬å¯ä»¥é€‰æ‹©æœ€é«˜åˆ†æ•°çš„ tokenï¼ˆtop_k=1ï¼‰ï¼Œä¹Ÿå¯ä»¥åŒæ—¶è€ƒè™‘å…¶ä»–è¯ï¼ˆtop kï¼‰ã€‚å‡è®¾æ¯ä¸ªä½ç½®è¾“å‡ºkä¸ªtokenï¼Œå‡è®¾æ€»å…±è¾“å‡ºnä¸ªtokenï¼Œé‚£ä¹ˆåŸºäºnä¸ªå•è¯çš„è”åˆæ¦‚ç‡é€‰æ‹©çš„è¾“å‡ºåºåˆ—ä¼šæ›´å¥½ã€‚ æ¨¡å‹å®Œæˆä¸€æ¬¡è¿­ä»£ï¼Œè¾“å‡ºä¸€ä¸ªå•è¯ã€‚æ¨¡å‹ä¼šç»§ç»­è¿­ä»£ï¼Œç›´åˆ°æ‰€æœ‰çš„å•è¯éƒ½å·²ç»ç”Ÿæˆï¼Œæˆ–è€…ç›´åˆ°è¾“å‡ºäº†è¡¨ç¤ºå¥å­æœ«å°¾çš„tokenã€‚ å…³äºSelf-Attention, Masked Self-AttentionSelf-AttentionSelf-Attention ä¸»è¦é€šè¿‡ 3 ä¸ªæ­¥éª¤æ¥å®ç°ï¼š ä¸ºæ¯ä¸ªè·¯å¾„åˆ›å»º Queryã€Keyã€Value çŸ©é˜µã€‚ å¯¹äºæ¯ä¸ªè¾“å…¥çš„tokenï¼Œä½¿ç”¨å®ƒçš„Queryå‘é‡ä¸ºæ‰€æœ‰å…¶ä»–çš„Keyå‘é‡è¿›è¡Œæ‰“åˆ†ã€‚ å°† Value å‘é‡ä¹˜ä»¥å®ƒä»¬å¯¹åº”çš„åˆ†æ•°åæ±‚å’Œã€‚ Masked Self-Attentionåœ¨Self-Attentionçš„ç¬¬2æ­¥ï¼ŒæŠŠæœªæ¥çš„ token è¯„åˆ†è®¾ç½®ä¸º0ï¼Œå› æ­¤æ¨¡å‹ä¸èƒ½çœ‹åˆ°æœªæ¥çš„è¯ã€‚ è¿™ä¸ªå±è”½ï¼ˆmaskingï¼‰ç»å¸¸ç”¨ä¸€ä¸ªçŸ©é˜µæ¥å®ç°ï¼Œç§°ä¸º attention maskçŸ©é˜µã€‚ GPT-2ä¸­çš„Self-Attention(skip) è‡ªå›å½’è¯­è¨€æ¨¡å‹çš„åº”ç”¨åº”ç”¨åœ¨ä¸‹æ¸¸å¹¶å–å¾—ä¸é”™æ•ˆæœçš„NLPä»»åŠ¡æœ‰ï¼šæœºå™¨ç¿»è¯‘ã€æ‘˜è¦ç”Ÿæˆã€éŸ³ä¹ç”Ÿæˆã€‚ï¼ˆå¯è§ï¼Œä¸»è¦æ˜¯è·Ÿé¢„è®­ç»ƒä»»åŠ¡ç›¸ä¼¼çš„ç”Ÿæˆç±»ä»»åŠ¡ã€‚ï¼‰ å‚è€ƒèµ„æ–™ åŸºäºtransformersçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨â€“åœ¨çº¿é˜…è¯» https://datawhalechina.github.io/learn-nlp-with-transformers/#/","categories":[{"name":"04 ç»„é˜Ÿå­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬29æœŸ åŸºäºtransformerçš„NLP","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬29æœŸ-åŸºäºtransformerçš„NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"ç¬”è®°","slug":"ç¬”è®°","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"ç»„é˜Ÿå­¦ä¹ ","slug":"ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"é¢„è®­ç»ƒæ¨¡å‹","slug":"é¢„è®­ç»ƒæ¨¡å‹","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"GPT","slug":"GPT","permalink":"http://example.com/tags/GPT/"}]},{"title":"Chapter01-02 PyTorchçš„ç®€ä»‹å’Œå®‰è£…ã€PyTorchåŸºç¡€çŸ¥è¯†","slug":"pytorch-chap01-02","date":"2021-09-18T02:26:03.000Z","updated":"2021-10-13T11:05:03.457Z","comments":true,"path":"pytorch-chap01-02/","link":"","permalink":"http://example.com/pytorch-chap01-02/","excerpt":"","text":"ç¬¬ä¸€ç«  PyTorchçš„ç®€ä»‹å’Œå®‰è£…PyTorchç®€ä»‹PyTorchæ˜¯ç”±Facebookäººå·¥æ™ºèƒ½ç ”ç©¶å°ç»„å¼€å‘çš„ä¸€ç§åŸºäºLuaç¼–å†™çš„Torchåº“çš„Pythonå®ç°çš„æ·±åº¦å­¦ä¹ åº“ï¼Œç›®å‰è¢«å¹¿æ³›åº”ç”¨äºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œï¼Œè€Œéšç€Caffe2é¡¹ç›®å¹¶å…¥Pytorchï¼Œ Pytorchå¼€å§‹å½±å“åˆ°TensorFlowåœ¨æ·±åº¦å­¦ä¹ åº”ç”¨æ¡†æ¶é¢†åŸŸçš„åœ°ä½ã€‚æ€»çš„æ¥è¯´ï¼ŒPyTorchæ˜¯å½“å‰éš¾å¾—çš„ç®€æ´ä¼˜é›…ä¸”é«˜æ•ˆå¿«é€Ÿçš„æ¡†æ¶ã€‚å› æ­¤æœ¬è¯¾ç¨‹æˆ‘ä»¬é€‰æ‹©äº†PyTorchæ¥è¿›è¡Œå¼€æºå­¦ä¹ ã€‚ PyTorchçš„å®‰è£…PyTorchå®˜ç½‘ï¼šhttps://pytorch.org/ PyTorchçš„å‘å±•å’Œä¼˜åŠ¿â€œAll in Pytorchâ€. PyTorch VS TensorFlow ç¬¬äºŒç«  PyTorchçš„åŸºç¡€çŸ¥è¯†Tensor/å¼ é‡å¼ é‡æ˜¯åŸºäºå‘é‡å’ŒçŸ©é˜µçš„æ¨å¹¿ï¼Œæ¯”å¦‚æˆ‘ä»¬å¯ä»¥å°†æ ‡é‡è§†ä¸ºé›¶é˜¶å¼ é‡ï¼ŒçŸ¢é‡å¯ä»¥è§†ä¸ºä¸€é˜¶å¼ é‡ï¼ŒçŸ©é˜µå°±æ˜¯äºŒé˜¶å¼ é‡ã€‚ 0ç»´å¼ é‡/æ ‡é‡ æ ‡é‡æ˜¯1ä¸ªæ•°å­— 1ç»´å¼ é‡/å‘é‡ 1ç»´å¼ é‡ç§°ä¸ºâ€œå‘é‡â€ 2ç»´å¼ é‡ 2ç»´å¼ é‡ç§°ä¸ºâ€œçŸ©é˜µâ€ 3ç»´å¼ é‡ æ—¶é—´åºåˆ—æ•°æ®ã€è‚¡ä»·ã€æ–‡æœ¬æ•°æ®ã€å½©è‰²å›¾ç‰‡(RGB) 4ç»´=å›¾åƒ 5ç»´=è§†é¢‘ åœ¨PyTorchä¸­ï¼Œ torch.Tensor æ˜¯å­˜å‚¨å’Œå˜æ¢æ•°æ®çš„ä¸»è¦å·¥å…·ã€‚ tensor-æ„é€ åˆ›å»ºä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„çŸ©é˜µï¼š 123x = torch.rand(4, 3) # æ„é€ å¼ é‡print(x.size()) # è·å–ç»´åº¦ä¿¡æ¯print(x.shape) # è·å–ç»´åº¦ä¿¡æ¯ è¿˜æœ‰ä¸€äº›å¸¸è§çš„æ„é€ Tensorçš„å‡½æ•°ï¼š PyTorchä¸­çš„Tensoræ”¯æŒè¶…è¿‡ä¸€ç™¾ç§æ“ä½œï¼ŒåŒ…æ‹¬è½¬ç½®ã€ç´¢å¼•ã€åˆ‡ç‰‡ã€æ•°å­¦è¿ç®—ã€çº¿æ€§ä»£æ•°ã€éšæœºæ•°ç­‰ç­‰ï¼Œå¯å‚è€ƒå®˜æ–¹æ–‡æ¡£ã€‚ tensor-squeeze å¢åŠ /åˆ é™¤ä¸€ä¸ªç»´åº¦ tensor-transpose è½¬ç½® tensor-cat concatenateå¤šä¸ªtensor è‡ªåŠ¨æ±‚å¯¼/è‡ªåŠ¨å¾®åˆ†PyTorchä¸­ï¼Œæ‰€æœ‰ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ˜¯autogradåŒ…ã€‚autogradåŒ…ä¸ºå¼ é‡ä¸Šçš„æ‰€æœ‰æ“ä½œæä¾›äº†è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶ã€‚ How to Calculate Gradient 1234567891011121314151617&gt;&gt;&gt; x = torch.tensor([[1., 0.], [-1., 1.]], requires_grad = True)&gt;&gt;&gt; xtensor([[ 1., 0.], [-1., 1.]], requires_grad=True)&gt;&gt;&gt; z = x.pow(2)&gt;&gt;&gt; ztensor([[1., 0.], [1., 1.]], grad_fn=&lt;PowBackward0&gt;)&gt;&gt;&gt; z = z.sum()&gt;&gt;&gt; ztensor(3., grad_fn=&lt;SumBackward0&gt;)&gt;&gt;&gt; z.backward()&gt;&gt;&gt; ztensor(3., grad_fn=&lt;SumBackward0&gt;)&gt;&gt;&gt; x.gradtensor([[ 2., 0.], [-2., 2.]]) å¹¶è¡Œè®¡ç®—ç®€ä»‹åœ¨åˆ©ç”¨PyTorchåšæ·±åº¦å­¦ä¹ çš„è¿‡ç¨‹ä¸­ï¼Œå¯èƒ½ä¼šé‡åˆ°æ•°æ®é‡è¾ƒå¤§æ— æ³•åœ¨å•å—GPUä¸Šå®Œæˆï¼Œæˆ–è€…éœ€è¦æå‡è®¡ç®—é€Ÿåº¦çš„åœºæ™¯ï¼Œè¿™æ—¶å°±éœ€è¦ç”¨åˆ°å¹¶è¡Œè®¡ç®—ã€‚GPUçš„å‡ºç°è®©æˆ‘ä»¬å¯ä»¥è®­ç»ƒçš„æ›´å¿«ï¼Œæ›´å¥½ã€‚PyTorchå¯ä»¥åœ¨ç¼–å†™å®Œæ¨¡å‹ä¹‹åï¼Œè®©å¤šä¸ªGPUæ¥å‚ä¸è®­ç»ƒã€‚ CUDAæ˜¯æˆ‘ä»¬ä½¿ç”¨GPUçš„æä¾›å•†â€”â€”NVIDIAæä¾›çš„GPUå¹¶è¡Œè®¡ç®—æ¡†æ¶ã€‚å¯¹äºGPUæœ¬èº«çš„ç¼–ç¨‹ï¼Œä½¿ç”¨çš„æ˜¯CUDAè¯­è¨€æ¥å®ç°çš„ã€‚ä½†æ˜¯ï¼Œåœ¨æˆ‘ä»¬ä½¿ç”¨PyTorchç¼–å†™æ·±åº¦å­¦ä¹ ä»£ç æ—¶ï¼Œä½¿ç”¨çš„CUDAåˆæ˜¯å¦ä¸€ä¸ªæ„æ€ã€‚åœ¨PyTorchä½¿ç”¨ CUDAè¡¨ç¤ºè¦å¼€å§‹è¦æ±‚æˆ‘ä»¬çš„æ¨¡å‹æˆ–è€…æ•°æ®å¼€å§‹ä½¿ç”¨GPUäº†ã€‚ åœ¨ç¼–å†™ç¨‹åºä¸­ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨äº† cuda() æ—¶ï¼Œå…¶åŠŸèƒ½æ˜¯è®©æˆ‘ä»¬çš„æ¨¡å‹æˆ–è€…æ•°æ®è¿ç§»åˆ°GPUå½“ä¸­ï¼Œé€šè¿‡GPUå¼€å§‹è®¡ç®—ã€‚ ä¸åŒçš„æ•°æ®åˆ†å¸ƒåˆ°ä¸åŒçš„è®¾å¤‡ä¸­ï¼Œæ‰§è¡Œç›¸åŒçš„ä»»åŠ¡(Data parallelism): å‚è€ƒèµ„æ–™ Datawhaleå¼€æºé¡¹ç›®ï¼šæ·±å…¥æµ…å‡ºPyTorch https://github.com/datawhalechina/thorough-pytorch/ æå®æ¯…æœºå™¨å­¦ä¹ 2021æ˜¥-PyTorch Tutorial https://www.bilibili.com/video/BV1Wv411h7kN?p=5 What is a gpu and do you need one in deep learning https://towardsdatascience.com/what-is-a-gpu-and-do-you-need-one-in-deep-learning-718b9597aa0d åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ pytorchç‰ˆ https://zh-v2.d2l.ai/chapter_preface/index.html","categories":[{"name":"04 ç»„é˜Ÿå­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬30æœŸ æ·±å…¥æµ…å‡ºPyTorch","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬30æœŸ-æ·±å…¥æµ…å‡ºPyTorch","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAPyTorch/"}],"tags":[{"name":"ç¬”è®°","slug":"ç¬”è®°","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"ç»„é˜Ÿå­¦ä¹ ","slug":"ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://example.com/tags/PyTorch/"}]},{"title":"Task03 å­¦ä¹ BERT","slug":"nlp-transformer-task03","date":"2021-09-17T01:44:18.000Z","updated":"2021-10-02T09:21:28.431Z","comments":true,"path":"nlp-transformer-task03/","link":"","permalink":"http://example.com/nlp-transformer-task03/","excerpt":"","text":"BERTç®€ä»‹BERTé¦–å…ˆåœ¨å¤§è§„æ¨¡æ— ç›‘ç£è¯­æ–™ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨é¢„è®­ç»ƒå¥½çš„å‚æ•°åŸºç¡€ä¸Šå¢åŠ ä¸€ä¸ªä¸ä»»åŠ¡ç›¸å…³çš„ç¥ç»ç½‘ç»œå±‚ï¼Œå¹¶åœ¨è¯¥ä»»åŠ¡çš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒè®­ï¼Œæœ€ç»ˆå–å¾—å¾ˆå¥½çš„æ•ˆæœã€‚BERTçš„è¿™ä¸ªè®­ç»ƒè¿‡ç¨‹å¯ä»¥ç®€è¿°ä¸ºï¼šé¢„è®­ç»ƒï¼ˆpre-trainï¼‰+å¾®è°ƒï¼ˆfine-tune/fine-tuningï¼‰ï¼Œå·²ç»æˆä¸ºæœ€è¿‘å‡ å¹´æœ€æµè¡Œçš„NLPè§£å†³æ–¹æ¡ˆçš„èŒƒå¼ã€‚ å¦‚ä½•ç›´æ¥åº”ç”¨BERT ä¸‹è½½åœ¨æ— ç›‘ç£è¯­æ–™ä¸Šé¢„è®­ç»ƒå¥½çš„BERTæ¨¡å‹ï¼Œä¸€èˆ¬æ¥è¯´å¯¹åº”äº†3ä¸ªæ–‡ä»¶ï¼šBERTæ¨¡å‹é…ç½®æ–‡ä»¶ï¼ˆç”¨æ¥ç¡®å®šTransformerçš„å±‚æ•°ï¼Œéšè—å±‚å¤§å°ç­‰ï¼‰ï¼ŒBERTæ¨¡å‹å‚æ•°ï¼ŒBERTè¯è¡¨ï¼ˆBERTæ‰€èƒ½å¤„ç†çš„æ‰€æœ‰tokenï¼‰ã€‚ é’ˆå¯¹ç‰¹å®šä»»åŠ¡éœ€è¦ï¼Œåœ¨BERTæ¨¡å‹ä¸Šå¢åŠ ä¸€ä¸ªä»»åŠ¡ç›¸å…³çš„ç¥ç»ç½‘ç»œï¼Œæ¯”å¦‚ä¸€ä¸ªç®€å•çš„åˆ†ç±»å™¨ï¼Œç„¶ååœ¨ç‰¹å®šä»»åŠ¡ç›‘ç£æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒè®­ç»ƒã€‚ï¼ˆå¾®è°ƒçš„ä¸€ç§ç†è§£ï¼šå­¦ä¹ ç‡è¾ƒå°ï¼Œè®­ç»ƒepochæ•°é‡è¾ƒå°‘ï¼Œå¯¹æ¨¡å‹æ•´ä½“å‚æ•°è¿›è¡Œè½»å¾®è°ƒæ•´ï¼‰ BERTçš„ç»“æ„BERTæ¨¡å‹ç»“æ„åŸºæœ¬ä¸Šå°±æ˜¯Transformerçš„encoderéƒ¨åˆ†ã€‚ BERTçš„è¾“å…¥å’Œè¾“å‡ºBERTæ¨¡å‹è¾“å…¥æœ‰ä¸€ç‚¹ç‰¹æ®Šçš„åœ°æ–¹æ˜¯åœ¨ä¸€å¥è¯æœ€å¼€å§‹æ‹¼æ¥äº†ä¸€ä¸ª[CLS] tokenï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚è¿™ä¸ªç‰¹æ®Šçš„[CLS] tokenç»è¿‡BERTå¾—åˆ°çš„å‘é‡è¡¨ç¤ºé€šå¸¸è¢«ç”¨ä½œå½“å‰çš„å¥å­è¡¨ç¤ºã€‚æˆ‘ä»¬ç›´æ¥ä½¿ç”¨ç¬¬1ä¸ªä½ç½®çš„å‘é‡è¾“å‡ºï¼ˆå¯¹åº”çš„æ˜¯[CLS]ï¼‰ä¼ å…¥classifierç½‘ç»œï¼Œç„¶åè¿›è¡Œåˆ†ç±»ä»»åŠ¡ã€‚ BERTçš„é¢„è®­ç»ƒä»»åŠ¡BERTæ˜¯ä¸€ä¸ªå¤šä»»åŠ¡æ¨¡å‹ï¼Œå®ƒçš„ä»»åŠ¡æ˜¯ç”±ä¸¤ä¸ªè‡ªç›‘ç£ä»»åŠ¡ç»„æˆã€‚ Masked Language Modelï¼ˆMLMï¼‰MLMï¼šå°†è¾“å…¥æ–‡æœ¬åºåˆ—çš„éƒ¨åˆ†ï¼ˆ15%ï¼‰å•è¯éšæœºMaskæ‰ï¼Œè®©BERTæ¥é¢„æµ‹è¿™äº›è¢«Maskçš„è¯è¯­ã€‚ï¼ˆå¯ä»¥è¯´æ˜¯å®Œå½¢å¡«ç©ºï¼‰ Masked Language Modelï¼ˆMLMï¼‰å’Œæ ¸å¿ƒæ€æƒ³å–è‡ªWilson Tayloråœ¨1953å¹´å‘è¡¨çš„ä¸€ç¯‡è®ºæ–‡ã€Šcloze procedure: A new tool for measuring readabilityã€‹ã€‚æ‰€è°“MLMæ˜¯æŒ‡åœ¨è®­ç»ƒçš„æ—¶å€™éšå³ä»è¾“å…¥é¢„æ–™ä¸Šmaskæ‰ä¸€äº›å•è¯ï¼Œç„¶åé€šè¿‡çš„ä¸Šä¸‹æ–‡é¢„æµ‹è¯¥å•è¯ï¼Œè¯¥ä»»åŠ¡éå¸¸åƒæˆ‘ä»¬åœ¨ä¸­å­¦æ—¶æœŸç»å¸¸åšçš„å®Œå½¢å¡«ç©ºã€‚æ­£å¦‚ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹ç®—æ³•å’ŒRNNåŒ¹é…é‚£æ ·ï¼ŒMLMçš„è¿™ä¸ªæ€§è´¨å’ŒTransformerçš„ç»“æ„æ˜¯éå¸¸åŒ¹é…çš„ã€‚ Next Sentence Predictionï¼ˆNSPï¼‰NSPï¼šåˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦æ˜¯ç›¸é‚»å¥å­ã€‚å³ï¼Œè¾“å…¥æ˜¯sentence Aå’Œsentence Bï¼Œç»è¿‡BERTç¼–ç ä¹‹åï¼Œä½¿ç”¨CLS tokençš„å‘é‡è¡¨ç¤ºæ¥é¢„æµ‹ä¸¤ä¸ªå¥å­æ˜¯å¦æ˜¯ç›¸é‚»å¥å­ã€‚ Next Sentence Predictionï¼ˆNSPï¼‰çš„ä»»åŠ¡æ˜¯åˆ¤æ–­å¥å­Bæ˜¯å¦æ˜¯å¥å­Açš„ä¸‹æ–‡ã€‚å¦‚æœæ˜¯çš„è¯è¾“å‡ºâ€™IsNextâ€˜ï¼Œå¦åˆ™è¾“å‡ºâ€™NotNextâ€˜ã€‚è®­ç»ƒæ•°æ®çš„ç”Ÿæˆæ–¹å¼æ˜¯ä»å¹³è¡Œè¯­æ–™ä¸­éšæœºæŠ½å–çš„è¿ç»­ä¸¤å¥è¯ï¼Œå…¶ä¸­50%ä¿ç•™æŠ½å–çš„ä¸¤å¥è¯ï¼Œå®ƒä»¬ç¬¦åˆIsNextå…³ç³»ï¼Œå¦å¤–50%çš„ç¬¬äºŒå¥è¯æ˜¯éšæœºä»é¢„æ–™ä¸­æå–çš„ï¼Œå®ƒä»¬çš„å…³ç³»æ˜¯NotNextçš„ã€‚è¿™ä¸ªå…³ç³»ä¿å­˜åœ¨[CLS]ç¬¦å·ä¸­ã€‚ BERTçš„åº”ç”¨ç‰¹å¾æå–ç”±äºBERTæ¨¡å‹å¯ä»¥å¾—åˆ°è¾“å…¥åºåˆ—æ‰€å¯¹åº”çš„æ‰€æœ‰tokençš„å‘é‡è¡¨ç¤ºï¼Œå› æ­¤ä¸ä»…å¯ä»¥ä½¿ç”¨æœ€åä¸€ç¨‹BERTçš„è¾“å‡ºè¿æ¥ä¸Šä»»åŠ¡ç½‘ç»œè¿›è¡Œå¾®è°ƒï¼Œè¿˜å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™äº›tokençš„å‘é‡å½“ä½œç‰¹å¾ã€‚æ¯”å¦‚ï¼Œå¯ä»¥ç›´æ¥æå–æ¯ä¸€å±‚encoderçš„tokenè¡¨ç¤ºå½“ä½œç‰¹å¾ï¼Œè¾“å…¥ç°æœ‰çš„ç‰¹å®šä»»åŠ¡ç¥ç»ç½‘ç»œä¸­è¿›è¡Œè®­ç»ƒã€‚ Pretrain + Fine tuneå‚è€ƒèµ„æ–™ åŸºäºtransformersçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨â€“åœ¨çº¿é˜…è¯» https://datawhalechina.github.io/learn-nlp-with-transformers/#/ æå®æ¯…æœºå™¨å­¦ä¹ 2019-ELMO,BERT,GPT https:// www.bilibili.com/video/BV1Gb411n7dE?p=61","categories":[{"name":"04 ç»„é˜Ÿå­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬29æœŸ åŸºäºtransformerçš„NLP","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬29æœŸ-åŸºäºtransformerçš„NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"ç¬”è®°","slug":"ç¬”è®°","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"ç»„é˜Ÿå­¦ä¹ ","slug":"ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"é¢„è®­ç»ƒæ¨¡å‹","slug":"é¢„è®­ç»ƒæ¨¡å‹","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"BERT","slug":"BERT","permalink":"http://example.com/tags/BERT/"}]},{"title":"Task02 å­¦ä¹ Attentioinå’ŒTransformer","slug":"nlp-transformer-task02","date":"2021-09-17T01:09:24.000Z","updated":"2021-10-02T09:21:22.586Z","comments":true,"path":"nlp-transformer-task02/","link":"","permalink":"http://example.com/nlp-transformer-task02/","excerpt":"","text":"Attentionseq2seqseq2seqæ˜¯ä¸€ç§å¸¸è§çš„NLPæ¨¡å‹ç»“æ„ï¼Œå…¨ç§°æ˜¯ï¼šsequence to sequenceï¼Œç¿»è¯‘ä¸ºâ€œåºåˆ—åˆ°åºåˆ—â€ã€‚é¡¾åæ€ä¹‰ï¼šä»ä¸€ä¸ªæ–‡æœ¬åºåˆ—å¾—åˆ°ä¸€ä¸ªæ–°çš„æ–‡æœ¬åºåˆ—ã€‚å…¸å‹çš„ä»»åŠ¡æœ‰ï¼šæœºå™¨ç¿»è¯‘ä»»åŠ¡ï¼Œæ–‡æœ¬æ‘˜è¦ä»»åŠ¡ã€‚ seq2seqæ¨¡å‹ç”±ç¼–ç å™¨ï¼ˆencoderï¼‰å’Œè§£ç å™¨ï¼ˆdecoderï¼‰ç»„æˆï¼Œç¼–ç å™¨ç”¨æ¥åˆ†æè¾“å…¥åºåˆ—ï¼Œè§£ç å™¨ç”¨æ¥ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚ç¼–ç å™¨ä¼šå¤„ç†è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼ŒæŠŠè¿™äº›ä¿¡æ¯è½¬æ¢æˆä¸ºä¸€ä¸ªèƒŒæ™¯å‘é‡ï¼ˆcontext vectorï¼‰ã€‚å½“æˆ‘ä»¬å¤„ç†å®Œæ•´ä¸ªè¾“å…¥åºåˆ—åï¼Œç¼–ç å™¨æŠŠèƒŒæ™¯å‘é‡å‘é€ç»™è§£ç å™¨ï¼Œè§£ç å™¨é€šè¿‡èƒŒæ™¯å‘é‡ä¸­çš„ä¿¡æ¯ï¼Œé€ä¸ªå…ƒç´ è¾“å‡ºæ–°çš„åºåˆ—ã€‚ åœ¨transformeræ¨¡å‹ä¹‹å‰ï¼Œseq2seqä¸­çš„ç¼–ç å™¨å’Œè§£ç å™¨ä¸€èˆ¬é‡‡ç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œè™½ç„¶éå¸¸ç»å…¸ï¼Œä½†æ˜¯å±€é™æ€§ä¹Ÿéå¸¸å¤§ã€‚æœ€å¤§çš„å±€é™æ€§å°±åœ¨äºç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„å”¯ä¸€è”ç³»å°±æ˜¯ä¸€ä¸ªå›ºå®šé•¿åº¦çš„contextå‘é‡ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç¼–ç å™¨è¦å°†æ•´ä¸ªåºåˆ—çš„ä¿¡æ¯å‹ç¼©è¿›ä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡ä¸­ã€‚è¿™æ ·åšå­˜åœ¨ä¸¤ä¸ªå¼Šç«¯ï¼š è¯­ä¹‰å‘é‡å¯èƒ½æ— æ³•å®Œå…¨è¡¨ç¤ºæ•´ä¸ªåºåˆ—çš„ä¿¡æ¯ å…ˆè¾“å…¥åˆ°ç½‘ç»œçš„å†…å®¹æºå¸¦çš„ä¿¡æ¯ä¼šè¢«åè¾“å…¥çš„ä¿¡æ¯è¦†ç›–æ‰ï¼Œè¾“å…¥åºåˆ—è¶Šé•¿ï¼Œè¿™ä¸ªç°è±¡å°±è¶Šä¸¥é‡ Attentionä¸ºäº†è§£å†³seq2seqæ¨¡å‹ä¸­çš„ä¸¤ä¸ªå¼Šç«¯ï¼ŒBahdanauç­‰äººåœ¨è®ºæ–‡ã€ŠNeural Machine Translation by Jointly Learning to Align and Translateã€‹ä¸­æå‡ºä½¿ç”¨Attentionæœºåˆ¶ï¼Œä½¿å¾—seq2seqæ¨¡å‹å¯ä»¥æœ‰åŒºåˆ†åº¦ã€æœ‰é‡ç‚¹åœ°å…³æ³¨è¾“å…¥åºåˆ—ï¼Œä»è€Œæå¤§åœ°æé«˜äº†æœºå™¨ç¿»è¯‘çš„è´¨é‡ã€‚ ä¸€ä¸ªæœ‰æ³¨æ„åŠ›æœºåˆ¶çš„seq2seqä¸ç»å…¸çš„seq2seqä¸»è¦æœ‰2ç‚¹ä¸åŒï¼š é¦–å…ˆï¼Œç¼–ç å™¨ä¼šæŠŠæ›´å¤šçš„æ•°æ®ä¼ é€’ç»™è§£ç å™¨ã€‚ç¼–ç å™¨æŠŠæ‰€æœ‰æ—¶é—´æ­¥çš„ hidden stateï¼ˆéšè—å±‚çŠ¶æ€ï¼‰ä¼ é€’ç»™è§£ç å™¨ï¼Œè€Œä¸æ˜¯åªä¼ é€’æœ€åä¸€ä¸ª hidden stateï¼ˆéšè—å±‚çŠ¶æ€ï¼‰ æ³¨æ„åŠ›æ¨¡å‹çš„è§£ç å™¨åœ¨äº§ç”Ÿè¾“å‡ºä¹‹å‰ï¼Œåšäº†ä¸€ä¸ªé¢å¤–çš„attentionå¤„ç† Transformeræ¨¡å‹æ¶æ„transformeråŸè®ºæ–‡çš„æ¶æ„å›¾ï¼š ä¸€ä¸ªæ›´æ¸…æ™°çš„æ¶æ„å›¾ï¼š ä»è¾“å…¥åˆ°è¾“å‡ºæ‹†å¼€çœ‹å°±æ˜¯ï¼š INPUTï¼šinput vector + position encoding ENCODERsï¼ˆÃ—6ï¼‰ï¼Œand each encoder includesï¼š input multi-head self-attention residual connection&amp;norm full-connected network residual connection&amp;norm output DECODERsï¼ˆÃ—6ï¼‰ï¼Œand each decoder includesï¼š input Masked multihead self-attention residual connection&amp;norm multi-head self-attention residual connection&amp;norm full-connected network residual connection&amp;norm output OUTPUTï¼š output (decoderâ€™s) linear layer softmax layer output æ¨¡å‹è¾“å…¥è¯å‘é‡å’Œå¸¸è§çš„NLPä»»åŠ¡ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆä¼šä½¿ç”¨è¯åµŒå…¥ç®—æ³•ï¼ˆembeddingï¼‰ï¼Œå°†è¾“å…¥æ–‡æœ¬åºåˆ—çš„æ¯ä¸ªè¯è½¬æ¢ä¸ºä¸€ä¸ªè¯å‘é‡ã€‚ ä½ç½®å‘é‡Transformeræ¨¡å‹å¯¹æ¯ä¸ªè¾“å…¥çš„è¯å‘é‡éƒ½åŠ ä¸Šäº†ä¸€ä¸ªä½ç½®å‘é‡ã€‚è¿™äº›å‘é‡æœ‰åŠ©äºç¡®å®šæ¯ä¸ªå•è¯çš„ä½ç½®ç‰¹å¾ï¼Œæˆ–è€…å¥å­ä¸­ä¸åŒå•è¯ä¹‹é—´çš„è·ç¦»ç‰¹å¾ã€‚è¯å‘é‡åŠ ä¸Šä½ç½®å‘é‡èƒŒåçš„ç›´è§‰æ˜¯ï¼šå°†è¿™äº›è¡¨ç¤ºä½ç½®çš„å‘é‡æ·»åŠ åˆ°è¯å‘é‡ä¸­ï¼Œå¾—åˆ°çš„æ–°å‘é‡ï¼Œå¯ä»¥ä¸ºæ¨¡å‹æä¾›æ›´å¤šæœ‰æ„ä¹‰çš„ä¿¡æ¯ï¼Œæ¯”å¦‚è¯çš„ä½ç½®ï¼Œè¯ä¹‹é—´çš„è·ç¦»ç­‰ã€‚ ï¼ˆç”Ÿæˆä½ç½®ç¼–ç å‘é‡çš„æ–¹æ³•æœ‰å¾ˆå¤šç§ï¼‰ ç¼–ç å™¨å’Œè§£ç å™¨æ³¨ï¼š1. ç¼–ç å™¨å’Œè§£ç å™¨ä¸­æœ‰ç›¸ä¼¼çš„æ¨¡å—å’Œç»“æ„ï¼Œæ‰€ä»¥åˆå¹¶åˆ°ä¸€èµ·ä»‹ç»ã€‚2. æœ¬éƒ¨åˆ†æŒ‰ç…§æå®æ¯…è€å¸ˆçš„Attentionï¼ŒTransformeréƒ¨åˆ†çš„è¯¾ç¨‹PPTæ¥ï¼Œå› ä¸ºleeçš„è¯¾ç¨‹å¯¹æ–°æ‰‹æ›´å‹å¥½ã€‚ Self-Attentionself-attentionå¯¹äºæ¯ä¸ªå‘é‡éƒ½ä¼šè€ƒè™‘æ•´ä¸ªsequenceçš„ä¿¡æ¯åè¾“å‡ºä¸€ä¸ªå‘é‡ï¼Œself-attentionç»“æ„å¦‚ä¸‹ï¼šFCï¼šFully-connected network å…¨è¿æ¥ç½‘ç»œai: è¾“å…¥å˜é‡ã€‚å¯èƒ½æ˜¯æ•´ä¸ªç½‘ç»œçš„è¾“å…¥ï¼Œä¹Ÿå¯èƒ½æ˜¯æŸä¸ªéšè—å±‚çš„è¾“å‡ºbi: è€ƒè™‘æ•´ä¸ªsequenceä¿¡æ¯åçš„è¾“å‡ºå˜é‡ çŸ©é˜µè®¡ç®—ï¼šç›®æ ‡ï¼šæ ¹æ®è¾“å…¥å‘é‡çŸ©é˜µIï¼Œè®¡ç®—è¾“å‡ºå‘é‡çŸ©é˜µOã€‚çŸ©é˜µè¿ç®—è¿‡ç¨‹ï¼š çŸ©é˜µIåˆ†åˆ«ä¹˜ä»¥Wq, Wk, Wvï¼ˆå‚æ•°çŸ©é˜µï¼Œéœ€è¦æ¨¡å‹è¿›è¡Œå­¦ä¹ ï¼‰ï¼Œå¾—åˆ°çŸ©é˜µQ, K, Vã€‚ çŸ©é˜µKçš„è½¬ç½®ä¹˜ä»¥Qï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡çŸ©é˜µAï¼Œå½’ä¸€åŒ–å¾—åˆ°çŸ©é˜µAâ€™ã€‚ çŸ©é˜µVä¹˜çŸ©é˜µAâ€˜ï¼Œå¾—åˆ°è¾“å‡ºå‘é‡çŸ©é˜µOã€‚ Multi Head Self-Attentionç®€å•åœ°è¯´ï¼Œå¤šäº†å‡ ç»„Qï¼ŒKï¼ŒVã€‚åœ¨Self-Attentionä¸­ï¼Œæˆ‘ä»¬æ˜¯ä½¿ç”¨ğ‘å»å¯»æ‰¾ä¸ä¹‹ç›¸å…³çš„ğ‘˜ï¼Œä½†æ˜¯è¿™ä¸ªç›¸å…³æ€§å¹¶ä¸ä¸€å®šæœ‰ä¸€ç§ã€‚é‚£å¤šç§ç›¸å…³æ€§ä½“ç°åˆ°è®¡ç®—æ–¹å¼ä¸Šå°±æ˜¯æœ‰å¤šä¸ªçŸ©é˜µğ‘ï¼Œä¸åŒçš„ğ‘è´Ÿè´£ä»£è¡¨ä¸åŒçš„ç›¸å…³æ€§ã€‚ Transformer çš„è®ºæ–‡é€šè¿‡å¢åŠ å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆä¸€ç»„æ³¨æ„åŠ›ç§°ä¸ºä¸€ä¸ª attention headï¼‰ï¼Œè¿›ä¸€æ­¥å®Œå–„äº†Self-Attentionã€‚è¿™ç§æœºåˆ¶ä»å¦‚ä¸‹ä¸¤ä¸ªæ–¹é¢å¢å¼ºäº†attentionå±‚çš„èƒ½åŠ›ï¼š å®ƒæ‰©å±•äº†æ¨¡å‹å…³æ³¨ä¸åŒä½ç½®çš„èƒ½åŠ›ã€‚ å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶èµ‹äºˆattentionå±‚å¤šä¸ªâ€œå­è¡¨ç¤ºç©ºé—´â€ã€‚ æ®‹å·®é“¾æ¥å’Œå½’ä¸€åŒ–æ®‹å·®é“¾æ¥ï¼šä¸€ç§æŠŠinputå‘é‡å’Œoutputå‘é‡ç›´æ¥åŠ èµ·æ¥çš„æ¶æ„ã€‚å½’ä¸€åŒ–ï¼šæŠŠæ•°æ®æ˜ å°„åˆ°0ï½1èŒƒå›´ä¹‹å†…å¤„ç†ã€‚ æ¨¡å‹è¾“å‡ºçº¿æ€§å±‚å’ŒsoftmaxDecoder æœ€ç»ˆçš„è¾“å‡ºæ˜¯ä¸€ä¸ªå‘é‡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯æµ®ç‚¹æ•°ã€‚æˆ‘ä»¬æ€ä¹ˆæŠŠè¿™ä¸ªå‘é‡è½¬æ¢ä¸ºå•è¯å‘¢ï¼Ÿè¿™æ˜¯çº¿æ€§å±‚å’Œsoftmaxå®Œæˆçš„ã€‚ çº¿æ€§å±‚å°±æ˜¯ä¸€ä¸ªæ™®é€šçš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œå¯ä»¥æŠŠè§£ç å™¨è¾“å‡ºçš„å‘é‡ï¼Œæ˜ å°„åˆ°ä¸€ä¸ªæ›´å¤§çš„å‘é‡ï¼Œè¿™ä¸ªå‘é‡ç§°ä¸º logits å‘é‡ï¼šå‡è®¾æˆ‘ä»¬çš„æ¨¡å‹æœ‰ 10000 ä¸ªè‹±è¯­å•è¯ï¼ˆæ¨¡å‹çš„è¾“å‡ºè¯æ±‡è¡¨ï¼‰ï¼Œæ­¤ logits å‘é‡ä¾¿ä¼šæœ‰ 10000 ä¸ªæ•°å­—ï¼Œæ¯ä¸ªæ•°è¡¨ç¤ºä¸€ä¸ªå•è¯çš„åˆ†æ•°ã€‚ ç„¶åï¼ŒSoftmax å±‚ä¼šæŠŠè¿™äº›åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆæŠŠæ‰€æœ‰çš„åˆ†æ•°è½¬æ¢ä¸ºæ­£æ•°ï¼Œå¹¶ä¸”åŠ èµ·æ¥ç­‰äº 1ï¼‰ã€‚ç„¶åé€‰æ‹©æœ€é«˜æ¦‚ç‡çš„é‚£ä¸ªæ•°å­—å¯¹åº”çš„è¯ï¼Œå°±æ˜¯è¿™ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºå•è¯ã€‚ æŸå¤±å‡½æ•°Transformerè®­ç»ƒçš„æ—¶å€™ï¼Œéœ€è¦å°†è§£ç å™¨çš„è¾“å‡ºå’Œlabelä¸€åŒé€å…¥æŸå¤±å‡½æ•°ï¼Œä»¥è·å¾—lossï¼Œæœ€ç»ˆæ¨¡å‹æ ¹æ®lossè¿›è¡Œæ–¹å‘ä¼ æ’­ã€‚ åªè¦Transformerè§£ç å™¨é¢„æµ‹äº†ç»„æ¦‚ç‡ï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠè¿™ç»„æ¦‚ç‡å’Œæ­£ç¡®çš„è¾“å‡ºæ¦‚ç‡åšå¯¹æ¯”ï¼Œç„¶åä½¿ç”¨åå‘ä¼ æ’­æ¥è°ƒæ•´æ¨¡å‹çš„æƒé‡ï¼Œä½¿å¾—è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒæ›´åŠ æ¥è¿‘æ•´æ•°è¾“å‡ºã€‚ é‚£æˆ‘ä»¬è¦æ€ä¹ˆæ¯”è¾ƒä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒå‘¢ï¼Ÿï¼šæˆ‘ä»¬å¯ä»¥ç®€å•çš„ç”¨ä¸¤ç»„æ¦‚ç‡å‘é‡çš„çš„ç©ºé—´è·ç¦»ä½œä¸ºlossï¼ˆå‘é‡ç›¸å‡ï¼Œç„¶åæ±‚å¹³æ–¹å’Œï¼Œå†å¼€æ–¹ï¼‰ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥ä½¿ç”¨äº¤å‰ç†µ(cross-entropy)]å’ŒKL æ•£åº¦(Kullbackâ€“Leibler divergence)ã€‚ å‚è€ƒèµ„æ–™ç†è®ºéƒ¨åˆ†[1] (å¼ºæ¨)æå®æ¯…2021æ˜¥æœºå™¨å­¦ä¹ è¯¾ç¨‹ https://www.bilibili.com/video/BV1Wv411h7kN?from=search&amp;seid=17090062977285779802&amp;spm_id_from=333.337.0.0[2] åŸºäºtransformersçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨ï¼ˆæ¶µç›–äº†å›¾è§£ç³»åˆ—ã€annotated transformerã€huggingfaceï¼‰ https://github.com/datawhalechina/learn-nlp-with-transformers[3] å›¾è§£transformer|The Illustrated Transformer http://jalammar.github.io/illustrated-transformer/[4] å›¾è§£seq2seq, attention|Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ ä»£ç éƒ¨åˆ†[5] The Annotated Transformer http://nlp.seas.harvard.edu//2018/04/03/attention.html[6] Huggingface/transformers https://github.com/huggingface/transformers/blob/master/README_zh-hans.md è®ºæ–‡éƒ¨åˆ†Attention is all â€œweâ€ need. å…¶ä»–ä¸é”™çš„åšå®¢æˆ–æ•™ç¨‹[7] åŸºäºtransformersçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨â€“åœ¨çº¿é˜…è¯» https://datawhalechina.github.io/learn-nlp-with-transformers/#/[8] æå®æ¯…2021æ˜¥æœºå™¨å­¦ä¹ è¯¾ç¨‹ç¬”è®°â€”â€”è‡ªæ³¨æ„åŠ›æœºåˆ¶ https://www.cnblogs.com/sykline/p/14730088.html[9] æå®æ¯…2021æ˜¥æœºå™¨å­¦ä¹ è¯¾ç¨‹ç¬”è®°â€”â€”Transformeræ¨¡å‹ https://www.cnblogs.com/sykline/p/14785552.html[10] æå®æ¯…æœºå™¨å­¦ä¹ å­¦ä¹ ç¬”è®°â€”â€”è‡ªæ³¨æ„åŠ›æœºåˆ¶ https://blog.csdn.net/p_memory/article/details/116271274[11] è½¦ä¸‡ç¿”-è‡ªç„¶è¯­è¨€å¤„ç†æ–°èŒƒå¼ï¼šåŸºäºé¢„è®­ç»ƒçš„æ–¹æ³•ã€è®²åº§+PPTã€‘ https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true[12] è‹å‰‘æ—-ã€ŠAttention is All You Needã€‹æµ…è¯»ï¼ˆç®€ä»‹+ä»£ç ï¼‰https://spaces.ac.cn/archives/4765","categories":[{"name":"04 ç»„é˜Ÿå­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬29æœŸ åŸºäºtransformerçš„NLP","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬29æœŸ-åŸºäºtransformerçš„NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"ç¬”è®°","slug":"ç¬”è®°","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"ç»„é˜Ÿå­¦ä¹ ","slug":"ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"é¢„è®­ç»ƒæ¨¡å‹","slug":"é¢„è®­ç»ƒæ¨¡å‹","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"attention","slug":"attention","permalink":"http://example.com/tags/attention/"},{"name":"transfomer","slug":"transfomer","permalink":"http://example.com/tags/transfomer/"}]},{"title":"Task01 NLPå­¦ä¹ æ¦‚è§ˆ","slug":"nlp-transformer-task01","date":"2021-09-12T16:14:06.000Z","updated":"2021-10-02T09:21:16.372Z","comments":true,"path":"nlp-transformer-task01/","link":"","permalink":"http://example.com/nlp-transformer-task01/","excerpt":"","text":"NLPæ€ç»´å¯¼å›¾(æœ€è¿‘æ›´æ–°æ—¥æœŸï¼š2021-09-13) å‚è€ƒèµ„æ–™ Datawhale-åŸºäºtransformersçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨ https://github.com/datawhalechina/learn-nlp-with-transformers ã€Šè‡ªç„¶è¯­è¨€å¤„ç†-åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‹ https://item.jd.com/13344628.html åˆ˜çŸ¥è¿œè€å¸ˆ-NLPç ”ç©¶å…¥é—¨ä¹‹é“ https://github.com/zibuyu/research_tao","categories":[{"name":"04 ç»„é˜Ÿå­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬29æœŸ åŸºäºtransformerçš„NLP","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬29æœŸ-åŸºäºtransformerçš„NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"}],"tags":[{"name":"ç¬”è®°","slug":"ç¬”è®°","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"ç»„é˜Ÿå­¦ä¹ ","slug":"ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"é¢„è®­ç»ƒæ¨¡å‹","slug":"é¢„è®­ç»ƒæ¨¡å‹","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"}]},{"title":"CS61A Week1 Comupter_Science, Functions","slug":"cs61a-week1","date":"2021-09-01T08:46:52.000Z","updated":"2021-09-17T01:51:24.035Z","comments":true,"path":"cs61a-week1/","link":"","permalink":"http://example.com/cs61a-week1/","excerpt":"","text":"å‰è¨€ CS61Aä½œä¸º61ç³»åˆ—åŸºç¡€è¯¾ç¨‹çš„ç¬¬ä¸€é—¨è¯¾ç¨‹ï¼Œæ˜¯ä¸€é—¨è®¡ç®—æœºå…¥é—¨å¯¼è®ºè¯¾ç¨‹ï¼Œä¼¯å…‹åˆ©å¤§ä¸€æ–°ç”Ÿçš„ç¬¬ä¸€é—¨è®¡ç®—æœºè¯¾ç¨‹ã€‚è¯¥è¯¾ç¨‹ä¸»è¦ä½¿ç”¨Pythonè¯­è¨€ï¼Œç®€è¦ä»‹ç»äº†è®¡ç®—æœºçš„å„ç§æ¦‚å¿µï¼ŒèŒƒå›´å¹¿è€Œæ¶‰çŒä¸æ·±ï¼ŒåŒ…æ‹¬é«˜é˜¶å‡½æ•°ï¼ŒæŠ½è±¡ï¼Œé€’å½’å’Œæ ‘ï¼ŒOOPï¼Œç®€å•çš„SQLè¯­å¥ï¼ŒSchemeè¯­æ³•å’Œè§£é‡Šå™¨ç­‰æ¦‚å¿µã€‚ ç›®å‰æ¨èçš„è¯¾ç¨‹æ˜¯20å¹´ç§‹å­£å­¦æœŸ(fa20)çš„è¯¾ç¨‹ã€‚ â€”â€”åæ ¡å…¬å¼€è¯¾ç¨‹è¯„ä»·ç½‘ åæ ¡å…¬å¼€è¯¾ç¨‹è¯„ä»·ç½‘-cs61a æ–‡æ¡£ç»„ç»‡å¯¹åº”ä¸åŒæ•™å­¦å†…å®¹ï¼Œæ–‡æ¡£ç»„ç»‡å¦‚ä¸‹ï¼š 0_è¯¾ä»¶ï¼šlecture 1_ä»£ç ï¼šlectureä»£ç  2_ç¬”è®°ï¼šå­¦ä¹ ç¬”è®° ä½¿ç”¨markdown å†…å®¹åŒ…æ‹¬ï¼šWeekxå†…å®¹(x=week number), Lecture Notes, Lab Notes, Homework Notes 3_å®éªŒï¼šlab 4_ä½œä¸šï¼šhomework 5_é¡¹ç›®ï¼šproject Week1å†…å®¹ Week1ä¸»è¦å†…å®¹ï¼š Lecture01 Computer Science; Lecture02 Functions ä»‹ç»è®¡ç®—æœºç§‘å­¦å’Œå‡½æ•°åŸºç¡€çŸ¥è¯† Lab00: Getting Started å®‰è£…Python3ï¼Œç»ˆç«¯çš„ä½¿ç”¨ï¼Œå¸¸ç”¨å‘½ä»¤è¡Œï¼Œæ–‡æ¡£æµ‹è¯•ï¼ˆdoctestï¼‰çš„ä½¿ç”¨ï¼Œæµ‹è¯•å’Œæäº¤ä½¿ç”¨OKç³»ç»Ÿ HW01: Variables &amp; Functions, Controlã€‚æŒæ¡å‡½æ•°ç‰¹æ€§ Lecture NotesWhat is Computer Science è®¡ç®—æœºç§‘å­¦æ˜¯ä¸€é—¨å®šä¹‰å’Œè§£å†³è®¡ç®—é—®é¢˜çš„æ–¹æ³•å’ŒæŠ€æœ¯çš„å­¦ç§‘ã€‚å®ƒçš„åˆ†æ”¯ç»“æ„å‚è€ƒCSRankingçš„åˆ†ç±»æ–¹å¼ï¼ˆ http://csrankings.org/#/index?all&amp;worldï¼‰ï¼Œå¤§æ¦‚å¯ä»¥åˆ†ä¸ºäººå·¥æ™ºèƒ½ï¼ˆè®¡ç®—æœºè§†è§‰ã€æœºå™¨å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€ä¿¡æ¯æ£€ç´¢â€¦ï¼‰ã€ç³»ç»Ÿï¼ˆè®¡ç®—æœºç»“æ„ã€ç½‘ç»œã€å®‰å…¨ã€æ•°æ®åº“ã€æ“ä½œç³»ç»Ÿã€åˆ†å¸ƒå¼â€¦ï¼‰ã€ç†è®ºï¼ˆç®—æ³•å’Œå¤æ‚åº¦ã€formal methodâ€¦ï¼‰ã€äº¤å‰ï¼ˆè®¡ç®—ç”Ÿç‰©/ç”Ÿç‰©è®¡ç®—ã€äººæœºäº¤äº’ã€æœºå™¨äººâ€¦ï¼‰ç­‰æ–¹å‘ã€‚ Anatomy of a Call Expression: Operator, Operand åœ¨ç¨‹å¼èªè¨€ä¸­, æŒ‡ç¤ºç¨‹å¼é€²è¡Œé‹ç®—(è¨ˆç®—ã€æ¯”è¼ƒæˆ–é€£çµ) çš„ç¬¦è™Ÿ, ç¨±ç‚ºoperators (é‹ç®—å­), è¢«é‹ç®—çš„è³‡æ–™ç¨±ç‚ºoperands (é‹ç®—å…ƒ), ä¸€å¥ä¸­æœ‰operators åŠoperands å°±ç¨±ç‚ºexpressionã€‚ Environment Diagrams Environment Diagrams Tools: http://pythontutor.com/composingprograms.html#mode=edit Defining Functions è¿™é‡Œæ¶‰åŠåˆ°å…¨å±€å˜é‡ï¼ˆGlobal Variableï¼‰å’Œå±€éƒ¨å˜é‡ï¼ˆLocal Variableï¼‰çš„é—®é¢˜ï¼Œå…¨å±€å˜é‡æ˜¯æ•´ä¸ªç¨‹åºéƒ½å¯è®¿é—®çš„å˜é‡ï¼Œç”Ÿå­˜æœŸä»ç¨‹åºå¼€å§‹åˆ°ç¨‹åºç»“æŸï¼›å±€éƒ¨å˜é‡å­˜åœ¨äºæ¨¡å—ä¸­(æ¯”å¦‚æŸä¸ªå‡½æ•°)ï¼Œåªæœ‰åœ¨æ¨¡å—ä¸­æ‰å¯ä»¥è®¿é—®ï¼Œç”Ÿå­˜æœŸä»æ¨¡å—å¼€å§‹åˆ°æ¨¡å—ç»“æŸã€‚ç®€å•çš„è¯´ï¼Œ å…¨å±€å˜é‡ï¼šåœ¨æ¨¡å—å†…ã€åœ¨æ‰€æœ‰å‡½æ•°çš„å¤–é¢ã€åœ¨classå¤–é¢ å±€éƒ¨å˜é‡ï¼šåœ¨å‡½æ•°å†…ã€åœ¨classçš„æ–¹æ³•å†… Lab Noteså¸¸ç”¨å‘½ä»¤: 1234ls: lists all files in the current directorycd &lt;path to directory&gt;: change into the specified directorymkdir &lt;directory name&gt;: make a new directory with the given namemv &lt;source path&gt; &lt;destination path&gt;: move the file at the given source to the given destination 1234python3 xxx.py # è¿è¡Œç¨‹åºpython3 -i xxx.py # è¿è¡Œç¨‹åºå¹¶æ‰“å¼€äº¤äº’å¼ä¼šè¯python3 -m doctest xxx.py # è¿è¡Œæ–‡æ¡£æµ‹è¯•python3 -m doctest - xxx.py # è¿è¡Œæ–‡æ¡£æµ‹è¯•å¹¶æ˜¾ç¤ºæ ·ä¾‹ Homework NotesBugTypeError: &#39;int&#39; object is not callable ä¿®æ”¹ç¨‹åºåå°±å¯ä»¥äº†ã€‚ QuestionQ5: If Function vs Statement whileâ€¦ å‚è€ƒèµ„æ–™ cs61a 20fall å®˜ç½‘ https://inst.eecs.berkeley.edu/~cs61a/fa20/","categories":[{"name":"01 è®¡ç®—æœºåŸºç¡€","slug":"01-è®¡ç®—æœºåŸºç¡€","permalink":"http://example.com/categories/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"CS61A è®¡ç®—æœºç¨‹åºçš„æ„é€ ä¸è§£é‡Š","slug":"01-è®¡ç®—æœºåŸºç¡€/CS61A-è®¡ç®—æœºç¨‹åºçš„æ„é€ ä¸è§£é‡Š","permalink":"http://example.com/categories/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/CS61A-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9E%84%E9%80%A0%E4%B8%8E%E8%A7%A3%E9%87%8A/"}],"tags":[{"name":"ç¬”è®°","slug":"ç¬”è®°","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"CSå…¬å¼€è¯¾","slug":"CSå…¬å¼€è¯¾","permalink":"http://example.com/tags/CS%E5%85%AC%E5%BC%80%E8%AF%BE/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"function","slug":"function","permalink":"http://example.com/tags/function/"}]},{"title":"å…¬å¼ä¹‹ç¾-EVERYTHING IS EPHEMERAL BUT FORMULA IS ETERNAL","slug":"formula","date":"2021-08-04T12:27:54.000Z","updated":"2021-09-27T02:33:41.405Z","comments":true,"path":"formula/","link":"","permalink":"http://example.com/formula/","excerpt":"","text":"ä¸€ä¸ªæœ‰ç‚¹æ„æ€çš„ç§‘æ™®ä¹¦ï¼Œå°¤å…¶åœ¨ä¸æƒ³å†™è®ºæ–‡çš„æ—¶å€™ï¼Œå®æ„¿å»çœ‹å‹¾è‚¡å®šç†çš„Nç§æ¨å¯¼ä¹Ÿä¸æ„¿æ„ç¢°è®ºæ–‡ã€‚æ›´å¥½ç©çš„æ˜¯ï¼Œè¿™é‡Œé¢çš„æ’å›¾è¦æ¯”å†…å®¹æ›´æ²¡æœ‰äº‰è®®çš„è·å¾—ä¸€è‡´å¥½è¯„ã€‚ 1854å¹´ä¹‹å‰ï¼Œæ¬§æ´²æ•°å­¦å®¶ç¿è‹¥æ˜Ÿè¾°ï¼Œç¬›å¡å„¿ã€æ‹‰æ ¼æœ—æ—¥ã€ç‰›é¡¿ã€è´å¶æ–¯ã€æ‹‰æ™®æ‹‰æ–¯ã€æŸ¯è¥¿ã€å‚…é‡Œå¶ã€ä¼½ç½—ç“¦ç­‰ï¼Œæ— ä¸€ä¸æ˜¯æ•°å­¦å¤©æ‰ã€‚1854â€”1935å¹´ï¼Œé«˜æ–¯ã€é»æ›¼ç­‰äººåœ¨æ•°å­¦ç•Œé¢†è¢–ç¾¤ä¼¦ï¼Œå¾·å›½å–ä»£è‹±æ³•æˆä¸ºä¸–ç•Œçš„æ•°å­¦ä¸­å¿ƒã€‚1935å¹´ä¹‹åï¼Œå¸Œç‰¹å‹’ç»™ç¾å›½é€ä¸Šâ€œç§‘å­¦å¤§ç¤¼åŒ…â€ï¼šå“¥å¾·å°”ã€çˆ±å› æ–¯å¦ã€å¾·æ‹œã€å†¯.è¯ºä¾æ›¼ã€è´¹ç±³ã€å†¯.å¡é—¨ã€å¤–å°”â€¦â€¦å¾ˆå¤šç§‘å­¦å®¶é€ƒè‡³åŒ—ç¾ï¼Œæ•°å­¦å¤§æœ¬è¥ä»å¾·å›½è½¬å‘ç¾å›½ï¼Œç¾å›½æˆä¸ºä¸–ç•Œçš„æ•°å­¦ä¸­å¿ƒã€‚ å¤å¸Œè…Šå‡ ä½•å­¦å®¶é˜¿æ³¢æ´›å°¼ä¹Œæ–¯æ€»ç»“äº†åœ†é”¥æ›²çº¿ç†è®ºï¼Œä¸€åƒå¤šå¹´åï¼Œå¾·å›½å¤©æ–‡å­¦å®¶å¼€æ™®å‹’æ‰å°†å…¶åº”ç”¨äºè¡Œæ˜Ÿè½¨é“ï¼›é«˜æ–¯è¢«è®¤ä¸ºæœ€æ—©å‘ç°éæ¬§å‡ ä½•ï¼ŒåŠä¸ªä¸–çºªåï¼Œç”±ä»–å¼Ÿå­åˆ›ç«‹çš„é»æ›¼å‡ ä½•æˆä¸ºå¹¿ä¹‰ç›¸å¯¹è®ºçš„æ•°å­¦åŸºç¡€ã€‚ä¼´éšç€æ æ†åŸç†ã€ç‰›é¡¿ä¸‰å¤§å®šå¾‹ã€éº¦å…‹æ–¯éŸ¦æ–¹ç¨‹ã€é¦™å†œå…¬å¼ã€è´å¶æ–¯å®šç†ç­‰ï¼Œäººç±»å‘è’¸æ±½æ—¶ä»£ã€ç”µåŠ›æ—¶ä»£ã€ä¿¡æ¯æ—¶ä»£ä¹ƒè‡³äººå·¥æ™ºèƒ½æ—¶ä»£å¾å¾è¿ˆè¿›ã€‚ 1+1=2ï¼šæ•°å­¦çš„æº¯æº å‹¾è‚¡å®šç†ï¼šæ•°ä¸å½¢çš„ç»“åˆ è´¹é©¬å¤§å®šç†ï¼šå›°æ‰°äººç±»358å¹´ ç‰›é¡¿-è±å¸ƒå°¼èŒ¨å…¬å¼ï¼šæ— ç©·å°çš„ç§˜å¯† ä¸‡æœ‰å¼•åŠ›ï¼šä»æ··æ²Œåˆ°å…‰æ˜ æ¬§æ‹‰å…¬å¼ï¼šæœ€ç¾çš„ç­‰å¼ ä¼½ç½—ç“¦ç†è®ºï¼šæ— è§£çš„æ–¹ç¨‹ å±é™©çš„é»æ›¼çŒœæƒ³ ç†µå¢å®šå¾‹ï¼šå¯‚ç­æ˜¯å®‡å®™å®¿å‘½ï¼Ÿ éº¦å…‹æ–¯éŸ¦æ–¹ç¨‹ç»„ï¼šè®©é»‘æš—æ¶ˆå¤± è´¨èƒ½æ–¹ç¨‹ï¼šå¼€å¯æ½˜å¤šæ‹‰çš„é­”ç›’ è–›å®šè°”æ–¹ç¨‹ï¼šçŒ«ä¸é‡å­ä¸–ç•Œ ç‹„æ‹‰å…‹æ–¹ç¨‹ï¼šåç‰©è´¨çš„â€œå…ˆçŸ¥â€ æ¨-ç±³å°”æ–¯è§„èŒƒåœºè®ºï¼šå¤§ç»Ÿä¸€ä¹‹è·¯ é¦™å†œå…¬å¼ï¼š5GèƒŒåçš„ä¸»å®° å¸ƒè±å…‹-æ–¯ç§‘å°”æ–¯æ–¹ç¨‹ï¼šé‡‘èâ€œå·«å¸ˆâ€ æªæ¢°ï¼šå¼¹é“é‡Œçš„â€œæŠ€æœ¯å“²å­¦â€ èƒ¡å…‹å®šå¾‹ï¼šæœºæ¢°è¡¨çš„å¿ƒè„ æ··æ²Œç†è®ºï¼šä¸€åªè´è¶å¼•å‘çš„æ€è€ƒ å‡¯åˆ©å…¬å¼ï¼šèµŒåœºä¸Šçš„æœ€å¤§èµ¢å®¶ è´å¶æ–¯å®šç†ï¼šAIå¦‚ä½•æ€è€ƒï¼Ÿ ä¸‰ä½“é—®é¢˜ï¼šæŒ¥ä¹‹ä¸å»çš„ä¹Œäº‘ æ¤­åœ†æ›²çº¿æ–¹ç¨‹ï¼šæ¯”ç‰¹å¸çš„åŸºçŸ³ å‚è€ƒèµ„æ–™ ã€Šå…¬å¼ä¹‹ç¾ã€‹https://book.douban.com/subject/35218287/","categories":[{"name":"æ²‰æ€å½•","slug":"æ²‰æ€å½•","permalink":"http://example.com/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"}],"tags":[{"name":"é˜…è¯»","slug":"é˜…è¯»","permalink":"http://example.com/tags/%E9%98%85%E8%AF%BB/"},{"name":"math","slug":"math","permalink":"http://example.com/tags/math/"}]},{"title":"NLPä¸­çš„æ–‡æœ¬è¡¨ç¤ºæ–¹æ³•","slug":"nlp-text-representation","date":"2021-07-31T11:48:54.000Z","updated":"2021-09-17T01:53:35.231Z","comments":true,"path":"nlp-text-representation/","link":"","permalink":"http://example.com/nlp-text-representation/","excerpt":"","text":"TODO å‚è€ƒèµ„æ–™","categories":[{"name":"02 äººå·¥æ™ºèƒ½","slug":"02-äººå·¥æ™ºèƒ½","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"è‡ªç„¶è¯­è¨€å¤„ç†","slug":"02-äººå·¥æ™ºèƒ½/è‡ªç„¶è¯­è¨€å¤„ç†","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"æ–‡æœ¬è¡¨ç¤º","slug":"æ–‡æœ¬è¡¨ç¤º","permalink":"http://example.com/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"}]},{"title":"SQLè¡¨è¿æ¥&èšåˆå‡½æ•°&çª—å£å‡½æ•°","slug":"SQLé‡ç‚¹","date":"2021-07-26T19:33:19.000Z","updated":"2021-09-17T01:53:20.789Z","comments":true,"path":"SQLé‡ç‚¹/","link":"","permalink":"http://example.com/SQL%E9%87%8D%E7%82%B9/","excerpt":"","text":"è¡¨è¿æ¥ joinjoin: ä»¥å­—æ®µï¼ˆåˆ—ï¼‰ä¸ºå•ä½è¿›è¡Œå¤šè¡¨è¿æ¥ã€‚ 1234Inner join # åªä¿ç•™ä¸¤ä¸ªè¡¨ä¸­åŒæ—¶å­˜åœ¨çš„è®°å½•ã€‚Left join # ä¿ç•™å·¦è¡¨æ‰€æœ‰çš„è®°å½•ï¼Œæ— è®ºå…¶æ˜¯å¦èƒ½å¤Ÿåœ¨å³è¡¨ä¸­åŒ¹é…åˆ°å¯¹åº”çš„è®°å½•ã€‚è‹¥æ— åŒ¹é…è®°å½•ï¼Œåˆ™éœ€è¦ç”¨NULLå¡«è¡¥ã€‚Right join # ä¿ç•™å³è¡¨æ‰€æœ‰çš„è®°å½•ï¼Œæ— è®ºå…¶æ˜¯å¦èƒ½å¤Ÿåœ¨å·¦è¡¨ä¸­åŒ¹é…åˆ°å¯¹åº”çš„è®°å½•ã€‚è‹¥æ— åŒ¹é…è®°å½•ï¼Œåˆ™éœ€è¦ç”¨NULLå¡«è¡¥ã€‚Full join # å·¦è¡¨å’Œå³è¡¨æ‰€æœ‰çš„è®°å½•éƒ½ä¼šä¿ç•™ï¼Œæ²¡æœ‰åŒ¹é…è®°å½•çš„ç”¨NULLå¡«è¡¥ã€‚ èšåˆå‡½æ•°1234sum() # è¿”å›åˆ†ç»„åç»„å†…æ‰€æœ‰è®°å½•çš„å’Œavg() # è¿”å›åˆ†ç»„åç»„å†…æ‰€æœ‰è®°å½•çš„å‡å€¼count() # è¿”å›åˆ†ç»„åç»„å†…æ‰€æœ‰è®°å½•çš„è®¡æ•°max()/min() # è¿”å›åˆ†ç»„åç»„å†…æ‰€æœ‰è®°å½•çš„æœ€å¤§å€¼ã€æœ€å°å€¼ çª—å£å‡½æ•°çª—å£å‡½æ•°å¯¹è®°å½•åˆ†ç»„ä¹‹åè¿›è¡Œèšåˆè®¡ç®—ï¼Œä¸ºåˆ†ç»„ä¸­çš„æ¯æ¡è®°å½•è¿”å›ç‰¹å®šå€¼ã€‚ çª—å£å‡½æ•°çš„åŸºæœ¬ç»“æ„æ˜¯ï¼š 12&lt;çª—å£å‡½æ•°&gt;() over (partition by &lt;col1, col2&gt; order by &lt;col3 desc/asc, col4 asc/desc&gt;) çª—å£å‡½æ•° ä»‹ç» rank() over() è¿”å›è®°å½•åœ¨åŒä¸€åˆ†ç»„å†…çš„æ’åºï¼Œå¦‚æœæœ‰å¹¶åˆ—åæ¬¡çš„è¡Œï¼Œä¼šå ç”¨ä¸‹ä¸€åæ¬¡çš„ä½ç½® dense_rank() over() è¿”å›è®°å½•åœ¨åŒä¸€åˆ†ç»„å†…çš„æ’åºï¼Œå¦‚æœæœ‰å¹¶åˆ—åæ¬¡çš„è¡Œï¼Œä¸å ç”¨ä¸‹ä¸€åæ¬¡çš„ä½ç½® row_number() over() è¿”å›è®°å½•åœ¨åŒä¸€åˆ†ç»„å†…çš„æ’åºï¼Œä¸è€ƒè™‘å¹¶åˆ—åæ¬¡çš„æƒ…å†µ percent_rank() over() è¿”å›è®°å½•åœ¨åŒä¸€åˆ†ç»„å†…æ’åºçš„åˆ†ä½æ•°ï¼Œä¸º0~1 sum(col) over() è¿”å›åŒä¸€åˆ†ç»„å†…æ‰€æœ‰è®°å½•colå€¼çš„å’Œï¼ŒåŒä¸€åˆ†ç»„å†…è®°å½•çš„è¿”å›å€¼ç›¸åŒ avg(col) over() è¿”å›åŒä¸€åˆ†ç»„å†…æ‰€æœ‰è®°å½•colå€¼çš„å¹³å‡å€¼ï¼ŒåŒä¸€åˆ†ç»„å†…è®°å½•çš„è¿”å›å€¼ç›¸åŒ max/min(col) over() è¿”å›åŒä¸€åˆ†ç»„å†…æ‰€æœ‰è®°å½•colå€¼çš„æœ€å¤§å€¼/æœ€å°å€¼ï¼ŒåŒä¸€åˆ†ç»„å†…è®°å½•çš„è¿”å›å€¼ç›¸åŒ èšåˆå‡½æ•°åœ¨çª—å£å‡½æ•°ä¸­ï¼Œæ˜¯å¯¹è‡ªèº«è®°å½•ã€åŠä½äºè‡ªèº«è®°å½•ä»¥ä¸Šçš„æ•°æ®è¿›è¡Œè¿ç®—çš„ç»“æœã€‚èšåˆå‡½æ•°ä½œä¸ºçª—å£å‡½æ•°ï¼Œå¯ä»¥åœ¨æ¯ä¸€è¡Œçš„æ•°æ®é‡Œç›´è§‚çš„çœ‹åˆ°ï¼Œæˆªæ­¢åˆ°æœ¬è¡Œæ•°æ®ï¼Œç»Ÿè®¡æ•°æ®æ˜¯å¤šå°‘ï¼ˆæœ€å¤§å€¼ã€æœ€å°å€¼ç­‰ï¼‰ã€‚åŒæ—¶å¯ä»¥çœ‹å‡ºæ¯ä¸€è¡Œæ•°æ®ï¼Œå¯¹æ•´ä½“ç»Ÿè®¡æ•°æ®çš„å½±å“ã€‚ ç´¢å¼•ç´¢å¼•ç”¨æ¥æ’åºæ•°æ®ä»¥åŠ å¿«æœç´¢å’Œæ’åºæ“ä½œçš„é€Ÿåº¦ã€‚å¯ä»¥åœ¨ä¸€ä¸ªæˆ–å¤šä¸ªåˆ—ä¸Šå®šä¹‰ç´¢å¼•ï¼Œä½¿DBMSä¿å­˜å…¶å†…å®¹çš„ä¸€ä¸ªæ’è¿‡åºçš„åˆ—è¡¨ã€‚åœ¨å®šä¹‰äº†ç´¢å¼•åï¼ŒDBMSä»¥ä½¿ç”¨ä¹¦çš„ç´¢å¼•ç±»ä¼¼çš„æ–¹æ³•ä½¿ç”¨å®ƒã€‚DBMS æœç´¢æ’è¿‡åºçš„ç´¢å¼•ï¼Œæ‰¾å‡ºåŒ¹é…çš„ä½ç½®ï¼Œç„¶åæ£€ç´¢è¿™äº›è¡Œã€‚ ç´¢å¼•æ˜¯å…³ç³»æ•°æ®åº“ä¸­å¯¹æŸä¸€åˆ—æˆ–å¤šä¸ªåˆ—çš„å€¼è¿›è¡Œé¢„æ’åºçš„æ•°æ®ç»“æ„ã€‚é€šè¿‡ä½¿ç”¨ç´¢å¼•ï¼Œå¯ä»¥è®©æ•°æ®åº“ç³»ç»Ÿä¸å¿…æ‰«ææ•´ä¸ªè¡¨ï¼Œè€Œæ˜¯ç›´æ¥å®šä½åˆ°ç¬¦åˆæ¡ä»¶çš„è®°å½•ï¼Œè¿™æ ·å°±å¤§å¤§åŠ å¿«äº†æŸ¥è¯¢é€Ÿåº¦ã€‚ ç´¢å¼•ç”¨CREATE INDEX è¯­å¥åˆ›å»ºï¼ˆä¸åŒDBMSåˆ›å»ºç´¢å¼•çš„è¯­å¥å˜åŒ–å¾ˆå¤§ï¼‰ã€‚ å‚è€ƒèµ„æ–™[1]é€šä¿—æ˜“æ‡‚çš„å­¦ä¼šï¼šSQLçª—å£å‡½æ•° https://zhuanlan.zhihu.com/p/92654574[2]ã€Šæ‹¿ä¸‹Offer:æ•°æ®åˆ†æå¸ˆæ±‚èŒé¢è¯•æŒ‡å—ã€‹ https://item.jd.com/12686131.html[3]ã€ŠSQLå¿…çŸ¥å¿…ä¼šã€‹ https://book.douban.com/subject/24250054/[4]å»–é›ªå³°çš„å®˜æ–¹ç½‘ç«™-SQLæ•™ç¨‹ https://www.liaoxuefeng.com/wiki/1177760294764384","categories":[{"name":"02 äººå·¥æ™ºèƒ½","slug":"02-äººå·¥æ™ºèƒ½","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"æ•°æ®åˆ†æ","slug":"02-äººå·¥æ™ºèƒ½/æ•°æ®åˆ†æ","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://example.com/tags/SQL/"}]},{"title":"5hæ‰“é€šGitå…¨å¥—æ•™ç¨‹","slug":"git","date":"2021-07-23T13:01:25.000Z","updated":"2021-09-17T01:53:30.399Z","comments":true,"path":"git/","link":"","permalink":"http://example.com/git/","excerpt":"","text":"æœ¬æ–‡æ˜¯ä»¥ä¸‹è¯¾ç¨‹çš„ç¬”è®°ï¼š ã€å°šç¡…è°·ã€‘5hæ‰“é€šGitå…¨å¥—æ•™ç¨‹ä¸¨2021æœ€æ–°IDEAç‰ˆï¼ˆæ¶µç›–GitHub\\Giteeç äº‘\\GitLabï¼‰https://www.bilibili.com/video/BV1vy4y1s7k6?p=41 è¯¾ç¨‹ç»“æ„æœ¬å¥—è§†é¢‘ä»åŸºç¡€çš„å¸¸ç”¨å‘½ä»¤å¼€å§‹è®²èµ·ï¼Œåˆ°å¼€å‘å·¥å…·é›†æˆGit ã€GitHubå¦‚ä½•è¿›è¡Œå›¢é˜Ÿåä½œã€å›½å†…ä»£ç æ‰˜ç®¡ä¸­å¿ƒGiteeç äº‘çš„ä½¿ç”¨ã€å±€åŸŸç½‘è‡ªå»ºä»£ç æ‰˜ç®¡å¹³å°GitLabæœåŠ¡å™¨çš„éƒ¨ç½²ã€‚ï¼ˆæœ¬æ–‡ä¸»è¦æ˜¯P1-P26çš„ç¬”è®°ã€‚ï¼‰ P1-P2 Git P3-P6 Gitæ¦‚è¿° P7-P14 Gitå‘½ä»¤ P15-P18 Gitåˆ†æ”¯ P19 Gitå›¢é˜Ÿåä½œ P20-P26 Git&amp;GitHub P27-P37 IDEAé›†æˆGitHub P38-P40 Giteeç äº‘ P41-P44 GitLab P45 è¯¾ç¨‹æ€»ç»“ Gitä»‹ç»GitGitæ˜¯ä¸€ä¸ªå…è´¹çš„ã€å¼€æºçš„åˆ†å¸ƒå¼ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿï¼Œå¯ä»¥å¿«é€Ÿé«˜æ•ˆåœ°å¤„ç†ä»å°å‹åˆ°å¤§å‹çš„å„ç§é¡¹ç›®ã€‚ ç‰ˆæœ¬æ§åˆ¶ç‰ˆæœ¬æ§åˆ¶æ˜¯ä¸€ç§è®°å½•æ–‡ä»¶å†…å®¹å˜åŒ–ï¼Œä»¥ä¾¿å°†æ¥æŸ¥é˜…ç‰¹å®šç‰ˆæœ¬ä¿®è®¢æƒ…å†µçš„ç³»ç»Ÿã€‚ç‰ˆæœ¬æ§åˆ¶å…¶å®æœ€é‡è¦çš„æ˜¯å¯ä»¥è®°å½•æ–‡ä»¶ä¿®æ”¹å†å²è®°å½•ï¼Œä»è€Œè®©ç”¨æˆ·èƒ½å¤ŸæŸ¥çœ‹å†å²ç‰ˆæœ¬ï¼Œæ–¹ä¾¿ç‰ˆæœ¬åˆ‡æ¢ã€‚ ç‰ˆæœ¬æ§åˆ¶å·¥å…· é›†ä¸­å¼ç‰ˆæœ¬æ§åˆ¶å·¥å…·ï¼šCVSã€SVN(Subversion)ã€VSSâ€¦â€¦ åˆ†å¸ƒå¼ç‰ˆæœ¬æ§åˆ¶å·¥å…·ï¼šGitã€ Mercurialã€ Bazaarã€ Darcsâ€¦â€¦ Gitç®€å²Gitæ˜¯Linuså¤§ç¥å†™çš„ï¼Œæ‰€ä»¥å’ŒLinuxä¸€å¥—å‘½ä»¤ Gitæœºåˆ¶åœ¨å·¥ä½œåŒºå†™ä»£ç ï¼Œé€šè¿‡git addæ·»åŠ åˆ°æš‚å­˜åŒºï¼Œå†é€šè¿‡git commitæäº¤åˆ°æœ¬åœ°åº“ï¼Œç”Ÿæˆå†å²ç‰ˆæœ¬ã€‚æœ¬åœ°åº“å¯ä»¥pushä»£ç åˆ°è¿œç¨‹åº“ï¼Œä¹Ÿå¯ä»¥ä»è¿œç¨‹åº“pullæ‹‰å–ä»£ç ï¼Œä¸åŒç‰ˆæœ¬çš„ä»£ç å¯ä»¥è¿›è¡Œmergeã€‚ ä»£ç æ‰˜ç®¡ä¸­å¿ƒ-è¿œç¨‹åº“ä»£ç æ‰˜ç®¡ä¸­å¿ƒæ˜¯åŸºäºç½‘ç»œæœåŠ¡å™¨çš„è¿œç¨‹ä»£ç ä»“åº“ï¼Œä¸€èˆ¬æˆ‘ä»¬ç®€å•ç§°ä¸ºè¿œç¨‹åº“ã€‚ å±€åŸŸç½‘ï¼šGitLab äº’è”ç½‘ï¼šGitHub(å›½å¤–)ï¼ŒGiteeï¼ˆå›½å†…ï¼‰ Gitå¸¸ç”¨å‘½ä»¤ä¸€å›¾ä»¥è”½ä¹‹ï¼Œ Gitåˆ†æ”¯åˆ†æ”¯åœ¨ç‰ˆæœ¬æ§åˆ¶è¿‡ç¨‹ä¸­ï¼ŒåŒæ—¶æ¨è¿›å¤šä¸ªä»»åŠ¡ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡ï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆ›å»ºæ¯ä¸ªä»»åŠ¡çš„å•ç‹¬åˆ†æ”¯ã€‚ä½¿ç”¨åˆ†æ”¯æ„å‘³ç€ç¨‹åºå‘˜å¯ä»¥æŠŠè‡ªå·±çš„å·¥ä½œä»å¼€å‘ä¸»çº¿ä¸Šåˆ†ç¦»å¼€æ¥ï¼Œå¼€å‘è‡ªå·±åˆ†æ”¯çš„æ—¶å€™ï¼Œä¸ä¼šå½±å“ä¸»çº¿åˆ†æ”¯çš„è¿è¡Œã€‚å¯¹äºåˆå­¦è€…è€Œè¨€ï¼Œåˆ†æ”¯å¯ä»¥ç®€å•ç†è§£ä¸ºå‰¯æœ¬ï¼Œä¸€ä¸ªåˆ†æ”¯å°±æ˜¯ä¸€ä¸ªå•ç‹¬çš„å‰¯æœ¬ã€‚ï¼ˆåˆ†æ”¯åº•å±‚å…¶å®ä¹Ÿæ˜¯æŒ‡é’ˆçš„å¼•ç”¨ï¼‰ åŒæ—¶å¹¶è¡Œæ¨è¿›å¤šä¸ªåŠŸèƒ½å¼€å‘ï¼Œå¯ä»¥æé«˜å¼€å‘æ•ˆç‡ã€‚å„ä¸ªåˆ†æ”¯åœ¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå¦‚æœæŸä¸€ä¸ªåˆ†æ”¯å¼€å‘å¤±è´¥ï¼Œä¸ä¼šå¯¹å…¶ä»–åˆ†æ”¯æœ‰ä»»ä½•å½±å“ã€‚å¤±è´¥çš„åˆ†æ”¯åˆ é™¤é‡æ–°å¼€å§‹å³å¯ã€‚ Gitåˆ†æ”¯å‘½ä»¤1234git branch -v # æŸ¥çœ‹åˆ†æ”¯git branch åˆ†æ”¯å # åˆ›å»ºåˆ†æ”¯git checkout åˆ†æ”¯å # åˆ‡æ¢åˆ†æ”¯git merge åˆ†æ”¯å # åˆå¹¶åˆ†æ”¯ åˆå¹¶å†²çªåˆå¹¶åˆ†æ”¯æ—¶ï¼Œä¸¤ä¸ªåˆ†æ”¯åœ¨åŒä¸€ä¸ªæ–‡ä»¶çš„åŒä¸€ä¸ªä½ç½®æœ‰ä¸¤å¥—å®Œå…¨ä¸åŒçš„ä¿®æ”¹ã€‚Gitæ— æ³•æ›¿æˆ‘ä»¬å†³å®šä½¿ç”¨å“ªä¸€ä¸ªã€‚å¿…é¡»äººä¸ºå†³å®šæ–°ä»£ç å†…å®¹ã€‚ Gitå›¢é˜Ÿåˆä½œä¸¤ä¸ªéå¸¸å½¢è±¡åŒ–çš„å›¾å’Œç”ŸåŠ¨çš„ä¾‹å­ï¼ å›¢é˜Ÿå†…åˆä½œ è·¨å›¢é˜Ÿåˆä½œ GitHubè¿œç¨‹ä»“åº“æ“ä½œ cloneä¼šåšå¦‚ä¸‹æ“ä½œï¼š æ‹‰å–ä»£ç ã€‚ åˆå§‹åŒ–æœ¬åœ°ä»“åº“ã€‚ åˆ›å»ºåˆ«åï¼ˆoriginï¼‰ é‚€è¯·åˆä½œè€… SSHç™»å½• Gitä¸å…¶ä»–ç¯å¢ƒé›†æˆå®˜æ–¹æ–‡æ¡£-Gitä¸å„ç§IDEçš„é›†æˆï¼šhttps://git-scm.com/book/en/v2 ç äº‘Giteeä¼—æ‰€å‘¨çŸ¥ï¼ŒGitHubæœåŠ¡å™¨åœ¨å›½å¤–ï¼Œä½¿ç”¨GitHubä½œä¸ºé¡¹ç›®æ‰˜ç®¡ç½‘ç«™ï¼Œå¦‚æœç½‘é€Ÿä¸å¥½çš„è¯ï¼Œä¸¥é‡å½±å“ä½¿ç”¨ä½“éªŒï¼Œç”šè‡³ä¼šå‡ºç°ç™»å½•ä¸ä¸Šçš„æƒ…å†µã€‚é’ˆå¯¹è¿™ä¸ªæƒ…å†µï¼Œå¤§å®¶ä¹Ÿå¯ä»¥ä½¿ç”¨å›½å†…çš„é¡¹ç›®æ‰˜ç®¡ç½‘ç«™-ç äº‘ https://gitee.com/ã€‚ GitLabGitLab https://about.gitlab.com/æ˜¯ç”±GitLabInc.å¼€å‘ï¼Œä½¿ç”¨MITè®¸å¯è¯çš„åŸºäºç½‘ç»œçš„Gitä»“åº“ç®¡ç†å·¥å…·ï¼Œä¸”å…·æœ‰wikiå’Œissueè·Ÿè¸ªåŠŸèƒ½ã€‚ä½¿ç”¨Gitä½œä¸ºä»£ç ç®¡ç†å·¥å…·ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ­å»ºèµ·æ¥çš„webæœåŠ¡ã€‚GitLabç”±ä¹Œå…‹å…°ç¨‹åºå‘˜DmitriyZaporozhetså’ŒValerySizovå¼€å‘ï¼Œå®ƒä½¿ç”¨Rubyè¯­è¨€å†™æˆã€‚åæ¥,ä¸€äº›éƒ¨åˆ†ç”¨Goè¯­è¨€é‡å†™ã€‚æˆªæ­¢2018å¹´5æœˆï¼Œè¯¥å…¬å¸çº¦æœ‰290åå›¢é˜Ÿæˆå‘˜ï¼Œä»¥åŠ2000å¤šåå¼€æºè´¡çŒ®è€…ã€‚GitLabè¢«IBM, Sony, JuÌˆlichResearchCenter, NASA, Alibaba, Invincea, Oâ€™Reilly Media, Leibniz-Rechenzentrum, CERN, SpaceXç­‰ç»„ç»‡ä½¿ç”¨ã€‚","categories":[{"name":"03 å·¥å…·ç®±","slug":"03-å·¥å…·ç®±","permalink":"http://example.com/categories/03-%E5%B7%A5%E5%85%B7%E7%AE%B1/"}],"tags":[{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"},{"name":"ç¬”è®°","slug":"ç¬”è®°","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"}]},{"title":"å¤§ç†-åœ¨æ°‘å®¿æ‰“å·¥çš„æ—¥å­","slug":"å¤§ç†","date":"2021-07-21T14:49:49.000Z","updated":"2021-09-27T02:33:17.823Z","comments":true,"path":"å¤§ç†/","link":"","permalink":"http://example.com/%E5%A4%A7%E7%90%86/","excerpt":"","text":"ç”Ÿæ´»å°±æ˜¯æ¢ä¸ªåœ°æ–¹æ‰“å·¥ã€‚ ç„¶è€Œç”°å›­çš„ç”Ÿæ´»ä¹Ÿä¸æ˜¯é‚£ä¹ˆç¾å¥½çš„ï¼Œæœ‰å¾ˆå¤šæƒ³è±¡ä¸åˆ°çš„é—®é¢˜ã€‚æ¯”å¦‚ï¼Œéšæ—¶éšåœ°å‡ºç°çš„å„ç§èšŠè™«ã€å¤§èœ˜è››ã€è›‡ï¼ŒèŠ±èŠ±è‰è‰éƒ½è¦æ¯å¤©æµ‡æ°´å’Œå®šæ—¶å‰ªæï¼Œä¸‹é›¨å¤©æ³³æ± ä¼šæœ‰å¾ˆå¤šè„ä¸œè¥¿éœ€è¦å¤„ç†ï¼Œåƒåœ¾å¿…é¡»æ¯å¤©æ¸…ç©ºä¸ç„¶ä¼šæœ‰è€é¼ å…‰é¡¾ï¼Œå¨æˆ¿æ’æ°´åšä¸å¥½çš„è¯ä¼šå€’æµæ²¡æ³•ç”¨ï¼ˆæ€ªä¸å¾—è¦ä¸“ä¸šç–é€šä¸‹æ°´é“ï¼‰ï¼Œè¿˜æœ‰ç”µè·¯ä¹Ÿæ—¶ä¸æ—¶çš„å‡ºç‚¹æ¯›ç—…ã€‚ ç”±äºæ¯å¤©éƒ½åœ¨å¤„ç†è¿™ç§çç¢çš„äº‹æƒ…ï¼Œè¿™æ®µæ—¶é—´æœ€å¸¸å»çš„åœ°æ–¹å°±æ˜¯ä»“åº“äº†ã€‚ä»“åº“å°±åƒå“†å•¦Aæ¢¦çš„å£è¢‹ä¸€æ ·ï¼Œå˜å‡ºæˆ‘ä»¥å‰ä¸çŸ¥é“çš„å„ç§å·¥å…·æ¥ï¼Œæœ‰äº›æˆ‘å³ä½¿è®¤è¯†ä¹Ÿä¸çŸ¥é“åå­—ï¼Œä½†æ˜¯è¿™äº›å°ç©æ„å„¿åœ¨ç”Ÿæ´»ä¸­å¦‚æ­¤æœ‰ç”¨ï¼Œæ¯”å¦‚ç»ç’ƒèƒ¶ã€ç£¨ç ‚çº¸ã€æ‰å¸¦ç§ç§ã€‚æœ‰ä¸€æ¬¡ï¼Œæ´—è¡£æœºæ—çš„ç”µæºå¼€å…³çš„æ¥è§¦ä¸å¥½ï¼Œç”µå·¥æ¥ä¿®ï¼ŒæŠŠç”µçº¿ç›´æ¥æ¥åˆ°äº†ä¸€ä¸ªæ’æ’ä¸Šï¼Œè™½ç„¶æˆ‘å¯¹è¿™ä¸ªåšæ³•çš„å®‰å…¨æ€§å’Œæ˜“ç”¨æ€§ä¿æŒæ€€ç–‘ï¼Œä½†è¶³å¤Ÿè®©ç‰©ç†ä¸€å‘ä¸å¥½çš„æˆ‘å¾ˆæ˜¯ä½©æœâ€”â€”æ¯•ç«Ÿæˆ‘è¿ç«çº¿é›¶çº¿éƒ½åˆ†ä¸æ¸…ï¼Œé«˜ä¸­ç‰©ç†è¿æ¥å°ç¯æ³¡çš„é¢˜ä¹Ÿå¾—æƒ³åŠå¤©ã€‚ åæ¥ï¼Œå°±åŒ†åŒ†ç¦»å¼€äº†ã€‚å°±åƒHarryè¯´çš„ï¼ŒIt is not better or worse than any place else - just different.","categories":[{"name":"æ²‰æ€å½•","slug":"æ²‰æ€å½•","permalink":"http://example.com/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"}],"tags":[{"name":"å¤§ç†","slug":"å¤§ç†","permalink":"http://example.com/tags/%E5%A4%A7%E7%90%86/"}]},{"title":"æ ¡å‡†å¯¹ä¸–ç•Œçš„é¢„æœŸ","slug":"æ ¡å‡†å¯¹ä¸–ç•Œçš„é¢„æœŸ","date":"2021-07-15T09:34:54.000Z","updated":"2021-10-12T05:50:20.430Z","comments":true,"path":"æ ¡å‡†å¯¹ä¸–ç•Œçš„é¢„æœŸ/","link":"","permalink":"http://example.com/%E6%A0%A1%E5%87%86%E5%AF%B9%E4%B8%96%E7%95%8C%E7%9A%84%E9%A2%84%E6%9C%9F/","excerpt":"","text":"å‰è¨€æ ¡å‡†å¯¹ä¸–ç•Œçš„é¢„æœŸï¼Œè¿™ä¸ªé¢˜ç›®å‡ºè‡ªäºæ’­å®¢é‡Œå¬åˆ°çš„ä¸€æ®µè¯ï¼š ä»å­¦æ ¡æ­¥å…¥èŒåœºï¼Œè¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯é‡æ–°æ ¡å‡†ä½ å¯¹ä¸–ç•Œçš„é¢„æœŸã€‚è¿™ä¸–ç•Œå°±æ˜¯å¾ˆç³Ÿç³•çš„ï¼ˆæˆ–è€…è¯´ï¼Œè¿™ä¸–ç•Œæ˜¯ okay çš„ï¼Œä½†æ˜¯ä½ å¯¹å®ƒæœ‰ä¸åˆ‡å®é™…çš„é¢„æœŸï¼‰ã€‚ å¾ˆæ—©çš„æ—¶å€™æˆ‘ç»å¸¸æ€€ç–‘ï¼Œè€å¸ˆè®²çš„ä¸œè¥¿æœ‰ä»€ä¹ˆç”¨ï¼Ÿæ€»æœ‰é‚£ä¹ˆäº›è¯¾ç¨‹ï¼Œæ—¢ä¸èƒ½ç›´æ¥ç”¨åˆ°å·¥ä½œä¸­ï¼Œåˆä¸èƒ½é”»ç‚¼æ€ç»´å’Œé€»è¾‘èƒ½åŠ›ã€‚æœ€åä¸å¾—ä¸æ‰¿è®¤ä¸€ä¸ªå±é™©çš„çœŸç›¸ï¼šä¸Šè¯¾å¤ªè€½è¯¯å­¦ä¹ äº†ã€‚è€Œè‡ªå·±ä¹‹æ‰€ä»¥ä¸ºæ­¤æ„Ÿåˆ°ç—›è‹¦ï¼Œæ˜¯å› ä¸ºæŠ±æœ‰ä¸åˆç†çš„å‡è®¾å’Œé¢„æœŸã€‚æ¯”å¦‚ï¼Œå½“æˆ‘äº§ç”Ÿæ€€ç–‘çš„æ—¶å€™ï¼Œå…¶å®æˆ‘çš„å‡è®¾å°±æ˜¯è€å¸ˆæ•™çš„å°±åº”è¯¥æ˜¯æœ‰ç”¨çš„ã€‚å¯æ˜¯å‡­ä»€ä¹ˆå‘¢ï¼Ÿæ˜æ˜æ˜¯æˆ‘ä¸€å¢æƒ…æ„¿çš„è¿™æ ·è®¤ä¸ºï¼Œç„¶åå‘ç°äº‹å®ä¸é¢„æœŸä¸ç¬¦ï¼Œäºæ˜¯å°±ä¸æ–­ç—›è‹¦ã€‚è¿™æ ·çš„å‡è®¾è¿˜åŒ…æ‹¬ï¼š H1. å­¦æ ¡æ•™çš„æ˜¯å¯¹å·¥ä½œæœ‰ç”¨çš„(Ã—)å­¦æ ¡æ˜¯ç”¨æ¥ç­›é€‰çš„ã€‚å­¦æ ¡æœ‰ä¸€å¥—è‡ªå·±çš„æ¸¸æˆè§„åˆ™ï¼ŒæŒ‰ç…§è¿™ä¸ªè§„åˆ™èµ°ï¼Œå°±ä¼šå¾—åˆ°è€å¸ˆå’ŒåŒå­¦çš„èµèµã€æ‹¿å¥–å­¦é‡‘ã€ä¿ç ”ã€å‘è®ºæ–‡ã€è·å¾—ä¸€ç³»åˆ—è£èª‰å¥–é¡¹ç­‰â€”â€”æœ€ç»ˆç­›é€‰å‡ºä¸€ä¸ªä¹–å·§ã€å¬è¯ã€èƒ½å¹²æ´»ã€è€Œä¸”å­¦ä¹ èƒ½åŠ›ä¸é”™çš„äººï¼ˆæˆ–è€…è‡³å°‘æ„¿æ„ä¼ªè£…æˆè¿™æ ·çš„äººï¼‰ã€‚ä½†æŠŠè¿™ä¸ªè§„åˆ™ç©å¥½ï¼Œå¹¶ä¸ç­‰åŒäºæ‹¥æœ‰äº†å·¥ä½œéœ€è¦çš„å®åŠ›å’Œèƒ½åŠ›ï¼Œå› ä¸ºå·¥ä½œæ˜¯å¦ä¸€å¥—è§„åˆ™å’Œç©æ³•ã€‚ç”±äºä»å°ä¸€ç›´çŒè¾“çš„æ˜¯å­¦æ ¡çš„è§„åˆ™ï¼Œæ‰€ä»¥å¾ˆéš¾æ„è¯†åˆ°æ¸¸æˆå·²ç»åˆ‡æ¢äº†ï¼Œè¿™å°±æ˜¯å¸¸è¯´çš„å­¦ç”Ÿæ€ç»´å§ã€‚ åŒæ—¶ï¼Œè¿™ä¸ªä¸–ç•Œä¸Šæœ‰æ— æ•°ä¸ªè¡Œä¸šå’Œé¢†åŸŸï¼Œå°±æœ‰æ— æ•°æ¸¸æˆå’Œè§„åˆ™ã€‚è¯šç„¶åº•å±‚çš„ä¸œè¥¿æ˜¯å¯ä»¥è¿ç§»ï¼Œä¾‹å¦‚åŠªåŠ›ã€è´Ÿè´£ç­‰å“è´¨åœ¨å“ªé‡Œéƒ½æ˜¯è¢«è®¤å¯çš„ï¼Œä½†ä»ç„¶è¦ä¿æŒå¼€æ”¾çš„å¿ƒæ€ï¼Œä¸èƒ½åšå‡ºä¸€ä¸ªé€‰æ‹©å°±å¦å®šå…¶ä»–é€‰æ‹©ï¼Œæ¯”å¦‚ï¼Œä¸€äº›é©¬ä¸Šæƒ³åˆ°ä½†æ˜¯ä¸å¯æè¿°çš„ä¾‹å­ã€‚è¿™ç§å¿ƒæ€çš„å½¢æˆè‡³å°‘æœ‰3ä¸ªå› ç´ ï¼š1.äººçš„å¿ƒç†å¤©ç„¶å€¾å‘äºæ‰¾å„ç§ç†ç”±æ”¯æŒè‡ªå·±çš„é€‰æ‹©ï¼Œè€Œå¿½ç•¥å¯¹è‡ªå·±é€‰æ‹©ä¸åˆ©çš„æ–¹é¢ã€‚2.å³ä½¿çŸ¥é“è‡ªå·±çš„é€‰æ‹©æœ‰ä¸å¥½çš„ä¸€é¢ï¼Œä½†å‡ºäºè™šè£åšå†³ä¸èƒ½æ‰¿è®¤ã€‚3.å³ä½¿çŸ¥é“è‡ªå·±çš„é€‰æ‹©æœ‰ä¸å¥½çš„ä¸€é¢ï¼Œä½†å¿½æ‚ æ›´å¤šäººè¿›æ¥æ‰æ–¹ä¾¿æ‰¾äººæ¥ç›˜ã€‚ æœ€ç»ˆé€šè¿‡èº«è¾¹çš„ç°è±¡æ¥çœ‹ï¼Œä¹–ä¹–å¬è€å¸ˆè¯çš„æ‰¾ä¸åˆ°å·¥ä½œåªèƒ½è€ƒå…¬ï¼ˆä¸æ˜¯è¯´å…¬åŠ¡å‘˜ä¸å¥½ï¼Œè€Œæ˜¯è¯´è¿™ç§æƒ…å†µä¸‹å¹¶æ²¡æœ‰é€‰æ‹©æƒï¼‰ï¼Œæ—©æ—©ç¿˜è¯¾å»å®ä¹ çš„æ‹¿åˆ°äº†ä»¤äººç¾¡æ…•çš„offerã€‚ç”¨æœ‹å‹åœˆé‡Œçœ‹åˆ°çš„ä¸€æ®µè¯æ¦‚æ‹¬å°±æ˜¯ï¼šä¸»ä¿®LeetCodeï¼Œè¾…ä¿®bilibiliï¼Œæ—å¬cså…¬å¼€è¯¾ï¼Œæœ€åæ··ä¸€ä¸‹å­¦ä½å¿…ä¿®è¯¾ä»¥è¾¾åˆ°æ¯•ä¸šè¦æ±‚ã€‚è¿™æ‰æ˜¯æ‰¾åˆ°å¥½å·¥ä½œçš„æ­£ç¡®å§¿åŠ¿ã€‚ H2. ä¸–ç•Œæ˜¯å…¬å¹³çš„(Ã—)ä¸–ç•Œæœ¬æ¥æ˜¯ä¸å…¬å¹³çš„ï¼Œä¹Ÿä¸å¤©ç„¶åº”è¯¥æ˜¯å…¬å¹³çš„ï¼Œå› ä¸ºäººç±»æ–‡æ˜çš„å‡ºç°äºæ˜¯å˜å¥½äº†ä¸€ç‚¹ç‚¹ã€‚æ‰€ä»¥é‚£äº›å£°ç§°åªèƒ½æ¥å—è¿™ä¸ªç³Ÿç³•çš„ä¸–ç•Œçš„è¯´æ³•ä¹Ÿä¸å¯¹ï¼Œè¿™ç®€ç›´æ˜¯åœ¨å¦å®šäººç±»æ–‡æ˜ï¼Œæ²¡æœ‰ä»€ä¹ˆæ˜¯ç†æ‰€å½“ç„¶çš„ã€‚ å°è¯•æ”¾ä¸‹æ‰€æœ‰å‡è®¾å’Œé¢„æœŸæˆ‘ä»¬å¯¹äºå¾ˆå¤šä¸œè¥¿çš„åˆ¤æ–­æ˜¯æ¥è‡ªäºä¹¦ç±ã€ç”µè§†ã€ç½‘ç»œã€å’Œä»–äººçš„åªè¨€ç‰‡è¯­ã€è‡ªå·±çš„ç»éªŒã€‚é€šè¿‡ç»éªŒå­¦ä¹ å½“ç„¶å¾ˆæœ‰æ•ˆï¼Œä½†é—®é¢˜æ˜¯è¿™äº›ç»éªŒä¸éƒ½æ˜¯å®¢è§‚ç†æ€§çš„ï¼Œå¦‚æœæˆ‘ä»¬æ”¾ä¸‹è¿™äº›å…ˆéªŒçŸ¥è¯†ï¼Œæ„Ÿå—äº‹ç‰©æœ¬èº«ï¼Œé€šè¿‡è‡ªå·±çš„ç‹¬ç«‹æ€è€ƒå¾—åˆ°è‡ªå·±çš„ç»“è®ºï¼Œé‚£ä¹ˆè¢«å¿½æ‚ çš„æ¦‚ç‡ä¼šå¤§å¤§é™ä½å§ã€‚ å‚è€ƒæ’­å®¢ BYM èŒåœº12: ç”Ÿæ´»å°±æ˜¯ä¸å…¬å¹³çš„ï¼Œä½ æ‰“ç®—æ€ä¹ˆåŠ æ”¶å¬é“¾æ¥ BYM èŒåœº14: è®¤æ¸…ä¸–ç•Œä¹‹æ®‹é…·ï¼Œä½ æ‰å¯èƒ½æˆä¸ºé‚£ 5% æ‹¥æœ‰è‡ªå·±äº‹ä¸šçš„äºº æ”¶å¬é“¾æ¥","categories":[{"name":"æ²‰æ€å½•","slug":"æ²‰æ€å½•","permalink":"http://example.com/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"}],"tags":[{"name":"æ€è€ƒ","slug":"æ€è€ƒ","permalink":"http://example.com/tags/%E6%80%9D%E8%80%83/"},{"name":"æ’­å®¢","slug":"æ’­å®¢","permalink":"http://example.com/tags/%E6%92%AD%E5%AE%A2/"}]},{"title":"ä¸€äº›å¥½ç”¨çš„ä¸­è‹±æ–‡LaTeXç®€å†æ¨¡æ¿","slug":"ä¸€äº›å¥½ç”¨çš„ä¸­è‹±æ–‡LaTeXç®€å†æ¨¡æ¿","date":"2021-07-15T08:23:20.000Z","updated":"2021-09-17T01:53:59.711Z","comments":true,"path":"ä¸€äº›å¥½ç”¨çš„ä¸­è‹±æ–‡LaTeXç®€å†æ¨¡æ¿/","link":"","permalink":"http://example.com/%E4%B8%80%E4%BA%9B%E5%A5%BD%E7%94%A8%E7%9A%84%E4%B8%AD%E8%8B%B1%E6%96%87LaTeX%E7%AE%80%E5%8E%86%E6%A8%A1%E6%9D%BF/","excerpt":"","text":"æ€»ç»“ç®€å†çš„ç»“æ„ä¸»è¦åŒ…æ‹¬ï¼š æ ‡é¢˜ Headingï¼ˆå§“åã€è”ç³»æ–¹å¼ç­‰ï¼‰ æ•™è‚²èƒŒæ™¯ Education ç»å† Experience å®ä¹  Work ç§‘ç ”/è®ºæ–‡ Publications é¡¹ç›® Projects ä¸ªäººæŠ€èƒ½ Skills GitHubæ¨¡æ¿è‹±æ–‡ç®€å† https://github.com/sb2nov/resume ä¸­æ–‡ç®€å† https://github.com/hijiangtao/resume ä¸­è‹±æ–‡å…¼å®¹çš„ç®€å† https://github.com/billryan/resume/tree/zh_CN æ€ä¹ˆå†™ç®€å†çš„å‚è€ƒæ–‡ç« å¸ˆå¦¹çœ‹äº†éƒ½è¯´å¥½çš„ç®€å†é•¿å•¥æ ·https://mp.weixin.qq.com/s/ea2Pq3ZbJ30lTV3etaECoQ ä¸€ä¸ªç®€æ´ä¼˜é›…çš„ XeLaTeX ç®€å†æ¨¡æ¿https://tiankuizhang.github.io/files/00CV_CN/README/","categories":[{"name":"03 å·¥å…·ç®±","slug":"03-å·¥å…·ç®±","permalink":"http://example.com/categories/03-%E5%B7%A5%E5%85%B7%E7%AE%B1/"}],"tags":[{"name":"laTeX","slug":"laTeX","permalink":"http://example.com/tags/laTeX/"},{"name":"ç®€å†","slug":"ç®€å†","permalink":"http://example.com/tags/%E7%AE%80%E5%8E%86/"}]},{"title":"SQLå­¦ä¹ èµ„æ–™","slug":"SQLå­¦ä¹ èµ„æ–™","date":"2021-07-14T07:32:58.000Z","updated":"2021-09-17T01:53:15.370Z","comments":true,"path":"SQLå­¦ä¹ èµ„æ–™/","link":"","permalink":"http://example.com/SQL%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/","excerpt":"","text":"&lt;&lt;SQLå¿…çŸ¥å¿…ä¼š&gt;&gt; https://book.douban.com/subject/35167240/ SQLZOO https://sqlzoo.net/ ç‰›å®¢ç½‘-SQL https://www.nowcoder.com/activity/oj?tab=1 LeetCode-æ•°æ®åº“ https://leetcode-cn.com/problemset/database/ Datafrog-SQLç»å…¸45é¢˜ https://www.bilibili.com/video/BV1pp4y1Q7Yv å°šç¡…è°·-MySQLåŸºç¡€æ•™ç¨‹ https://www.bilibili.com/video/BV1xW411u7ax","categories":[{"name":"02 äººå·¥æ™ºèƒ½","slug":"02-äººå·¥æ™ºèƒ½","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"æ•°æ®åˆ†æ","slug":"02-äººå·¥æ™ºèƒ½/æ•°æ®åˆ†æ","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://example.com/tags/SQL/"}]},{"title":"æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡","slug":"æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡","date":"2021-07-14T03:03:00.000Z","updated":"2021-09-17T01:53:40.027Z","comments":true,"path":"æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡/","link":"","permalink":"http://example.com/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/","excerpt":"","text":"æ¦‚ç‡ç»Ÿè®¡çš„åŸºç¡€æ¦‚å¿µéšæœºè¯•éªŒï¼›éšæœºå˜é‡ï¼›æ¦‚ç‡åˆ†å¸ƒï¼›æ¦‚ç‡åˆ†å¸ƒå‡½æ•°ï¼›æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼›ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼›æ ·æœ¬å’Œæ€»ä½“ã€‚ ç¦»æ•£å‹éšæœºå˜é‡åŠå…¶åˆ†å¸ƒæ ¹æ®éšæœºè¯•éªŒçš„ç»“æœæ•°é‡æ˜¯å¦å¯æ•°ï¼Œåˆ†ä¸ºç¦»æ•£å‹éšæœºå˜é‡å’Œè¿ç»­å‹éšæœºå˜é‡ã€‚ 0-1åˆ†å¸ƒ/ä¼¯åŠªåˆ©åˆ†å¸ƒ å®šä¹‰ï¼šä¼¯åŠªåˆ©åˆ†å¸ƒæŒ‡çš„æ˜¯å¯¹äºéšæœºå˜é‡Xæœ‰å‚æ•°ä¸ºpï¼ˆ0&lt;1&lt;Pï¼‰ï¼Œå®ƒåˆ†åˆ«ä»¥æ¦‚ç‡på’Œ1-på–1å’Œ0ä¸ºå€¼ã€‚ex. ä»¤Xè¡¨ç¤ºæŠ›ç¡¬å¸çš„ç»“æœã€‚ è¡¨ç¤ºï¼šX ~ b( p) å…¬å¼ï¼šP(X=1) = p, P(X=0) = 1-p, where p in [0,1] æœŸæœ›ä¸æ–¹å·®ï¼šE(X)=p, D(X)=p(1-p) äºŒé¡¹åˆ†å¸ƒ/nä¸ªé‡å¤ç‹¬ç«‹çš„ä¼¯åŠªåˆ©åˆ†å¸ƒ åœ¨næ¬¡ç‹¬ç«‹é‡å¤çš„ä¼¯åŠªåˆ©è¯•éªŒä¸­ï¼Œè®¾æ¯æ¬¡è¯•éªŒä¸­äº‹ä»¶Aå‘ç”Ÿçš„æ¦‚ç‡ä¸ºpã€‚ç”¨Xè¡¨ç¤ºné‡ä¼¯åŠªåˆ©è¯•éªŒä¸­äº‹ä»¶Aå‘ç”Ÿçš„æ¬¡æ•°ï¼Œåˆ™Xçš„å¯èƒ½å–å€¼ä¸º0ï¼Œ1ï¼Œâ€¦ï¼Œn,ä¸”å¯¹æ¯ä¸€ä¸ªkï¼ˆ0â‰¤kâ‰¤nï¼‰,äº‹ä»¶{X=k}å³ä¸ºâ€œnæ¬¡è¯•éªŒä¸­äº‹ä»¶Aæ°å¥½å‘ç”Ÿkæ¬¡â€ï¼Œéšæœºå˜é‡Xçš„ç¦»æ•£æ¦‚ç‡åˆ†å¸ƒå³ä¸ºäºŒé¡¹åˆ†å¸ƒï¼ˆBinomial Distributionï¼‰ã€‚ è¡¨ç¤ºï¼šX ~ b(n, p) æ¦‚ç‡å‡½æ•°ï¼š æœŸæœ›ä¸æ–¹å·®ï¼šE(X)=np, D(X)=np(1-p) å‡ ä½•åˆ†å¸ƒ å®šä¹‰ï¼šå‡ ä½•åˆ†å¸ƒï¼ˆGeometric distributionï¼‰æ˜¯ç¦»æ•£å‹æ¦‚ç‡åˆ†å¸ƒã€‚å…¶ä¸­ä¸€ç§å®šä¹‰ä¸ºï¼šåœ¨næ¬¡ä¼¯åŠªåˆ©è¯•éªŒä¸­ï¼Œè¯•éªŒkæ¬¡æ‰å¾—åˆ°ç¬¬ä¸€æ¬¡æˆåŠŸçš„æœºç‡ã€‚è¯¦ç»†åœ°è¯´ï¼Œæ˜¯ï¼šå‰k-1æ¬¡çš†å¤±è´¥ï¼Œç¬¬kæ¬¡æˆåŠŸçš„æ¦‚ç‡ã€‚ è¡¨ç¤ºï¼šX ~ g( p) æ¦‚ç‡å‡½æ•°ï¼š æœŸæœ›ä¸æ–¹å·®ï¼šE(X)=1/p, D(X)=(1-p)/p^2 æ³Šæ¾åˆ†å¸ƒ å®šä¹‰ï¼šPoissonåˆ†å¸ƒï¼Œæ˜¯ä¸€ç§ç»Ÿè®¡ä¸æ¦‚ç‡å­¦é‡Œå¸¸è§åˆ°çš„ç¦»æ•£æ¦‚ç‡åˆ†å¸ƒã€‚æ³Šæ¾åˆ†å¸ƒçš„å‚æ•°Î»æ˜¯å•ä½æ—¶é—´(æˆ–å•ä½é¢ç§¯)å†…éšæœºäº‹ä»¶çš„å¹³å‡å‘ç”Ÿæ¬¡æ•°ã€‚ æ³Šæ¾åˆ†å¸ƒé€‚åˆäºæè¿°å•ä½æ—¶é—´å†…éšæœºäº‹ä»¶å‘ç”Ÿçš„æ¬¡æ•°ã€‚ è¡¨ç¤ºï¼šX ~ p(Î») æ¦‚ç‡å‡½æ•°ï¼š æœŸæœ›ä¸æ–¹å·®ï¼šE(X)=Î», D(X)=Î» ex. æŸä¸€æœåŠ¡è®¾æ–½åœ¨ä¸€å®šæ—¶é—´å†…åˆ°è¾¾çš„äººæ•°ã€æŸç½‘ç«™æˆ–APPåœ¨å•ä½æ—¶é—´å†…çš„è®¿é—®äººæ•°ã€‚ æ³Šæ¾åˆ†å¸ƒä¸äºŒé¡¹åˆ†å¸ƒï¼šå½“äºŒé¡¹åˆ†å¸ƒçš„nå¾ˆå¤§è€Œpå¾ˆå°æ—¶ï¼Œæ³Šæ¾åˆ†å¸ƒå¯ä½œä¸ºäºŒé¡¹åˆ†å¸ƒçš„è¿‘ä¼¼ï¼Œå…¶ä¸­Î»ä¸ºnpã€‚é€šå¸¸å½“nâ‰§20,pâ‰¦0.05æ—¶ï¼Œå°±å¯ä»¥ç”¨æ³Šæ¾å…¬å¼è¿‘ä¼¼å¾—è®¡ç®—ã€‚ è¿ç»­å‹éšæœºå˜é‡åŠå…¶åˆ†å¸ƒå‡åŒ€åˆ†å¸ƒ å®šä¹‰ï¼šå‡åŒ€åˆ†å¸ƒä¹Ÿå«çŸ©å½¢åˆ†å¸ƒï¼Œå®ƒæ˜¯å¯¹ç§°æ¦‚ç‡åˆ†å¸ƒï¼Œåœ¨ç›¸åŒé•¿åº¦é—´éš”çš„åˆ†å¸ƒæ¦‚ç‡æ˜¯ç­‰å¯èƒ½çš„ã€‚ å‡åŒ€åˆ†å¸ƒç”±ä¸¤ä¸ªå‚æ•°aå’Œbå®šä¹‰ï¼Œå®ƒä»¬æ˜¯æ•°è½´ä¸Šçš„æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼Œé€šå¸¸ç¼©å†™ä¸ºUï¼ˆaï¼Œbï¼‰ã€‚ è¡¨ç¤ºï¼šX ~ u(a,b) æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼š ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼š æœŸæœ›å’Œæ–¹å·®ï¼šE(X)=(a+b)/2, D(X)=(a-b)^2/12 æ­£æ€åˆ†å¸ƒ/é«˜æ–¯åˆ†å¸ƒ å®šä¹‰ï¼šè‹¥éšæœºå˜é‡Xæœä»ä¸€ä¸ªæ•°å­¦æœŸæœ›ä¸ºÎ¼ã€æ–¹å·®ä¸ºÏƒ^2çš„æ­£æ€åˆ†å¸ƒï¼Œè®°ä¸ºN(Î¼ï¼ŒÏƒ^2)ã€‚å…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºæ­£æ€åˆ†å¸ƒçš„æœŸæœ›å€¼Î¼å†³å®šäº†å…¶ä½ç½®ï¼Œå…¶æ ‡å‡†å·®Ïƒå†³å®šäº†åˆ†å¸ƒçš„å¹…åº¦ã€‚å½“Î¼ = 0,Ïƒ = 1æ—¶çš„æ­£æ€åˆ†å¸ƒæ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚ è¡¨ç¤ºï¼šN(Î¼ï¼ŒÏƒ^2) æ ‡å‡†åŒ–å˜æ¢ï¼š æ¦‚ç‡åˆ†å¸ƒå‡½æ•°ï¼š æœŸæœ›å’Œæ–¹å·®ï¼šE(X)=Î¼ï¼ŒD(X)=Ïƒ^2 æ€§è´¨ï¼š æŒ‡æ•°åˆ†å¸ƒ å®šä¹‰ï¼šåœ¨æ¦‚ç‡ç†è®ºå’Œç»Ÿè®¡å­¦ä¸­ï¼ŒæŒ‡æ•°åˆ†å¸ƒï¼ˆä¹Ÿç§°ä¸ºè´ŸæŒ‡æ•°åˆ†å¸ƒï¼‰æ˜¯æè¿°æ³Šæ¾è¿‡ç¨‹ä¸­çš„äº‹ä»¶ä¹‹é—´çš„æ—¶é—´çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå³äº‹ä»¶ä»¥æ’å®šå¹³å‡é€Ÿç‡è¿ç»­ä¸”ç‹¬ç«‹åœ°å‘ç”Ÿçš„è¿‡ç¨‹ã€‚ X ~ E(Î») å…¶ä¸­Î» &gt; 0æ˜¯åˆ†å¸ƒçš„ä¸€ä¸ªå‚æ•°ï¼Œå¸¸è¢«ç§°ä¸ºç‡å‚æ•°ï¼ˆrate parameterï¼‰ã€‚å³æ¯å•ä½æ—¶é—´å†…å‘ç”ŸæŸäº‹ä»¶çš„æ¬¡æ•°ã€‚æŒ‡æ•°åˆ†å¸ƒçš„åŒºé—´æ˜¯[0,âˆ)ã€‚ æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼š ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼š æœŸæœ›å’Œæ–¹å·®ï¼šE(X)=1/Î», D(X)=1/Î»^2 éšæœºå˜é‡çš„åˆ†å¸ƒç‰¹å¾ä¸ç»Ÿè®¡é‡æ•°æ®çš„åˆ†å¸ƒç‰¹å¾ä¸ç»Ÿè®¡é‡ é‡è¦éšæœºå˜é‡çš„æœŸæœ›å’Œæ–¹å·®æ€»ç»“ éšæœºå˜é‡ è¡¨ç¤º æœŸæœ› æ–¹å·® 0-1åˆ†å¸ƒ X ~ b( p) E(X)=p D(X)=p(1-p) äºŒé¡¹åˆ†å¸ƒ X ~ b(n, p) E(X)=np D(X)=np(1-p) å‡ ä½•åˆ†å¸ƒ X ~ g( p) E(X)=1/p D(X)=(1-p)/p^2 æ³Šæ¾åˆ†å¸ƒ X ~ p(Î») E(X)=Î» D(X)=Î» å‡åŒ€åˆ†å¸ƒ X ~ u(a,b) E(X)=(a+b)/2 D(X)=(a-b)^2/12 æ­£æ€åˆ†å¸ƒ N(Î¼ï¼ŒÏƒ^2) E(X)=Î¼ D(X)=Ïƒ^2 æŒ‡æ•°åˆ†å¸ƒ X ~ E(Î») E(X)=1/Î» D(X)=1/Î»^2 åæ–¹å·®å’Œç›¸å…³ç³»æ•°åæ–¹å·®åæ–¹å·®ï¼ˆCovarianceï¼‰åœ¨æ¦‚ç‡è®ºå’Œç»Ÿè®¡å­¦ä¸­ç”¨äºè¡¡é‡ä¸¤ä¸ªå˜é‡çš„æ€»ä½“è¯¯å·®ã€‚è€Œæ–¹å·®æ˜¯åæ–¹å·®çš„ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼Œå³å½“ä¸¤ä¸ªå˜é‡æ˜¯ç›¸åŒçš„æƒ…å†µã€‚åæ–¹å·®è¡¨ç¤ºçš„æ˜¯ä¸¤ä¸ªå˜é‡çš„æ€»ä½“çš„è¯¯å·®ã€‚https://baike.baidu.com/item/åæ–¹å·® ç›¸å…³ç³»æ•°ç›¸å…³å…³ç³»æ˜¯ä¸€ç§éç¡®å®šæ€§çš„å…³ç³»ï¼Œç›¸å…³ç³»æ•°æ˜¯ç ”ç©¶å˜é‡ä¹‹é—´çº¿æ€§ç›¸å…³ç¨‹åº¦çš„é‡ã€‚ç›¸å…³ç³»æ•°æ˜¯æœ€æ—©ç”±ç»Ÿè®¡å­¦å®¶å¡å°”Â·çš®å°”é€Šè®¾è®¡çš„ç»Ÿè®¡æŒ‡æ ‡ï¼Œæ˜¯ç ”ç©¶å˜é‡ä¹‹é—´çº¿æ€§ç›¸å…³ç¨‹åº¦çš„é‡ï¼Œä¸€èˆ¬ç”¨å­—æ¯ r è¡¨ç¤ºã€‚ç”±äºç ”ç©¶å¯¹è±¡çš„ä¸åŒï¼Œç›¸å…³ç³»æ•°æœ‰å¤šç§å®šä¹‰æ–¹å¼ï¼Œè¾ƒä¸ºå¸¸ç”¨çš„æ˜¯çš®å°”é€Šç›¸å…³ç³»æ•°ã€‚https://baike.baidu.com/item/ç›¸å…³ç³»æ•° ç‹¬ç«‹äº‹ä»¶ã€æ¡ä»¶æ¦‚ç‡ã€è´å¶æ–¯å®šç†ç‹¬ç«‹äº‹ä»¶ç‹¬ç«‹äº‹ä»¶ï¼šä¸¤ä¸ªäº‹ä»¶ä¸è®ºå“ªä¸€ä¸ªäº‹ä»¶å‘ç”Ÿéƒ½ä¸å½±å“å¦ä¸€ä¸ªå‘ç”Ÿçš„æ¦‚ç‡ã€‚Aå’ŒBæ˜¯ç‹¬ç«‹çš„, å½“ä¸”ä»…å½“: P(AB) = P(A)P(B) æ¡ä»¶æ¦‚ç‡æ¡ä»¶æ¦‚ç‡ï¼šå½“äº‹ä»¶Bå·²ç»å‘ç”Ÿæ—¶ï¼Œäº‹ä»¶Aå‘ç”Ÿçš„æ¦‚ç‡ã€‚äº‹ä»¶Bå‘ç”Ÿæ¡ä»¶ä¸‹äº‹ä»¶Aå‘ç”Ÿçš„æ¦‚ç‡ä¸º: P(A|B) = P(AB)/P(B)æ¦‚ç‡çš„ä¹˜æ³•å…¬å¼: P(AB) = P(A|B)P(B) = P(B|A)P(A) å…¨æ¦‚ç‡å…¬å¼P(B) = âˆ‘P(B|Ai)P(Ai)ï¼Œi=1,2,...,k è´å¶æ–¯å®šç†é€šå¸¸P(Ai)ä¸ºAçš„å…ˆéªŒæ¦‚ç‡ï¼ŒP(Ai|B)ä¸ºAçš„åéªŒæ¦‚ç‡ã€‚ å¤§æ•°å®šç†ä¸ä¸­å¿ƒæé™å®šç†å¤§æ•°å®šå¾‹å¤§æ•°å®šå¾‹ï¼šå°†éšæœºå˜é‡Xæ‰€å¯¹åº”çš„éšæœºè¯•éªŒé‡å¤å¤šæ¬¡ï¼Œéšç€è¯•éªŒæ¬¡æ•°çš„å¢åŠ ï¼ŒXçš„å‡å€¼ä¼šæ„ˆå‘è¶‹è¿‘äºE(X)ã€‚æˆ–å½“æ ·æœ¬æ•°æ®æ— é™å¤§æ—¶ï¼Œæ ·æœ¬å‡å€¼è¶‹äºæ€»ä½“å‡å€¼ã€‚ ä¸­å¿ƒæé™å®šç†ä¸­å¿ƒæé™å®šç†ï¼ˆcentral limit theoremï¼‰:è®¾ä»å‡å€¼ä¸ºÎ¼ã€æ–¹å·®ä¸ºo2 ï¼ˆæœ‰é™ï¼‰çš„ä»»æ„ä¸€ä¸ªæ€»ä½“ä¸­æŠ½å–æ ·æœ¬é‡ä¸ºnçš„æ ·æœ¬ï¼Œå½“nå……åˆ†å¤§æ—¶ï¼Œæ ·æœ¬å‡å€¼Xçš„æŠ½æ ·åˆ†å¸ƒè¿‘ä¼¼æœä»å‡å€¼ä¸ºÎ¼ã€æ–¹å·®ä¸ºo2/nçš„æ­£æ€åˆ†å¸ƒã€‚ å‚æ•°ä¼°è®¡ç½®ä¿¡åŒºé—´ã€ç½®ä¿¡åº¦åœ¨åŒºé—´ä¼°è®¡ä¸­ï¼Œç”±æ ·æœ¬ç»Ÿè®¡é‡æ‰€æ„é€ çš„æ€»ä½“å‚æ•°çš„ä¼°è®¡åŒºé—´ç§°ä¸ºç½®ä¿¡åŒºé—´ï¼ˆconfidence intervalï¼‰ï¼Œå…¶ä¸­åŒºé—´çš„æœ€å°å€¼ç§°ä¸ºç½®ä¿¡ä¸‹é™ï¼Œæœ€å¤§å€¼ç§°ä¸ºç½®ä¿¡ä¸Šé™ã€‚ç”±äºç»Ÿè®¡å­¦å®¶åœ¨æŸç§ç¨‹åº¦ä¸Šç¡®ä¿¡è¿™ä¸ªåŒºé—´ä¼šåŒ…å«çœŸæ­£çš„æ€»ä½“å‚æ•°ï¼Œæ‰€ä»¥ç»™å®ƒå–åä¸ºç½®ä¿¡åŒºé—´ã€‚åŸå› æ˜¯ï¼Œå¦‚æœæŠ½å–äº†è®¸å¤šä¸åŒçš„æ ·æœ¬ï¼Œæ¯”å¦‚è¯´æŠ½å–100ä¸ªæ ·æœ¬ï¼Œæ ¹æ®æ¯ä¸€ä¸ªæ ·æœ¬æ„é€ ä¸€ä¸ªç½®ä¿¡åŒºé—´ï¼Œè¿™æ ·ï¼Œç”±100ä¸ªæ ·æœ¬æ„é€ çš„æ€»ä½“å‚æ•°çš„100ä¸ªç½®ä¿¡åŒºé—´ä¸­ï¼Œæœ‰95%çš„åŒºé—´åŒ…å«äº†æ€»ä½“å‚æ•°çš„çœŸå€¼ï¼Œè€Œ5%åˆ™æ²¡åŒ…å«ï¼Œåˆ™95%è¿™ä¸ªå€¼ç§°ä¸ºç½®ä¿¡æ°´å¹³ã€‚ä¸€ èˆ¬åœ°ï¼Œå¦‚æœå°†æ„é€ ç½®ä¿¡åŒºé—´çš„æ­¥éª¤é‡å¤å¤šæ¬¡ï¼Œç½®ä¿¡åŒºé—´ä¸­åŒ…å«æ€»ä½“å‚æ•°çœŸå€¼çš„æ¬¡æ•°æ‰€å çš„æ¯”ä¾‹ç§°ä¸ºç½®ä¿¡æ°´å¹³ï¼ˆ confidence levelï¼‰ï¼Œä¹Ÿç§°ä¸ºç½®ä¿¡åº¦æˆ–ç½®ä¿¡ç³»æ•°ï¼ˆ confidence coefficientï¼‰ã€‚ è¯„ä»·ä¼°è®¡é‡çš„æ ‡å‡†ï¼š1.æ— åæ€§ 2.æœ‰æ•ˆæ€§ 3.ä¸€è‡´æ€§ ä¸€ä¸ªæ€»ä½“å‚æ•°çš„åŒºé—´ä¼°è®¡ ä¸¤ä¸ªæ€»ä½“å‚æ•°çš„åŒºé—´ä¼°è®¡ å‡è®¾æ£€éªŒH0: åŸå‡è®¾H1: å¤‡æ‹©å‡è®¾ æ£€éªŒç»Ÿè®¡é‡ï¼šç”¨äºå‡è®¾æ£€éªŒè®¡ç®—çš„ç»Ÿè®¡é‡ï¼ŒåŸºäºæ ·æœ¬æ£€éªŒç»Ÿè®¡é‡çš„å€¼æ¥æ¥å—æˆ–è€…æ‹’ç»åŸå‡è®¾ã€‚åœ¨åŸå‡è®¾æˆç«‹çš„æƒ…å†µä¸‹ï¼Œæ£€éªŒç»Ÿè®¡é‡æœä»ä¸€ä¸ªç‰¹å®šçš„åˆ†å¸ƒï¼›è€Œåœ¨å¤‡æ‹©å‡è®¾æˆç«‹çš„æƒ…å†µä¸‹ï¼Œåˆ™ä¸æœä»è¯¥åˆ†å¸ƒã€‚å¸¸ç”¨çš„æ£€éªŒç»Ÿè®¡é‡æœ‰tç»Ÿè®¡é‡ã€zç»Ÿè®¡é‡ç­‰ã€‚ å‡è®¾æ£€éªŒçš„åŸºæœ¬æ€æƒ³é€šè¿‡è¯æ˜åœ¨åŸå‡è®¾æˆç«‹çš„å‰æä¸‹ï¼Œæ£€éªŒç»Ÿè®¡é‡å‡ºç°å½“å‰å€¼æˆ–è€…æ›´ä¸ºæç«¯çš„å€¼å±äºâ€œå°æ¦‚ç‡â€äº‹ä»¶ï¼Œä»¥æ­¤æ¨ç¿»åŸå‡è®¾ï¼Œæ¥å—å¤‡æ‹©å‡è®¾ã€‚ p-valueâ€œæ£€éªŒç»Ÿè®¡é‡å‡ºç°å½“å‰å€¼æˆ–è€…æ›´ä¸ºæç«¯çš„å€¼â€çš„æ¦‚ç‡å°±æ˜¯p-value.å°†på€¼ä¸é¢„å…ˆè®¾å®šçš„æ˜¾è‘—æ€§æ°´å¹³Î±è¿›è¡Œå¯¹æ¯”ï¼Œå¦‚æœpå€¼å°äºÎ±ï¼Œå°±å¯ä»¥æ¨ç¿»åŸå‡è®¾ï¼Œæ¥å—å¤‡æ‹©å‡è®¾ã€‚ ä¸¤ç±»é”™è¯¯ é¡¹ç›® æ²¡æœ‰æ‹’ç»H0 æ‹’ç»H0 H0ä¸ºçœŸ 1-Î± (æ­£ç¡®å†³ç­–) Î± (å¼ƒçœŸé”™è¯¯/ç¬¬iç±»é”™è¯¯) H0ä¸ºä¼ª Î² (å–ä¼ªé”™è¯¯/ç¬¬iiç±»é”™è¯¯) 1-Î² (æ­£ç¡®å†³ç­–) æŠ½æ ·åˆ†å¸ƒï¼šzåˆ†å¸ƒã€å¡æ–¹åˆ†å¸ƒã€tåˆ†å¸ƒã€Fåˆ†å¸ƒzåˆ†å¸ƒzåˆ†å¸ƒï¼šæ­£æ€åˆ†å¸ƒï¼ˆNormal distributionï¼‰åˆåé«˜æ–¯åˆ†å¸ƒï¼ˆGaussiandistributionï¼‰ï¼Œè‹¥éšæœºå˜é‡Xæœä»ä¸€ä¸ªæ•°å­¦æœŸæœ›ä¸ºÎ¼ã€æ–¹å·®ä¸ºÏƒ2çš„é«˜æ–¯åˆ†å¸ƒï¼Œè®°ä¸ºN(Î¼ï¼ŒÏƒ2)ã€‚ å¡æ–¹åˆ†å¸ƒè‹¥nä¸ªç›¸äº’ç‹¬ç«‹çš„éšæœºå˜é‡Î¾â‚ï¼ŒÎ¾â‚‚ï¼Œâ€¦,Î¾n ï¼Œå‡æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼ˆä¹Ÿç§°ç‹¬ç«‹åŒåˆ†å¸ƒäºæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼‰ï¼Œåˆ™è¿™nä¸ªæœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„éšæœºå˜é‡çš„å¹³æ–¹å’Œæ„æˆä¸€æ–°çš„éšæœºå˜é‡ï¼Œå…¶åˆ†å¸ƒè§„å¾‹ç§°ä¸ºå¡æ–¹åˆ†å¸ƒï¼ˆchi-square distributionï¼‰ã€‚å…¶ä¸­å‚æ•°nç§°ä¸ºè‡ªç”±åº¦ã€‚å¡æ–¹åˆ†å¸ƒæ˜¯ç”±æ­£æ€åˆ†å¸ƒæ„é€ è€Œæˆçš„ä¸€ä¸ªæ–°çš„åˆ†å¸ƒï¼Œå½“è‡ªç”±åº¦å¾ˆå¤§æ—¶ï¼Œå¡æ–¹åˆ†å¸ƒè¿‘ä¼¼ä¸ºæ­£æ€åˆ†å¸ƒã€‚E = n, D = 2n tåˆ†å¸ƒtåˆ†å¸ƒ:é¦–å…ˆè¦æä¸€å¥uåˆ†å¸ƒï¼Œæ­£æ€åˆ†å¸ƒï¼ˆnormal distributionï¼‰æ˜¯è®¸å¤šç»Ÿè®¡æ–¹æ³•çš„ç†è®ºåŸºç¡€ã€‚æ­£æ€åˆ†å¸ƒçš„ä¸¤ä¸ªå‚æ•°Î¼å’ŒÏƒå†³å®šäº†æ­£æ€åˆ†å¸ƒçš„ä½ç½®å’Œå½¢æ€ã€‚ä¸ºäº†åº”ç”¨æ–¹ä¾¿ï¼Œå¸¸å°†ä¸€èˆ¬çš„æ­£æ€å˜ é‡Xé€šè¿‡uå˜æ¢(Xä¸€Î¼)/Ïƒè½¬åŒ–æˆæ ‡å‡†æ­£æ€å˜é‡uï¼Œ ä»¥ä½¿åŸæ¥å„ç§å½¢æ€çš„æ­£æ€åˆ†å¸ƒéƒ½è½¬æ¢ä¸ºÎ¼=0ï¼ŒÏƒ= 1çš„æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼ˆstandard normaldistributionï¼‰ ï¼Œäº¦ç§°uåˆ†å¸ƒã€‚æ ¹æ®ä¸­å¿ƒæé™å®šç†ï¼Œé€šè¿‡æŠ½æ ·æ¨¡æ‹Ÿè¯•éªŒè¡¨æ˜ï¼Œåœ¨æ­£æ€åˆ†å¸ƒæ€»ä½“ä¸­ä»¥å›ºå®šnæŠ½å–è‹¥å¹²ä¸ªæ ·æœ¬æ—¶ï¼Œæ ·æœ¬å‡æ•°çš„åˆ†å¸ƒä»æœä»æ­£æ€åˆ†å¸ƒï¼Œå³N (Î¼ï¼Œ Ïƒ)ã€‚æ‰€ä»¥ï¼Œå¯¹æ ·æœ¬å‡æ•°çš„åˆ†å¸ƒè¿›è¡Œuå˜æ¢ï¼Œä¹Ÿå¯å˜æ¢ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒN(0,1)ç”±äºåœ¨å®é™…å·¥ä½œä¸­ï¼Œå¾€å¾€Ïƒï¼ˆæ€»ä½“æ–¹å·®ï¼‰æ˜¯æœªçŸ¥çš„ï¼Œå¸¸ç”¨s ï¼ˆæ ·æœ¬æ–¹å·®ï¼‰ä½œä¸ºoçš„ä¼°è®¡å€¼ï¼Œä¸ºäº†ä¸uå˜æ¢åŒºåˆ«ï¼Œç§°ä¸ºtå˜æ¢ï¼Œç»Ÿè®¡é‡tå€¼çš„åˆ†å¸ƒç§°ä¸ºtåˆ†å¸ƒã€‚ Fåˆ†å¸ƒ ä¸€ä¸ªæ€»ä½“å‚æ•°çš„æ£€éªŒ ä¸¤ä¸ªæ€»ä½“å‚æ•°çš„æ£€éªŒ å‚è€ƒèµ„æ–™[1]ã€Šç»Ÿè®¡å­¦å®Œå…¨æ•™ç¨‹ã€‹ç¬¬ä¸€ç‰ˆ L.æ²ƒå¡æ›¼[2]ã€Šç»Ÿè®¡å­¦ã€‹ç¬¬å…­ç‰ˆ è´¾ä¿Šå¹³","categories":[{"name":"02 äººå·¥æ™ºèƒ½","slug":"02-äººå·¥æ™ºèƒ½","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"æ•°æ®åˆ†æ","slug":"02-äººå·¥æ™ºèƒ½/æ•°æ®åˆ†æ","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"ç»Ÿè®¡","slug":"ç»Ÿè®¡","permalink":"http://example.com/tags/%E7%BB%9F%E8%AE%A1/"}]}],"categories":[{"name":"04 ç»„é˜Ÿå­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬30æœŸ æå®æ¯…æœºå™¨å­¦ä¹ ","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬30æœŸ-æå®æ¯…æœºå™¨å­¦ä¹ ","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"ç¬¬30æœŸ æ·±å…¥æµ…å‡ºPyTorch","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬30æœŸ-æ·±å…¥æµ…å‡ºPyTorch","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAPyTorch/"},{"name":"ç¬¬29æœŸ åŸºäºtransformerçš„NLP","slug":"04-ç»„é˜Ÿå­¦ä¹ /ç¬¬29æœŸ-åŸºäºtransformerçš„NLP","permalink":"http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"},{"name":"01 è®¡ç®—æœºåŸºç¡€","slug":"01-è®¡ç®—æœºåŸºç¡€","permalink":"http://example.com/categories/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"CS61A è®¡ç®—æœºç¨‹åºçš„æ„é€ ä¸è§£é‡Š","slug":"01-è®¡ç®—æœºåŸºç¡€/CS61A-è®¡ç®—æœºç¨‹åºçš„æ„é€ ä¸è§£é‡Š","permalink":"http://example.com/categories/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/CS61A-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9E%84%E9%80%A0%E4%B8%8E%E8%A7%A3%E9%87%8A/"},{"name":"æ²‰æ€å½•","slug":"æ²‰æ€å½•","permalink":"http://example.com/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"},{"name":"02 äººå·¥æ™ºèƒ½","slug":"02-äººå·¥æ™ºèƒ½","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"è‡ªç„¶è¯­è¨€å¤„ç†","slug":"02-äººå·¥æ™ºèƒ½/è‡ªç„¶è¯­è¨€å¤„ç†","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"æ•°æ®åˆ†æ","slug":"02-äººå·¥æ™ºèƒ½/æ•°æ®åˆ†æ","permalink":"http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"03 å·¥å…·ç®±","slug":"03-å·¥å…·ç®±","permalink":"http://example.com/categories/03-%E5%B7%A5%E5%85%B7%E7%AE%B1/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"http://example.com/tags/machine-learning/"},{"name":"team leaning","slug":"team-leaning","permalink":"http://example.com/tags/team-leaning/"},{"name":"back propagation","slug":"back-propagation","permalink":"http://example.com/tags/back-propagation/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://example.com/tags/PyTorch/"},{"name":"team learning","slug":"team-learning","permalink":"http://example.com/tags/team-learning/"},{"name":"note","slug":"note","permalink":"http://example.com/tags/note/"},{"name":"ç»„é˜Ÿå­¦ä¹ ","slug":"ç»„é˜Ÿå­¦ä¹ ","permalink":"http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"},{"name":"regression","slug":"regression","permalink":"http://example.com/tags/regression/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"é¢„è®­ç»ƒæ¨¡å‹","slug":"é¢„è®­ç»ƒæ¨¡å‹","permalink":"http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"name":"transfomer","slug":"transfomer","permalink":"http://example.com/tags/transfomer/"},{"name":"æ€»ç»“","slug":"æ€»ç»“","permalink":"http://example.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"ç¬”è®°","slug":"ç¬”è®°","permalink":"http://example.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"BERT","slug":"BERT","permalink":"http://example.com/tags/BERT/"},{"name":"æ–‡æœ¬åˆ†ç±»","slug":"æ–‡æœ¬åˆ†ç±»","permalink":"http://example.com/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"},{"name":"GPT","slug":"GPT","permalink":"http://example.com/tags/GPT/"},{"name":"attention","slug":"attention","permalink":"http://example.com/tags/attention/"},{"name":"CSå…¬å¼€è¯¾","slug":"CSå…¬å¼€è¯¾","permalink":"http://example.com/tags/CS%E5%85%AC%E5%BC%80%E8%AF%BE/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"function","slug":"function","permalink":"http://example.com/tags/function/"},{"name":"é˜…è¯»","slug":"é˜…è¯»","permalink":"http://example.com/tags/%E9%98%85%E8%AF%BB/"},{"name":"math","slug":"math","permalink":"http://example.com/tags/math/"},{"name":"æ–‡æœ¬è¡¨ç¤º","slug":"æ–‡æœ¬è¡¨ç¤º","permalink":"http://example.com/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"},{"name":"SQL","slug":"SQL","permalink":"http://example.com/tags/SQL/"},{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"},{"name":"å¤§ç†","slug":"å¤§ç†","permalink":"http://example.com/tags/%E5%A4%A7%E7%90%86/"},{"name":"æ€è€ƒ","slug":"æ€è€ƒ","permalink":"http://example.com/tags/%E6%80%9D%E8%80%83/"},{"name":"æ’­å®¢","slug":"æ’­å®¢","permalink":"http://example.com/tags/%E6%92%AD%E5%AE%A2/"},{"name":"laTeX","slug":"laTeX","permalink":"http://example.com/tags/laTeX/"},{"name":"ç®€å†","slug":"ç®€å†","permalink":"http://example.com/tags/%E7%AE%80%E5%8E%86/"},{"name":"ç»Ÿè®¡","slug":"ç»Ÿè®¡","permalink":"http://example.com/tags/%E7%BB%9F%E8%AE%A1/"}]}