<!DOCTYPE html>
<html lang=en>
<head>
    <meta charset="utf-8">
    
    <title>task04 学习GPT | Memex</title>
    
    
        <meta name="keywords" content="笔记,NLP,组队学习,预训练模型,GPT" />
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="从语言模型说起自编码语言模型（auto-encoder）自编码语言模型通过随机Mask输入的部分单词，然后预训练的目标是预测被Mask的单词，不仅可以融入上文信息，还可以自然的融入下文信息。ex. BERT.  优点：自然地融入双向语言模型，同时看到被预测单词的上文和下文 缺点：训练和预测不一致。训练的时候输入引入了[Mask]标记，但是在预测阶段往往没有这个[Mask]标记，导致预训练阶段和Fi">
<meta property="og:type" content="article">
<meta property="og:title" content="task04 学习GPT">
<meta property="og:url" content="http://example.com/nlp-transformer-task04/index.html">
<meta property="og:site_name" content="Memex">
<meta property="og:description" content="从语言模型说起自编码语言模型（auto-encoder）自编码语言模型通过随机Mask输入的部分单词，然后预训练的目标是预测被Mask的单词，不仅可以融入上文信息，还可以自然的融入下文信息。ex. BERT.  优点：自然地融入双向语言模型，同时看到被预测单词的上文和下文 缺点：训练和预测不一致。训练的时候输入引入了[Mask]标记，但是在预测阶段往往没有这个[Mask]标记，导致预训练阶段和Fi">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/trm_1.png?raw=true">
<meta property="article:published_time" content="2021-09-19T12:02:17.000Z">
<meta property="article:modified_time" content="2021-09-25T19:41:59.566Z">
<meta property="article:author" content="Xiaoyu CHU">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="组队学习">
<meta property="article:tag" content="预训练模型">
<meta property="article:tag" content="GPT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/trm_1.png?raw=true">
    

    
        <link rel="alternate" href="/atom.xml" title="Memex" type="application/atom+xml" />
    

    
        <link rel="icon" href="/favicon.ico" />
    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/open-sans/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">

    
<script src="/libs/jquery/2.1.3/jquery.min.js"></script>

    
<script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">Memex</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">首页</a>
                
                    <a class="main-nav-link" href="/archives">归档</a>
                
                    <a class="main-nav-link" href="/categories">分类</a>
                
                    <a class="main-nav-link" href="/tags">标签</a>
                
                    <a class="main-nav-link" href="/about">关于</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">首页</a></td>
                
                    <td><a class="main-nav-link" href="/archives">归档</a></td>
                
                    <td><a class="main-nav-link" href="/categories">分类</a></td>
                
                    <td><a class="main-nav-link" href="/tags">标签</a></td>
                
                    <td><a class="main-nav-link" href="/about">关于</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>categories</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            01 计算机基础
                        </a>
                         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            CS61A 计算机程序的构造与解释
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/cs61a-week1/">CS61A Week1 Comupter_Science, Functions</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            02 人工智能
                        </a>
                         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            数据分析
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/">概率论与数理统计</a></li>  <li class="file"><a href="/SQL%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/">SQL学习资料</a></li>  <li class="file"><a href="/SQL%E9%87%8D%E7%82%B9/">SQL表连接&聚合函数&窗口函数</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            自然语言处理
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/nlp-text-representation/">NLP中的文本表示方法</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            03 工具箱
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/%E4%B8%80%E4%BA%9B%E5%A5%BD%E7%94%A8%E7%9A%84%E4%B8%AD%E8%8B%B1%E6%96%87LaTeX%E7%AE%80%E5%8E%86%E6%A8%A1%E6%9D%BF/">一些好用的中英文LaTeX简历模板</a></li>  <li class="file"><a href="/git/">5h打通Git全套教程</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            04 组队学习
                        </a>
                         <ul class="unstyled" id="tree" > 
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            第29期 基于Transformer的NLP
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/nlp-transformer-task01/">task01 NLP学习概览</a></li>  <li class="file"><a href="/nlp-transformer-task02/">task02 学习Attentioin和Transformer</a></li>  <li class="file"><a href="/nlp-transformer-task03/">task03 学习BERT</a></li>  <li class="file active"><a href="/nlp-transformer-task04/">task04 学习GPT</a></li>  <li class="file"><a href="/nlp-transformer-task05/">task05 编写BERT模型</a></li>  <li class="file"><a href="/nlp-transformer-task06/">task06 BERT应用、训练和优化</a></li>  <li class="file"><a href="/nlp-transformer-task07/">task07 Transformers解决文本分类</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            第30期 深入浅出PyTorch
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/pytorch-chap01-02/">chap01-02 PyTorch的简介和安装、PyTorch基础知识</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            沉思录
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/%E6%A0%A1%E5%87%86%E5%AF%B9%E4%B8%96%E7%95%8C%E7%9A%84%E9%A2%84%E6%9C%9F/">校准对世界的预期</a></li>  <li class="file"><a href="/%E5%A4%A7%E7%90%86/">大理：在民宿打工的日子</a></li>  <li class="file"><a href="/%E5%B0%8F%E7%8B%97%E9%92%B1%E9%92%B1/">小狗钱钱：理财启蒙的童话</a></li>  <li class="file"><a href="/formula/">公式之美：EVERYTHING IS EPHEMERAL BUT FORMULA IS ETERNAL</a></li>  </ul> 
                    </li> 
                     </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title"><span>tag cloud</span></h3>
        <div class="widget tagcloud">
            <a href="/tags/BERT/" style="font-size: 14px;">BERT</a> <a href="/tags/CS%E5%85%AC%E5%BC%80%E8%AF%BE/" style="font-size: 10px;">CS公开课</a> <a href="/tags/GPT/" style="font-size: 10px;">GPT</a> <a href="/tags/NLP/" style="font-size: 18px;">NLP</a> <a href="/tags/PyTorch/" style="font-size: 10px;">PyTorch</a> <a href="/tags/SQL/" style="font-size: 12px;">SQL</a> <a href="/tags/attention/" style="font-size: 10px;">attention</a> <a href="/tags/function/" style="font-size: 10px;">function</a> <a href="/tags/git/" style="font-size: 10px;">git</a> <a href="/tags/laTeX/" style="font-size: 10px;">laTeX</a> <a href="/tags/math/" style="font-size: 10px;">math</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/transfomer/" style="font-size: 14px;">transfomer</a> <a href="/tags/%E5%A4%A7%E7%90%86/" style="font-size: 10px;">大理</a> <a href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" style="font-size: 10px;">文本分类</a> <a href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/" style="font-size: 10px;">文本表示</a> <a href="/tags/%E6%B2%89%E6%80%9D%E5%BD%95/" style="font-size: 10px;">沉思录</a> <a href="/tags/%E7%AC%94%E8%AE%B0/" style="font-size: 20px;">笔记</a> <a href="/tags/%E7%AE%80%E5%8E%86/" style="font-size: 10px;">简历</a> <a href="/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/" style="font-size: 18px;">组队学习</a> <a href="/tags/%E7%BB%9F%E8%AE%A1/" style="font-size: 10px;">统计</a> <a href="/tags/%E8%B4%A2%E5%8A%A1%E7%AE%A1%E7%90%86/" style="font-size: 10px;">财务管理</a> <a href="/tags/%E9%98%85%E8%AF%BB/" style="font-size: 12px;">阅读</a> <a href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" style="font-size: 16px;">预训练模型</a>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title"><span>recent</span></h3>
        <div class="widget">
            <ul id="recent-post" class="">
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/nlp-transformer-task07/" class="thumbnail">
    
    
        <span style="background-image:url(https://github.com/chuxiaoyu/blog_image/blob/master/nlp/trm_1.png?raw=true)" alt="task07 Transformers解决文本分类" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/">04 组队学习</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8ETransformer%E7%9A%84NLP/">第29期 基于Transformer的NLP</a></p>
                            <p class="item-title"><a href="/nlp-transformer-task07/" class="title">task07 Transformers解决文本分类</a></p>
                            <p class="item-date"><time datetime="2021-09-25T08:57:45.000Z" itemprop="datePublished">2021-09-25</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/nlp-transformer-task06/" class="thumbnail">
    
    
        <span style="background-image:url(https://github.com/chuxiaoyu/blog_image/blob/master/nlp/trm_1.png?raw=true)" alt="task06 BERT应用、训练和优化" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/">04 组队学习</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8ETransformer%E7%9A%84NLP/">第29期 基于Transformer的NLP</a></p>
                            <p class="item-title"><a href="/nlp-transformer-task06/" class="title">task06 BERT应用、训练和优化</a></p>
                            <p class="item-date"><time datetime="2021-09-24T07:18:42.000Z" itemprop="datePublished">2021-09-24</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/nlp-transformer-task05/" class="thumbnail">
    
    
        <span style="background-image:url(https://github.com/chuxiaoyu/blog_image/blob/master/nlp/trm_1.png?raw=true)" alt="task05 编写BERT模型" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/">04 组队学习</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8ETransformer%E7%9A%84NLP/">第29期 基于Transformer的NLP</a></p>
                            <p class="item-title"><a href="/nlp-transformer-task05/" class="title">task05 编写BERT模型</a></p>
                            <p class="item-date"><time datetime="2021-09-20T19:01:35.000Z" itemprop="datePublished">2021-09-21</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/nlp-transformer-task04/" class="thumbnail">
    
    
        <span style="background-image:url(https://github.com/chuxiaoyu/blog_image/blob/master/nlp/trm_1.png?raw=true)" alt="task04 学习GPT" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/">04 组队学习</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8ETransformer%E7%9A%84NLP/">第29期 基于Transformer的NLP</a></p>
                            <p class="item-title"><a href="/nlp-transformer-task04/" class="title">task04 学习GPT</a></p>
                            <p class="item-date"><time datetime="2021-09-19T12:02:17.000Z" itemprop="datePublished">2021-09-19</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/pytorch-chap01-02/" class="thumbnail">
    
    
        <span style="background-image:url(https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/torch_logo.jpeg?raw=true)" alt="chap01-02 PyTorch的简介和安装、PyTorch基础知识" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/">04 组队学习</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAPyTorch/">第30期 深入浅出PyTorch</a></p>
                            <p class="item-title"><a href="/pytorch-chap01-02/" class="title">chap01-02 PyTorch的简介和安装、PyTorch基础知识</a></p>
                            <p class="item-date"><time datetime="2021-09-18T02:26:03.000Z" itemprop="datePublished">2021-09-18</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-nlp-transformer-task04" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/">04 组队学习</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8ETransformer%E7%9A%84NLP/">第29期 基于Transformer的NLP</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/tags/GPT/" rel="tag">GPT</a>, <a class="tag-link-link" href="/tags/NLP/" rel="tag">NLP</a>, <a class="tag-link-link" href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag">笔记</a>, <a class="tag-link-link" href="/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/" rel="tag">组队学习</a>, <a class="tag-link-link" href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" rel="tag">预训练模型</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/nlp-transformer-task04/">
            <time datetime="2021-09-19T12:02:17.000Z" itemprop="datePublished">2021-09-19</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            task04 学习GPT
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
                <div id="toc" class="toc-article">
                <strong class="toc-title">Catalogue</strong>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AF%B4%E8%B5%B7"><span class="toc-number">1.</span> <span class="toc-text">从语言模型说起</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E7%BC%96%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88auto-encoder%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">自编码语言模型（auto-encoder）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88auto-regressive%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">自回归语言模型（auto-regressive）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer-BERT-GPT-2%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">2.</span> <span class="toc-text">Transformer, BERT, GPT-2的关系</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GPT-2%E6%A6%82%E8%BF%B0"><span class="toc-number">3.</span> <span class="toc-text">GPT-2概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BE%93%E5%85%A5"><span class="toc-number">3.1.</span> <span class="toc-text">模型的输入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Decoder%E5%B1%82"><span class="toc-number">3.2.</span> <span class="toc-text">Decoder层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BE%93%E5%87%BA"><span class="toc-number">3.3.</span> <span class="toc-text">模型的输出</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B3%E4%BA%8ESelf-Attention-Masked-Self-Attention"><span class="toc-number">4.</span> <span class="toc-text">关于Self-Attention, Masked Self-Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-Attention"><span class="toc-number">4.1.</span> <span class="toc-text">Self-Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Masked-Self-Attention"><span class="toc-number">4.2.</span> <span class="toc-text">Masked Self-Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GPT-2%E4%B8%AD%E7%9A%84Self-Attention"><span class="toc-number">4.3.</span> <span class="toc-text">GPT-2中的Self-Attention</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">5.</span> <span class="toc-text">自回归语言模型的应用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">6.</span> <span class="toc-text">参考资料</span></a></li></ol>
                </div>
            
        
        
            <h1 id="从语言模型说起"><a href="#从语言模型说起" class="headerlink" title="从语言模型说起"></a>从语言模型说起</h1><h2 id="自编码语言模型（auto-encoder）"><a href="#自编码语言模型（auto-encoder）" class="headerlink" title="自编码语言模型（auto-encoder）"></a>自编码语言模型（auto-encoder）</h2><p>自编码语言模型通过随机Mask输入的部分单词，然后预训练的目标是预测被Mask的单词，不仅可以融入上文信息，还可以自然的融入下文信息。ex. BERT.</p>
<ul>
<li>优点：自然地融入双向语言模型，同时看到被预测单词的上文和下文</li>
<li>缺点：训练和预测不一致。训练的时候输入引入了[Mask]标记，但是在预测阶段往往没有这个[Mask]标记，导致预训练阶段和Fine-tuning阶段不一致。</li>
</ul>
<h2 id="自回归语言模型（auto-regressive）"><a href="#自回归语言模型（auto-regressive）" class="headerlink" title="自回归语言模型（auto-regressive）"></a>自回归语言模型（auto-regressive）</h2><p>语言模型根据输入句子的一部分文本来预测下一个词。ex. GPT-2</p>
<ul>
<li>优点：对于生成类的NLP任务，比如文本摘要，机器翻译等，从左向右的生成内容，天然和自回归语言模型契合。</li>
<li>缺点：由于一般是从左到右（当然也可能从右到左），所以只能利用上文或者下文的信息，不能同时利用上文和下文的信息。</li>
</ul>
<h1 id="Transformer-BERT-GPT-2的关系"><a href="#Transformer-BERT-GPT-2的关系" class="headerlink" title="Transformer, BERT, GPT-2的关系"></a>Transformer, BERT, GPT-2的关系</h1><p>Transformer的Encoder进化成了BERT，Decoder进化成了GPT2。</p>
<p>如果要使用Transformer来解决语言模型任务，并不需要完整的Encoder部分和Decoder部分，于是在原始Transformer之后的许多研究工作中，人们尝试只使用Transformer Encoder或者Decoder进行预训练。比如BERT只使用了Encoder部分进行masked language model（自编码）训练，GPT-2便是只使用了Decoder部分进行自回归（auto regressive）语言模型训练。</p>
<h1 id="GPT-2概述"><a href="#GPT-2概述" class="headerlink" title="GPT-2概述"></a>GPT-2概述</h1><h2 id="模型的输入"><a href="#模型的输入" class="headerlink" title="模型的输入"></a>模型的输入</h2><p>输入的处理分为两步：token embedding + position encoding。即:</p>
<ol>
<li>在嵌入矩阵中查找输入的单词的对应的embedding向量</li>
<li>融入位置编码</li>
</ol>
<h2 id="Decoder层"><a href="#Decoder层" class="headerlink" title="Decoder层"></a>Decoder层</h2><p>每一层decoder的组成：Masked Self-Attention + Feed Forward Neural Network</p>
<p>Self-Attention所做的事情是：它通过对句子片段中每个词的相关性打分，并将这些词的表示向量根据相关性加权求和，从而让模型能够将词和其他相关词向量的信息融合起来。</p>
<p>Masked Self-Attention做的是：将mask位置对应的的attention score变成一个非常小的数字或者0，让其他单词再self attention的时候（加权求和的时候）不考虑这些单词。</p>
<h2 id="模型的输出"><a href="#模型的输出" class="headerlink" title="模型的输出"></a>模型的输出</h2><p>当模型顶部的Decoder层产生输出向量时，模型会将这个向量乘以一个巨大的嵌入矩阵（vocab size x embedding size）来计算该向量和所有单词embedding向量的相关得分。这个相乘的结果，被解释为模型词汇表中每个词的分数，经过softmax之后被转换成概率。</p>
<p>我们可以选择最高分数的 token（top_k=1），也可以同时考虑其他词（top k）。假设每个位置输出k个token，假设总共输出n个token，那么基于n个单词的联合概率选择的输出序列会更好。</p>
<p>模型完成一次迭代，输出一个单词。模型会继续迭代，直到所有的单词都已经生成，或者直到输出了表示句子末尾的token。</p>
<h1 id="关于Self-Attention-Masked-Self-Attention"><a href="#关于Self-Attention-Masked-Self-Attention" class="headerlink" title="关于Self-Attention, Masked Self-Attention"></a>关于Self-Attention, Masked Self-Attention</h1><h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>Self-Attention 主要通过 3 个步骤来实现：</p>
<ol>
<li>为每个路径创建 Query、Key、Value 矩阵。</li>
<li>对于每个输入的token，使用它的Query向量为所有其他的Key向量进行打分。</li>
<li>将 Value 向量乘以它们对应的分数后求和。</li>
</ol>
<h2 id="Masked-Self-Attention"><a href="#Masked-Self-Attention" class="headerlink" title="Masked Self-Attention"></a>Masked Self-Attention</h2><p>在Self-Attention的第2步，把未来的 token 评分设置为0，因此模型不能看到未来的词。</p>
<p>这个屏蔽（masking）经常用一个矩阵来实现，称为 attention mask矩阵。<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/mask_1.jpg?raw=true" width="600" alt="" align="center" /><br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/mask_2.jpg?raw=true" width="600" alt="" align="center" /></p>
<h2 id="GPT-2中的Self-Attention"><a href="#GPT-2中的Self-Attention" class="headerlink" title="GPT-2中的Self-Attention"></a>GPT-2中的Self-Attention</h2><p>(skip)</p>
<h1 id="自回归语言模型的应用"><a href="#自回归语言模型的应用" class="headerlink" title="自回归语言模型的应用"></a>自回归语言模型的应用</h1><p>应用在下游并取得不错效果的NLP任务有：机器翻译、摘要生成、音乐生成。<em>（可见，主要是跟预训练任务相似的生成类任务。）</em></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li>基于transformers的自然语言处理(NLP)入门–在线阅读 <a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/">https://datawhalechina.github.io/learn-nlp-with-transformers/#/</a></li>
</ul>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/nlp-transformer-task05/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Newer</strong>
            <div class="article-nav-title">
                
                    task05 编写BERT模型
                
            </div>
        </a>
    
    
        <a href="/pytorch-chap01-02/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Older</strong>
            <div class="article-nav-title">chap01-02 PyTorch的简介和安装、PyTorch基础知识</div>
        </a>
    
</nav>





    
    




<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            Xiaoyu CHU &copy; 2021 
            <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>
            <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a target="_blank" rel="noopener" href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a>
            
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
            
        </div>
    </div>
</footer>

        

    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


    </div>
</body>
</html>