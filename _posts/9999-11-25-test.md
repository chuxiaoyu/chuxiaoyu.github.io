---
layout: post
title: "[Pinned] Feature Post for Test üê≠"
date: 2025-11-25 18:30:00 +0200
---

> When ChatGPT came out, like many of my colleagues, I was disoriented. What surprised
me wasn‚Äôt the model‚Äôs size or capabilities. For over a decade, the AI community
has known that scaling up a model improves it. In 2012, the AlexNet authors
noted in their landmark paper that: ‚ÄúAll of our experiments suggest that our results
can be improved simply by waiting for faster GPUs and bigger datasets to become
available.‚Äù1, 2\
## test

What surprised me was the sheer number of applications this capability boost
unlocked. I thought a small increase in model quality metrics might result in a modest
increase in applications. Instead, it resulted in an explosion of new possibilities.
Not only have these new AI capabilities increased the demand for AI applications,
but they have also lowered the entry barrier for developers. It‚Äôs become so easy to get
started with building AI applications. It‚Äôs even possible to build an application
without writing a single line of code. This shift has transformed AI from a specialized
discipline into a powerful development tool everyone can use.
Even though AI adoption today seems new, it‚Äôs built upon techniques that have been
around for a while. Papers about language modeling came out as early as the 1950s.
Retrieval-augmented generation (RAG) applications are built upon retrieval technology
that has powered search and recommender systems since long before the term
RAG was coined. The best practices for deploying traditional machine learning applications‚Äî
systematic experimentation, rigorous evaluation, relentless optimization for
faster and cheaper models‚Äîare still the best practices for working with foundation
model-based applications.