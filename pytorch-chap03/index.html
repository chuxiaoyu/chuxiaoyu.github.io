<!DOCTYPE html>
<html lang=en>
<head>
    <meta charset="utf-8">
    
    <title>Chapter03 PyTorch的主要组成模块 | CHU XIAOYU</title>
    
    
        <meta name="keywords" content="笔记,组队学习,PyTorch" />
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="第三章 PyTorch的主要组成模块完成深度学习的必要部分机器学习：  数据预处理（数据格式、数据转换、划分数据集） 选择模型，设定损失和优化函数，设置超参数 训练模型，拟合训练集 评估模型，在并在验证集&#x2F;测试集上计算模型表现  深度学习的注意事项：  数据预处理（数据加载、批处理） 逐层搭建模型，组装不同模块 GPU的配置和操作    基本配置导入必须的包： 123456import osimp">
<meta property="og:type" content="article">
<meta property="og:title" content="Chapter03 PyTorch的主要组成模块">
<meta property="og:url" content="http://example.com/pytorch-chap03/index.html">
<meta property="og:site_name" content="CHU XIAOYU">
<meta property="og:description" content="第三章 PyTorch的主要组成模块完成深度学习的必要部分机器学习：  数据预处理（数据格式、数据转换、划分数据集） 选择模型，设定损失和优化函数，设置超参数 训练模型，拟合训练集 评估模型，在并在验证集&#x2F;测试集上计算模型表现  深度学习的注意事项：  数据预处理（数据加载、批处理） 逐层搭建模型，组装不同模块 GPU的配置和操作    基本配置导入必须的包： 123456import osimp">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/torch_logo.jpeg?raw=true">
<meta property="article:published_time" content="2021-09-28T01:31:42.000Z">
<meta property="article:modified_time" content="2022-01-24T15:29:02.544Z">
<meta property="article:author" content="chuxiaoyu">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="组队学习">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/torch_logo.jpeg?raw=true">
    

    
        <link rel="alternate" href="/atom.xml" title="CHU XIAOYU" type="application/atom+xml" />
    

    
        <link rel="icon" href="/images/logo.png" />
    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/open-sans/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">

    
<script src="/libs/jquery/2.1.3/jquery.min.js"></script>

    
<script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">CHU XIAOYU</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">首页</a>
                
                    <a class="main-nav-link" href="/archives">归档</a>
                
                    <a class="main-nav-link" href="/categories">分类</a>
                
                    <a class="main-nav-link" href="/tags">标签</a>
                
                    <a class="main-nav-link" href="/about">关于</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">首页</a></td>
                
                    <td><a class="main-nav-link" href="/archives">归档</a></td>
                
                    <td><a class="main-nav-link" href="/categories">分类</a></td>
                
                    <td><a class="main-nav-link" href="/tags">标签</a></td>
                
                    <td><a class="main-nav-link" href="/about">关于</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>categories</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            01 计算机基础
                        </a>
                         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            CS61A 计算机程序的构造与解释
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/cs61a-week1/">CS61A Week1 Comupter_Science, Functions</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            数据结构与算法
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/dsa-python/">数据结构与算法Python</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            02 人工智能
                        </a>
                         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            CS224n 自然语言处理
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/cs224n-lec07/">CS224n Lec07 机器翻译、Seq2seq模型、注意力机制</a></li>  <li class="file"><a href="/cs224n-lec09/">CS224n Lec09 自注意力模型、Transformers</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            数据分析
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/">概率论与数理统计</a></li>  <li class="file"><a href="/SQL%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/">SQL学习资料</a></li>  <li class="file"><a href="/SQL%E9%87%8D%E7%82%B9/">SQL表连接&聚合函数&窗口函数</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            李宏毅机器学习
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/leeml-lec01-introduction/">Lecture01 机器学习和深度学习简介</a></li>  <li class="file"><a href="/leeml-rnn/">循环神经网络</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            03 工具箱
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/%E4%B8%80%E4%BA%9B%E5%A5%BD%E7%94%A8%E7%9A%84%E4%B8%AD%E8%8B%B1%E6%96%87LaTeX%E7%AE%80%E5%8E%86%E6%A8%A1%E6%9D%BF/">一些好用的中英文LaTeX简历模板</a></li>  <li class="file"><a href="/git/">5h打通Git全套教程</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            04 组队学习
                        </a>
                         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            2021-09 基于transformer的NLP
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/nlp-transformer-task01/">Task01 NLP学习概览</a></li>  <li class="file"><a href="/nlp-transformer-task02/">Task02 学习Attentioin和Transformer</a></li>  <li class="file"><a href="/nlp-transformer-task03/">Task03 学习BERT</a></li>  <li class="file"><a href="/nlp-transformer-task04/">Task04 学习GPT</a></li>  <li class="file"><a href="/nlp-transformer-task05/">Task05 编写BERT模型</a></li>  <li class="file"><a href="/nlp-transformer-task06/">Task06 BERT应用、训练和优化</a></li>  <li class="file"><a href="/nlp-transformer-task07/">Task07 使用Transformers解决文本分类任务</a></li>  <li class="file"><a href="/nlp-transformer-summary/">Summary Transformer课程总结</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            2021-10 深入浅出PyTorch
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/pytorch-chap01-02/">Chapter01-02 PyTorch的简介和安装、PyTorch基础知识</a></li>  <li class="file active"><a href="/pytorch-chap03/">Chapter03 PyTorch的主要组成模块</a></li>  <li class="file"><a href="/pytorch-chap04/">Chapter04 PyTorch基础实战——FashionMNIST图像分类</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            2022-01 LeetCode刷题
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/leetcode/">Leetcode刷题（第1-2期）</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            沉思录
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/%E5%A4%A7%E7%90%86/">大理，在民宿打工的日子</a></li>  <li class="file"><a href="/formula/">公式之美，EVERYTHING IS EPHEMERAL BUT FORMULA IS ETERNAL</a></li>  <li class="file"><a href="/feynman-quotes/">费曼语录（翻译自Twitter）</a></li>  </ul> 
                    </li> 
                     <li class="file"><a href="/hello-world/">Hello-World|本站导航</a></li>  </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title"><span>recent</span></h3>
        <div class="widget">
            <ul id="recent-post" class="">
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/cs224n-lec09/" class="thumbnail">
    
    
        <span style="background-image:url(https://github.com/chuxiaoyu/blog_image/blob/master/cs224n/nlp.jpg?raw=true)" alt="CS224n Lec09 自注意力模型、Transformers" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">02 人工智能</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/CS224n-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">CS224n 自然语言处理</a></p>
                            <p class="item-title"><a href="/cs224n-lec09/" class="title">CS224n Lec09 自注意力模型、Transformers</a></p>
                            <p class="item-date"><time datetime="2022-03-22T22:19:47.000Z" itemprop="datePublished">2022-03-22</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/dsa-python/" class="thumbnail">
    
    
        <span style="background-image:url(https://github.com/chuxiaoyu/blog_image/blob/master/cs224n/leetcode.png?raw=true)" alt="数据结构与算法Python" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/">01 计算机基础</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/">数据结构与算法</a></p>
                            <p class="item-title"><a href="/dsa-python/" class="title">数据结构与算法Python</a></p>
                            <p class="item-date"><time datetime="2022-02-07T00:31:45.000Z" itemprop="datePublished">2022-02-07</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/leetcode/" class="thumbnail">
    
    
        <span style="background-image:url(https://github.com/chuxiaoyu/blog_image/blob/master/cs224n/leetcode.png?raw=true)" alt="Leetcode刷题（第1-2期）" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/">04 组队学习</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/2022-01-LeetCode%E5%88%B7%E9%A2%98/">2022-01 LeetCode刷题</a></p>
                            <p class="item-title"><a href="/leetcode/" class="title">Leetcode刷题（第1-2期）</a></p>
                            <p class="item-date"><time datetime="2022-01-24T15:26:18.000Z" itemprop="datePublished">2022-01-24</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/hello-world/" class="thumbnail">
    
    
        <span style="background-image:url(https://github.com/chuxiaoyu/blog_image/blob/master/formula/08-shang.jpg?raw=true)" alt="Hello-World|本站导航" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"></p>
                            <p class="item-title"><a href="/hello-world/" class="title">Hello-World|本站导航</a></p>
                            <p class="item-date"><time datetime="2022-01-19T18:39:32.000Z" itemprop="datePublished">2022-01-19</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/feynman-quotes/" class="thumbnail">
    
    
        <span style="background-image:url(https://github.com/chuxiaoyu/blog_image/blob/master/cs224n/feynman.jpg?raw=true)" alt="费曼语录（翻译自Twitter）" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E6%B2%89%E6%80%9D%E5%BD%95/">沉思录</a></p>
                            <p class="item-title"><a href="/feynman-quotes/" class="title">费曼语录（翻译自Twitter）</a></p>
                            <p class="item-date"><time datetime="2021-12-02T05:38:38.000Z" itemprop="datePublished">2021-12-02</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title"><span>archives</span></h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">October 2021</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">August 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/07/">July 2021</a><span class="archive-list-count">6</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title"><span>tag cloud</span></h3>
        <div class="widget tagcloud">
            <a href="/tags/BERT/" style="font-size: 14.29px;">BERT</a> <a href="/tags/CS%E5%85%AC%E5%BC%80%E8%AF%BE/" style="font-size: 12.86px;">CS公开课</a> <a href="/tags/GPT/" style="font-size: 10px;">GPT</a> <a href="/tags/NLP/" style="font-size: 18.57px;">NLP</a> <a href="/tags/PyTorch/" style="font-size: 12.86px;">PyTorch</a> <a href="/tags/SQL/" style="font-size: 11.43px;">SQL</a> <a href="/tags/attention/" style="font-size: 10px;">attention</a> <a href="/tags/data-structure/" style="font-size: 10px;">data structure</a> <a href="/tags/feynman/" style="font-size: 10px;">feynman</a> <a href="/tags/function/" style="font-size: 10px;">function</a> <a href="/tags/git/" style="font-size: 10px;">git</a> <a href="/tags/laTeX/" style="font-size: 10px;">laTeX</a> <a href="/tags/leetcode/" style="font-size: 10px;">leetcode</a> <a href="/tags/machine-learning/" style="font-size: 11.43px;">machine learning</a> <a href="/tags/math/" style="font-size: 10px;">math</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/quote/" style="font-size: 10px;">quote</a> <a href="/tags/regression/" style="font-size: 10px;">regression</a> <a href="/tags/rnn/" style="font-size: 10px;">rnn</a> <a href="/tags/team-learning/" style="font-size: 10px;">team learning</a> <a href="/tags/transfomer/" style="font-size: 15.71px;">transfomer</a> <a href="/tags/%E5%A4%A7%E7%90%86/" style="font-size: 10px;">大理</a> <a href="/tags/%E6%80%BB%E7%BB%93/" style="font-size: 10px;">总结</a> <a href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" style="font-size: 10px;">文本分类</a> <a href="/tags/%E7%AC%94%E8%AE%B0/" style="font-size: 20px;">笔记</a> <a href="/tags/%E7%AE%80%E5%8E%86/" style="font-size: 10px;">简历</a> <a href="/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">组队学习</a> <a href="/tags/%E7%BB%9F%E8%AE%A1/" style="font-size: 10px;">统计</a> <a href="/tags/%E9%98%85%E8%AF%BB/" style="font-size: 10px;">阅读</a> <a href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" style="font-size: 17.14px;">预训练模型</a>
        </div>
    </div>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-pytorch-chap03" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/">04 组队学习</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/2021-10-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAPyTorch/">2021-10 深入浅出PyTorch</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/tags/PyTorch/" rel="tag">PyTorch</a>, <a class="tag-link-link" href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag">笔记</a>, <a class="tag-link-link" href="/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/" rel="tag">组队学习</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/pytorch-chap03/">
            <time datetime="2021-09-28T01:31:42.000Z" itemprop="datePublished">2021-09-28</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            Chapter03 PyTorch的主要组成模块
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
                <div id="toc" class="toc-article">
                <strong class="toc-title">Catalogue</strong>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-PyTorch%E7%9A%84%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97"><span class="toc-number">1.</span> <span class="toc-text">第三章 PyTorch的主要组成模块</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%88%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BF%85%E8%A6%81%E9%83%A8%E5%88%86"><span class="toc-number">1.1.</span> <span class="toc-text">完成深度学习的必要部分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE"><span class="toc-number">1.2.</span> <span class="toc-text">基本配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%92%8C%E5%A4%84%E7%90%86"><span class="toc-number">1.3.</span> <span class="toc-text">数据加载和处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="toc-number">1.4.</span> <span class="toc-text">模型构建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9E%84%E9%80%A0"><span class="toc-number">1.4.1.</span> <span class="toc-text">神经网络的构造</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%84%E5%B1%82"><span class="toc-number">1.4.2.</span> <span class="toc-text">神经网络中常见的层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8D%E5%90%AB%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E7%9A%84%E5%B1%82"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">不含模型参数的层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AB%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E7%9A%84%E5%B1%82"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">含模型参数的层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E7%BB%B4%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">二维卷积层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">1.4.2.4.</span> <span class="toc-text">池化层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%A4%BA%E4%BE%8B%EF%BC%9ALeNet"><span class="toc-number">1.4.2.5.</span> <span class="toc-text">模型示例：LeNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%A4%BA%E4%BE%8B%EF%BC%9AAlexNet"><span class="toc-number">1.4.2.6.</span> <span class="toc-text">模型示例：AlexNet</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.5.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.5.1.</span> <span class="toc-text">二分类交叉熵损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.5.2.</span> <span class="toc-text">其他损失函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.6.</span> <span class="toc-text">优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.6.1.</span> <span class="toc-text">什么是优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch%E6%8F%90%E4%BE%9B%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.6.2.</span> <span class="toc-text">PyTorch提供的优化器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E8%AF%84%E4%BC%B0"><span class="toc-number">1.7.</span> <span class="toc-text">训练与评估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.8.</span> <span class="toc-text">可视化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">2.</span> <span class="toc-text">参考资料</span></a></li></ol>
                </div>
            
        
        
            <h1 id="第三章-PyTorch的主要组成模块"><a href="#第三章-PyTorch的主要组成模块" class="headerlink" title="第三章 PyTorch的主要组成模块"></a>第三章 PyTorch的主要组成模块</h1><h2 id="完成深度学习的必要部分"><a href="#完成深度学习的必要部分" class="headerlink" title="完成深度学习的必要部分"></a>完成深度学习的必要部分</h2><p>机器学习：</p>
<ol>
<li>数据预处理（数据格式、数据转换、划分数据集）</li>
<li>选择模型，设定损失和优化函数，设置超参数</li>
<li>训练模型，拟合训练集</li>
<li>评估模型，在并在验证集/测试集上计算模型表现</li>
</ol>
<p>深度学习的注意事项：</p>
<ol>
<li>数据预处理（数据加载、批处理）</li>
<li>逐层搭建模型，组装不同模块</li>
<li>GPU的配置和操作</li>
</ol>
<img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/11_dnn.jpg?raw=true" width="600" alt="" align="center" />

<h2 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h2><p>导入必须的包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optimizer</span><br></pre></td></tr></table></figure>

<p>超参数设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">16</span>  <span class="comment"># batch size</span></span><br><span class="line">lr = <span class="number">1e-4</span>  <span class="comment"># 初始学习率</span></span><br><span class="line">max_epochs = <span class="number">100</span>  <span class="comment"># 训练次数 </span></span><br></pre></td></tr></table></figure>

<p>GPU的设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方案一：使用os.environ，这种情况如果使用GPU不需要设置</span></span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;0,1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:1&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="数据加载和处理"><a href="#数据加载和处理" class="headerlink" title="数据加载和处理"></a>数据加载和处理</h2><p>PyTorch数据读入是通过Dataset+Dataloader的方式完成的，Dataset定义好数据的格式和数据变换形式，Dataloader用iterative的方式不断读入批次数据。</p>
<p>我们可以定义自己的Dataset类来实现灵活的数据读取，定义的类需要继承PyTorch自身的Dataset类。主要包含三个函数：</p>
<ul>
<li><code>__init__</code>: 用于向类中传入外部参数，同时定义样本集</li>
<li><code>__getitem__</code>: 用于逐个读取样本集合中的元素，可以进行一定的变换，并将返回训练/验证所需的数据</li>
<li><code>__len__</code>: 用于返回数据集的样本数</li>
</ul>
<img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/12_dataset.jpg?raw=true" width="600" alt="" align="center" />

<ul>
<li>batch_size：样本是按“批”读入的，batch_size就是每次读入的样本数</li>
<li>num_workers：有多少个进程用于读取数据</li>
<li>shuffle：是否将读入的数据打乱</li>
<li>drop_last：对于样本最后一部分没有达到批次数的样本，不再参与训练</li>
</ul>
<p>下面是本部分代码在notebook中的运行情况。主要参考 PyTorch官方教程中文版 <a target="_blank" rel="noopener" href="https://pytorch123.com/SecondSection/training_a_classifier/">https://pytorch123.com/SecondSection/training_a_classifier/</a></p>
<img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/chap03.jpg?raw=true" width="" alt="" align="center" />

<h2 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h2><h3 id="神经网络的构造"><a href="#神经网络的构造" class="headerlink" title="神经网络的构造"></a>神经网络的构造</h3><p>PyTorch中神经网络构造一般是基于 Module 类的模型来完成的。Module 类是 nn 模块里提供的一个模型构造类，是所有神经⽹网络模块的基类，我们可以继承它来定义我们想要的模型。下面继承 Module 类构造多层感知机（MLP）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="comment"># 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例例时还可以指定其他函数</span></span><br><span class="line">    <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br><span class="line">    self.hidden = nn.Linear(<span class="number">784</span>, <span class="number">256</span>)</span><br><span class="line">    self.act = nn.ReLU()</span><br><span class="line">    self.output = nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">   <span class="comment"># 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    o = self.act(self.hidden(x))</span><br><span class="line">    <span class="keyword">return</span> self.output(o)</span><br></pre></td></tr></table></figure>

<p>我们可以实例化 MLP 类得到模型变量 net 。下⾯的代码初始化 net 并传入输⼊数据 X 做一次前向计算。其中， net(X) 会调用 MLP 继承⾃自 Module 类的 <strong>call</strong> 函数，这个函数将调⽤用 MLP 类定义的forward 函数来完成前向计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = torch.rand(<span class="number">2</span>, <span class="number">784</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X</span><br><span class="line">tensor([[<span class="number">0.3277</span>, <span class="number">0.2204</span>, <span class="number">0.5239</span>,  ..., <span class="number">0.4333</span>, <span class="number">0.1906</span>, <span class="number">0.1318</span>],</span><br><span class="line">        [<span class="number">0.9850</span>, <span class="number">0.2121</span>, <span class="number">0.8405</span>,  ..., <span class="number">0.3796</span>, <span class="number">0.2717</span>, <span class="number">0.5553</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"><span class="meta">... </span>  <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line"><span class="meta">... </span>  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="comment"># 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例例时还可以指定其他函数</span></span><br><span class="line"><span class="meta">... </span>    <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br><span class="line"><span class="meta">... </span>    self.hidden = nn.Linear(<span class="number">784</span>, <span class="number">256</span>)</span><br><span class="line"><span class="meta">... </span>    self.act = nn.ReLU()</span><br><span class="line"><span class="meta">... </span>    self.output = nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line"><span class="meta">... </span>    </span><br><span class="line"><span class="meta">... </span>   <span class="comment"># 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出</span></span><br><span class="line"><span class="meta">... </span>  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line"><span class="meta">... </span>    o = self.act(self.hidden(x))</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> self.output(o)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = MLP()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net</span><br><span class="line">MLP(</span><br><span class="line">  (hidden): Linear(in_features=<span class="number">784</span>, out_features=<span class="number">256</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (act): ReLU()</span><br><span class="line">  (output): Linear(in_features=<span class="number">256</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net(X)</span><br><span class="line">tensor([[ <span class="number">0.1317</span>,  <span class="number">0.0702</span>,  <span class="number">0.1707</span>, -<span class="number">0.0081</span>, -<span class="number">0.2730</span>,  <span class="number">0.2837</span>,  <span class="number">0.0700</span>,  <span class="number">0.1718</span>,</span><br><span class="line">          <span class="number">0.0299</span>,  <span class="number">0.2082</span>],</span><br><span class="line">        [ <span class="number">0.1094</span>,  <span class="number">0.0936</span>,  <span class="number">0.2474</span>, -<span class="number">0.0139</span>, -<span class="number">0.1861</span>,  <span class="number">0.1846</span>,  <span class="number">0.1658</span>,  <span class="number">0.2051</span>,</span><br><span class="line">          <span class="number">0.2609</span>,  <span class="number">0.2227</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br></pre></td></tr></table></figure>

<h3 id="神经网络中常见的层"><a href="#神经网络中常见的层" class="headerlink" title="神经网络中常见的层"></a>神经网络中常见的层</h3><h4 id="不含模型参数的层"><a href="#不含模型参数的层" class="headerlink" title="不含模型参数的层"></a>不含模型参数的层</h4><p>下⾯构造的 MyLayer 类通过继承 Module 类自定义了一个<strong>将输入减掉均值后输出</strong>的层。这个层里不含模型参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">MyLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line"><span class="meta">... </span>        <span class="built_in">super</span>(MyLayer, self).__init__(**kwargs)</span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line"><span class="meta">... </span>        <span class="keyword">return</span> x - x.mean()  </span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>layer = MyLayer()  <span class="comment"># 实例化该层</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>layer</span><br><span class="line">MyLayer()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>layer(torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.<span class="built_in">float</span>))</span><br><span class="line">tensor([-<span class="number">2.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>])</span><br></pre></td></tr></table></figure>

<h4 id="含模型参数的层"><a href="#含模型参数的层" class="headerlink" title="含模型参数的层"></a>含模型参数的层</h4><p>我们还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。</p>
<p>Parameter 类其实是 Tensor 的子类，如果一 个 Tensor 是 Parameter ，那么它会⾃动被添加到模型的参数列表里。所以在⾃定义含模型参数的层时，我们应该将参数定义成 Parameter ，除了直接定义成 Parameter 类外，还可以使⽤ ParameterList 和 ParameterDict 分别定义参数的列表和字典。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyListDense</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyListDense, self).__init__()</span><br><span class="line">        self.params = nn.ParameterList([nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)])</span><br><span class="line">        self.params.append(nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.params)):</span><br><span class="line">            x = torch.mm(x, self.params[i])</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">      </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = MyListDense()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(net)</span><br><span class="line">MyListDense(</span><br><span class="line">  (params): ParameterList(</span><br><span class="line">      (<span class="number">0</span>): Parameter containing: [torch.FloatTensor of size 4x4]</span><br><span class="line">      (<span class="number">1</span>): Parameter containing: [torch.FloatTensor of size 4x4]</span><br><span class="line">      (<span class="number">2</span>): Parameter containing: [torch.FloatTensor of size 4x4]</span><br><span class="line">      (<span class="number">3</span>): Parameter containing: [torch.FloatTensor of size 4x1]</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDictDense</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyDictDense, self).__init__()</span><br><span class="line">        self.params = nn.ParameterDict(&#123;</span><br><span class="line">                <span class="string">&#x27;linear1&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)),</span><br><span class="line">                <span class="string">&#x27;linear2&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        self.params.update(&#123;<span class="string">&#x27;linear3&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">2</span>))&#125;) <span class="comment"># 新增</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, choice=<span class="string">&#x27;linear1&#x27;</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.mm(x, self.params[choice])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = MyDictDense()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(net)</span><br><span class="line">MyDictDense(</span><br><span class="line">  (params): ParameterDict(</span><br><span class="line">      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]</span><br><span class="line">      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]</span><br><span class="line">      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>下面给出常见的神经网络的一些层，比如卷积层、池化层，以及较为基础的AlexNet，LeNet等。</p>
<h4 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h4><p>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积运算（二维互相关）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span>(<span class="params">X, K</span>):</span> </span><br><span class="line">    h, w = K.shape</span><br><span class="line">    X, K = X.<span class="built_in">float</span>(), K.<span class="built_in">float</span>()</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i: i + h, j: j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维卷积层</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, kernel_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Conv2D, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure>

<p>填充(padding)是指在输⼊入⾼高和宽的两侧填充元素(通常是0元素)。</p>
<p>在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下 的顺序，依次在输⼊数组上滑动。我们将每次滑动的行数和列数称为步幅(stride)。</p>
<p>（skip）</p>
<h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>池化层每次对输入数据的一个固定形状窗口(⼜称池化窗口)中的元素计算输出。不同于卷积层里计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也 分别叫做最大池化或平均池化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span></span><br><span class="line"><span class="meta">... </span>    p_h, p_w = pool_size</span><br><span class="line"><span class="meta">... </span>    Y = np.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line"><span class="meta">... </span>        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="meta">... </span>            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line"><span class="meta">... </span>                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line"><span class="meta">... </span>            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line"><span class="meta">... </span>                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> Y</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pool2d(X, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">array([[<span class="number">4.</span>, <span class="number">5.</span>],</span><br><span class="line">       [<span class="number">7.</span>, <span class="number">8.</span>]])</span><br></pre></td></tr></table></figure>

<h4 id="模型示例：LeNet"><a href="#模型示例：LeNet" class="headerlink" title="模型示例：LeNet"></a>模型示例：LeNet</h4><p>（待补充）</p>
<h4 id="模型示例：AlexNet"><a href="#模型示例：AlexNet" class="headerlink" title="模型示例：AlexNet"></a>模型示例：AlexNet</h4><p>（待补充）</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>一个好的训练离不开优质的负反馈，这里的损失函数就是模型的负反馈。</p>
<p>这里将列出PyTorch中常用的损失函数（一般通过torch.nn调用），并详细介绍每个损失函数的功能介绍、数学公式和调用代码。</p>
<h3 id="二分类交叉熵损失函数"><a href="#二分类交叉熵损失函数" class="headerlink" title="二分类交叉熵损失函数"></a>二分类交叉熵损失函数</h3><p><code>torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></p>
<p><strong>功能</strong>：计算二分类任务时的交叉熵（Cross Entropy）函数。在二分类中，label是{0,1}。对于进入交叉熵函数的input为概率分布的形式。一般来说，input为sigmoid激活层的输出，或者softmax的输出。</p>
<p><strong>主要参数</strong>：</p>
<ul>
<li><code>weight</code>:每个类别的loss设置权值</li>
<li><code>size_average</code>:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和.</li>
<li><code>reduce</code>:数据类型为bool，为True时，loss的返回是标量。</li>
</ul>
<h3 id="其他损失函数"><a href="#其他损失函数" class="headerlink" title="其他损失函数"></a>其他损失函数</h3><p>交叉熵损失函数</p>
<p>L1损失函数</p>
<p>MSE损失函数</p>
<p>平滑L1 (Smooth L1)损失函数</p>
<p>目标泊松分布的负对数似然损失</p>
<p>KL散度</p>
<h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><h3 id="什么是优化器"><a href="#什么是优化器" class="headerlink" title="什么是优化器"></a>什么是优化器</h3><p>深度学习的目标是通过不断改变网络参数，使得参数能够对输入做各种非线性变换拟合输出，本质上就是一个函数去寻找最优解，只不过这个最优解使一个矩阵。那么我们如何计算出来这么多的系数，有以下两种方法：</p>
<ol>
<li>第一种是最直接的暴力穷举一遍参数，这种方法的实施可能性基本为0，堪比愚公移山plus的难度。</li>
<li>为了使求解参数过程更加快，人们提出了第二种办法，即就是是BP+优化器逼近求解。</li>
</ol>
<p>因此，优化器就是根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值，使得模型输出更加接近真实标签。</p>
<h3 id="PyTorch提供的优化器"><a href="#PyTorch提供的优化器" class="headerlink" title="PyTorch提供的优化器"></a>PyTorch提供的优化器</h3><p>Pytorch很人性化的给我们提供了一个优化器的库torch.optim，在这里面给我们提供了十种优化器。</p>
<ul>
<li>torch.optim.ASGD</li>
<li>torch.optim.Adadelta</li>
<li>torch.optim.Adagrad</li>
<li>torch.optim.Adam</li>
<li>torch.optim.AdamW</li>
<li>torch.optim.Adamax</li>
<li>torch.optim.LBFGS</li>
<li>torch.optim.RMSprop</li>
<li>torch.optim.Rprop</li>
<li>torch.optim.SGD</li>
<li>torch.optim.SparseAdam</li>
</ul>
<h2 id="训练与评估"><a href="#训练与评估" class="headerlink" title="训练与评估"></a>训练与评估</h2><p>完成了上述设定后就可以加载数据开始训练模型了。首先应该设置模型的状态：如果是训练状态，那么模型的参数应该支持反向传播的修改；如果是验证/测试状态，则不应该修改模型参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.train()   <span class="comment"># 训练状态</span></span><br><span class="line">model.<span class="built_in">eval</span>()   <span class="comment"># 验证/测试状态</span></span><br></pre></td></tr></table></figure>

<p>训练过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_loader:  <span class="comment"># 此时要用for循环读取DataLoader中的全部数据。</span></span><br><span class="line">        data, label = data.cuda(), label.cuda()  <span class="comment"># 之后将数据放到GPU上用于后续计算，此处以.cuda()为例</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 开始用当前批次数据做训练时，应当先将优化器的梯度置零</span></span><br><span class="line">        output = model(data)  <span class="comment"># 之后将data送入模型中训练</span></span><br><span class="line">        loss = criterion(label, output)   <span class="comment"># 根据预先定义的criterion计算损失函数</span></span><br><span class="line">        loss.backward()  <span class="comment"># 将loss反向传播回网络</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 使用优化器更新模型参数</span></span><br><span class="line">        train_loss += loss.item()*data.size(<span class="number">0</span>)</span><br><span class="line">    train_loss = train_loss/<span class="built_in">len</span>(train_loader.dataset)</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125; \tTraining Loss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, train_loss))</span><br></pre></td></tr></table></figure>

<p>验证/测试的流程基本与训练过程一致，不同点在于：</p>
<ul>
<li>需要预先设置torch.no_grad，以及将model调至eval模式</li>
<li>不需要将优化器的梯度置零</li>
<li>不需要将loss反向回传到网络</li>
<li>不需要更新optimizer</li>
</ul>
<p>验证/测试过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val</span>(<span class="params">epoch</span>):</span>       </span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    val_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> val_loader:</span><br><span class="line">            data, label = data.cuda(), label.cuda()</span><br><span class="line">            output = model(data)</span><br><span class="line">            preds = torch.argmax(output, <span class="number">1</span>)</span><br><span class="line">            loss = criterion(output, label)</span><br><span class="line">            val_loss += loss.item()*data.size(<span class="number">0</span>)</span><br><span class="line">            running_accu += torch.<span class="built_in">sum</span>(preds == label.data)</span><br><span class="line">    val_loss = val_loss/<span class="built_in">len</span>(val_loader.dataset)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125; \tTraining Loss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, val_loss))</span><br></pre></td></tr></table></figure>

<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>在PyTorch深度学习中，可视化是一个可选项，指的是某些任务在训练完成后，需要对一些必要的内容进行可视化，比如分类的ROC曲线，卷积网络中的卷积核，以及训练/验证过程的损失函数曲线等等。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li>Datawhale开源项目：深入浅出PyTorch <a target="_blank" rel="noopener" href="https://github.com/datawhalechina/thorough-pytorch/">https://github.com/datawhalechina/thorough-pytorch/</a></li>
<li>李宏毅机器学习2021春-PyTorch Tutorial <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Wv411h7kN?p=5">https://www.bilibili.com/video/BV1Wv411h7kN?p=5</a></li>
<li>动手学深度学习pytorch版 <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_preface/index.html">https://zh-v2.d2l.ai/chapter_preface/index.html</a></li>
<li>PyTorch官方教程中文版 <a target="_blank" rel="noopener" href="https://pytorch123.com/SecondSection/training_a_classifier/">https://pytorch123.com/SecondSection/training_a_classifier/</a></li>
</ul>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/nlp-transformer-summary/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Newer</strong>
            <div class="article-nav-title">
                
                    Summary Transformer课程总结
                
            </div>
        </a>
    
    
        <a href="/nlp-transformer-task07/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Older</strong>
            <div class="article-nav-title">Task07 使用Transformers解决文本分类任务</div>
        </a>
    
</nav>





    
    




<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            chuxiaoyu &copy; 2022 
            <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>
            <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a target="_blank" rel="noopener" href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a>
            
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
            
        </div>
    </div>
</footer>

        

    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


    </div>
</body>
</html>