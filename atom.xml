<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Memex</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-09-25T08:56:01.901Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Xiaoyu CHU</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>task06 BERT应用到下游任务、训练和优化</title>
    <link href="http://example.com/nlp-transformer-task06/"/>
    <id>http://example.com/nlp-transformer-task06/</id>
    <published>2021-09-24T07:18:42.000Z</published>
    <updated>2021-09-25T08:56:01.901Z</updated>
    
    <content type="html"><![CDATA[<p><em>该部分的内容翻译自🤗HuggingFace官网教程第1部分（1-4章），见 <a href="https://huggingface.co/course/chapter1">https://huggingface.co/course/chapter1</a>。该系列教程由3大部分共12章组成（如图），其中第1部分介绍transformers库的主要概念、模型的工作原理和使用方法、怎样在特定数据集上微调等内容。</em><br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_1.png?raw=true" width="500" alt="" align="center" /></p><h1 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h1><p>简单的说，有两种可以跑模型代码的方式：</p><ol><li>Google Colab</li><li>本地虚拟环境 <code>pip install transformers</code></li></ol><p>详见 <a href="https://huggingface.co/course/chapter0?fw=pt">https://huggingface.co/course/chapter0?fw=pt</a></p><h1 id="Transformer模型概述"><a href="#Transformer模型概述" class="headerlink" title="Transformer模型概述"></a>Transformer模型概述</h1><h2 id="Transformers-可以做什么？"><a href="#Transformers-可以做什么？" class="headerlink" title="Transformers, 可以做什么？"></a>Transformers, 可以做什么？</h2><p>目前可用的一些pipeline是：</p><ul><li>feature-extraction 获取文本的向量表示</li><li>fill-mask 完形填空</li><li>ner (named entity recognition) 命名实体识别</li><li>question-answering 问答</li><li>sentiment-analysis 情感分析</li><li>summarization 摘要生成</li><li>text-generation 文本生成</li><li>translation 翻译</li><li>zero-shot-classification 零样本分类</li></ul><p><em>pipeline: 直译管道/流水线，可以理解为流程。</em></p><h2 id="Transformers-如何工作？"><a href="#Transformers-如何工作？" class="headerlink" title="Transformers, 如何工作？"></a>Transformers, 如何工作？</h2><h3 id="Transformer简史"><a href="#Transformer简史" class="headerlink" title="Transformer简史"></a>Transformer简史</h3><p>Transformer 架构于 2017 年 6 月推出。原始研究的重点是翻译任务。随后推出了几个有影响力的模型，包括：</p><ul><li>2018 年 6 月：GPT，第一个预训练的 Transformer 模型，用于各种 NLP 任务的微调并获得最先进的结果</li><li>2018 年 10 月：BERT，另一个大型预训练模型，该模型旨在生成更好的句子摘要</li><li>2019 年 2 月：GPT-2，GPT 的改进（和更大）版本</li><li>2019 年 10 月：DistilBERT，BERT 的蒸馏版本，速度提高 60%，内存减轻 40%，但仍保留 BERT 97% 的性能</li><li>2019 年 10 月：BART 和 T5，两个使用与原始 Transformer 模型相同架构的大型预训练模型（第一个这样做）</li><li>2020 年 5 月，GPT-3，GPT-2 的更大版本，无需微调即可在各种任务上表现良好（称为零样本学习zero-shot learning）</li></ul><p>大体上，它们可以分为三类：</p><ul><li>GPT类（又称为自回归 Transformer 模型）：只使用transformer-decoder部分</li><li>BERT类（又称为自编码 Transformer 模型）：只使用transformer-encoder部分</li><li>BART/T5类（又称为序列到序列 Transformer 模型）：使用Transformer-encoder-decoder部分</li></ul><p>它们的分类、具体模型、主要应用任务如下：<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_2.jpg?raw=true" width="800" alt="" align="center" /></p><p>其他需要知道的：</p><ul><li>Transformers是语言模型</li><li>Transformers是大模型</li><li>Transformers的应用通过预训练和微调两个过程</li></ul><h3 id="名词解释：Architecture和Checkpoints"><a href="#名词解释：Architecture和Checkpoints" class="headerlink" title="名词解释：Architecture和Checkpoints"></a>名词解释：Architecture和Checkpoints</h3><p><strong>Architecture/架构</strong>：定义了模型的基本结构和基本运算。<br><strong>Checkpoints/检查点</strong>：模型的某个训练状态，加载此checkpoint会加载此时的权重。训练时可以选择自动保存checkpoint。模型在训练时可以设置自动保存于某个时间点（比如模型训练了一轮epoch，更新了参数，将这个状态的模型保存下来，为一个checkpoint。） 所以每个checkpoint对应模型的一个状态，一组权重。</p><h1 id="使用Transformers"><a href="#使用Transformers" class="headerlink" title="使用Transformers"></a>使用Transformers</h1><h2 id="3个处理步骤"><a href="#3个处理步骤" class="headerlink" title="3个处理步骤"></a>3个处理步骤</h2><p>将一些文本传递到pipeline时涉及3个主要步骤：</p><ol><li>文本被预处理为模型可以理解的格式。</li><li>预处理后的输入传递给模型。</li><li>模型的预测结果被后处理为人类可以理解的格式。</li></ol><p>Pipeline将3个步骤组合在一起：预处理/Tokenizer、通过模型传递输入/Model和后处理/Post-Processing：<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_3.png?raw=true" width="800" alt="" align="center" /></p><h2 id="Tokenizer-预处理"><a href="#Tokenizer-预处理" class="headerlink" title="Tokenizer/预处理"></a>Tokenizer/预处理</h2><p>Tokenizer的作用：</p><ul><li>将输入拆分为称为token的单词、子词/subword或符号/symbols（如标点符号）</li><li>将每个token映射到一个整数</li><li>添加可能对模型有用的其他输入</li></ul><h2 id="Going-Through-Models-穿过模型"><a href="#Going-Through-Models-穿过模型" class="headerlink" title="Going Through Models/穿过模型"></a>Going Through Models/穿过模型</h2><h3 id="模型实例化"><a href="#模型实例化" class="headerlink" title="模型实例化"></a>模型实例化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span><br><span class="line">model = AutoModel.from_pretrained(checkpoint)</span><br></pre></td></tr></table></figure><p>在这段代码中，我们下载了在pipeline中使用的相同检查点（实际上已经缓存）并将模型实例化。</p><h3 id="模型的输出：高维向量"><a href="#模型的输出：高维向量" class="headerlink" title="模型的输出：高维向量"></a>模型的输出：高维向量</h3><p>模型的输出向量通常有三个维度：</p><ul><li>Batch size: 一次处理的序列数</li><li>Sequence length: 序列向量的长度</li><li>Hidden size: 每个模型输入处理后的向量维度（hidden state vector）</li></ul><h3 id="Model-Heads：为了处理不同的任务"><a href="#Model-Heads：为了处理不同的任务" class="headerlink" title="Model Heads：为了处理不同的任务"></a>Model Heads：为了处理不同的任务</h3><p>Model heads:将隐藏状态的高维向量作为输入，并将它们投影到不同的维度上。它们通常由一个或几个线性层组成。<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_4.png?raw=true" width="800" alt="这个图表示了Pipeline第二步在经过模型时发生的事情。" align="center" /><br>如上图所示，紫色代表向量，粉色代表模组，Embeddings+layers表示Transformer的架构，经过这层架构后的输出送入Model Head进行处理，从而应用到不同的下游任务。<br>🤗 Transformers 中有许多不同的Head架构可用，每一种架构都围绕着处理特定任务而设计。 下面列举了部分Model heads：</p><ul><li>*Model (retrieve the hidden states)</li><li>*ForCausalLM</li><li>*ForMaskedLM</li><li>*ForMultipleChoice</li><li>*ForQuestionAnswering</li><li>*ForSequenceClassification</li><li>*ForTokenClassification</li><li>and others 🤗</li></ul><h2 id="Post-processing-后处理"><a href="#Post-processing-后处理" class="headerlink" title="Post-processing/后处理"></a>Post-processing/后处理</h2><p>从模型中获得的作为输出的值本身并不一定有意义。要转换为概率，它们需要经过一个 SoftMax 层。</p><h1 id="微调一个预训练模型"><a href="#微调一个预训练模型" class="headerlink" title="微调一个预训练模型"></a>微调一个预训练模型</h1><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>在本节中，我们将使用MRPC（Microsoft Research Praphrase Corpus）数据集作为示例。该DataSet由5,801对句子组成，标签指示它们是否是同义句（即两个句子是否表示相同的意思）。 我们选择它是因为它是一个小型数据集，因此可以轻松训练。</p><h3 id="从Hub上加载数据集"><a href="#从Hub上加载数据集" class="headerlink" title="从Hub上加载数据集"></a>从Hub上加载数据集</h3><p>Hub不仅包含模型，还含有多种语言的datasets。<br>例如，MRPC数据集是构成 GLUE benchmark的 10 个数据集之一。GLUE（General Language Understanding Evaluation）是一个多任务的自然语言理解基准和分析平台。GLUE包含九项NLU任务，语言均为英语。GLUE九项任务涉及到自然语言推断、文本蕴含、情感分析、语义相似等多个任务。像BERT、XLNet、RoBERTa、ERINE、T5等知名模型都会在此基准上进行测试。</p><p>🤗 Datasets库提供了一个非常简单的命令来下载和缓存Hub上的dataset。 我们可以像这样下载 MRPC 数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>raw_datasets</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence1&#x27;</span>, <span class="string">&#x27;sentence2&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">3668</span></span><br><span class="line">    &#125;)</span><br><span class="line">    validation: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence1&#x27;</span>, <span class="string">&#x27;sentence2&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">408</span></span><br><span class="line">    &#125;)</span><br><span class="line">    test: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence1&#x27;</span>, <span class="string">&#x27;sentence2&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">1725</span></span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>这样就得到一个DatasetDict对象，包含训练集、验证集和测试集，训练集中有3,668 个句子对，验证集中有408对，测试集中有1,725 对。每个句子对包含四个字段：’sentence1’, ‘sentence2’, ‘label’和 ‘idx’。</p><p>我们可以通过索引访问raw_datasets 的句子对：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>raw_train_dataset = raw_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>raw_train_dataset[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;sentence1&#x27;</span>: <span class="string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>, </span><br><span class="line"><span class="string">&#x27;sentence2&#x27;</span>: <span class="string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>, </span><br><span class="line"><span class="string">&#x27;label&#x27;</span>: <span class="number">1</span>, </span><br><span class="line"><span class="string">&#x27;idx&#x27;</span>: <span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure><p>我们可以通过features获得数据集的字段类型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>raw_train_dataset.features</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="string">&#x27;string&#x27;</span>, <span class="built_in">id</span>=<span class="literal">None</span>), </span><br><span class="line"><span class="string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="string">&#x27;string&#x27;</span>, <span class="built_in">id</span>=<span class="literal">None</span>), </span><br><span class="line"><span class="string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="number">2</span>, names=[<span class="string">&#x27;not_equivalent&#x27;</span>, <span class="string">&#x27;equivalent&#x27;</span>], names_file=<span class="literal">None</span>, <span class="built_in">id</span>=<span class="literal">None</span>), </span><br><span class="line"><span class="string">&#x27;idx&#x27;</span>: Value(dtype=<span class="string">&#x27;int32&#x27;</span>, <span class="built_in">id</span>=<span class="literal">None</span>)&#125;</span><br></pre></td></tr></table></figure><blockquote><p>TIPS：</p><ol><li>没有数据集的话首先安装一下：<code>pip install datasets</code></li><li>这里很容易出现连接错误，解决方法如下：<a href="https://blog.csdn.net/qq_20849045/article/details/117462846?utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.no_search_link">https://blog.csdn.net/qq_20849045/article/details/117462846?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link</a></li></ol></blockquote><h3 id="数据集预处理"><a href="#数据集预处理" class="headerlink" title="数据集预处理"></a>数据集预处理</h3><p>通过数据集预处理，我们将文本转换成模型能理解的向量。这个过程通过Tokenizer实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenized_sentences_1 = tokenizer(raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="string">&quot;sentence1&quot;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenized_sentences_2 = tokenizer(raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="string">&quot;sentence2&quot;</span>])</span><br></pre></td></tr></table></figure><p>（TODO）</p><h2 id="使用Trainer-API微调一个模型"><a href="#使用Trainer-API微调一个模型" class="headerlink" title="使用Trainer API微调一个模型"></a>使用Trainer API微调一个模型</h2><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><h3 id="评估函数"><a href="#评估函数" class="headerlink" title="评估函数"></a>评估函数</h3><h1 id="补充部分"><a href="#补充部分" class="headerlink" title="补充部分"></a>补充部分</h1><h2 id="为什么4中用Trainer来微调模型？"><a href="#为什么4中用Trainer来微调模型？" class="headerlink" title="为什么4中用Trainer来微调模型？"></a>为什么4中用Trainer来微调模型？</h2><h2 id="Training-Arguments主要参数"><a href="#Training-Arguments主要参数" class="headerlink" title="Training Arguments主要参数"></a>Training Arguments主要参数</h2><h2 id="不同模型的加载方式"><a href="#不同模型的加载方式" class="headerlink" title="不同模型的加载方式"></a>不同模型的加载方式</h2><h2 id="Dynamic-Padding——动态填充技术"><a href="#Dynamic-Padding——动态填充技术" class="headerlink" title="Dynamic Padding——动态填充技术"></a>Dynamic Padding——动态填充技术</h2><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>基于transformers的自然语言处理(NLP)入门–在线阅读 <a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/">https://datawhalechina.github.io/learn-nlp-with-transformers/#/</a></li><li>Huggingface官方教程 <a href="https://huggingface.co/course/chapter1">https://huggingface.co/course/chapter1</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;该部分的内容翻译自🤗HuggingFace官网教程第1部分（1-4章），见 &lt;a href=&quot;https://huggingface.co/course/chapter1&quot;&gt;https://huggingface.co/course/chapter1&lt;/a&gt;。该系</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="第29期 基于transformer的NLP" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="预训练模型" scheme="http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="BERT" scheme="http://example.com/tags/BERT/"/>
    
    <category term="transfomer" scheme="http://example.com/tags/transfomer/"/>
    
  </entry>
  
  <entry>
    <title>task05 编写BERT模型</title>
    <link href="http://example.com/nlp-transformer-task05/"/>
    <id>http://example.com/nlp-transformer-task05/</id>
    <published>2021-09-20T19:01:35.000Z</published>
    <updated>2021-09-22T07:03:54.456Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>本部分是BERT源码的解读，来自HuggingFace/transfomers/BERT[1]。<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/bert_1.png?raw=true" width="600" alt="" align="center" /></p><p><img src="/Users/chuxiaoyu/Blog/blog_image/nlp/bert_1.png"></p><p>如图所示，代码结构和作用如下：</p><ul><li>BertTokenizer 预处理和切词</li><li>BertModel<ul><li>BertEmbeddings 词嵌入</li><li>BertEncoder<ul><li>BertAttention 注意力机制</li><li>BertIntermediate 全连接和激活函数</li><li>BertOutput 全连接、残差链接和正则化</li></ul></li><li>BertPooler 取出[CLS]对应的向量，然后通过全连接层和激活函数后输出结果</li></ul></li></ul><h1 id="BERT的实现"><a href="#BERT的实现" class="headerlink" title="BERT的实现"></a>BERT的实现</h1><h2 id="BertConfig"><a href="#BertConfig" class="headerlink" title="BertConfig"></a>BertConfig</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">classtransformers.BertConfig(vocab_size=<span class="number">30522</span>, hidden_size=<span class="number">768</span>, </span><br><span class="line">    num_hidden_layers=<span class="number">12</span>, num_attention_heads=<span class="number">12</span>, intermediate_size=<span class="number">3072</span>, </span><br><span class="line">    hidden_act=<span class="string">&#x27;gelu&#x27;</span>, hidden_dropout_prob=<span class="number">0.1</span>, attention_probs_dropout_prob=<span class="number">0.1</span></span><br><span class="line">    , max_position_embeddings=<span class="number">512</span>, type_vocab_size=<span class="number">2</span>, initializer_range=<span class="number">0.02</span>, </span><br><span class="line">    layer_norm_eps=<span class="number">1e-12</span>, pad_token_id=<span class="number">0</span>, gradient_checkpointing=<span class="literal">False</span>, </span><br><span class="line">    position_embedding_type=<span class="string">&#x27;absolute&#x27;</span>, use_cache=<span class="literal">True</span>, classifier_dropout=<span class="literal">None</span>,</span><br><span class="line">     **kwargs)</span><br></pre></td></tr></table></figure><p>这是存储BertModel（Torch.nn.Module的子类）或TFBertModel（tf.keras.Model的子类）配置的配置类。它用于根据指定的参数来实例化BERT模型，定义模型架构。</p><p>配置对象从PretrainedConfig继承，可用于控制模型输出。</p><p>参数：</p><ul><li>vocab_size: BERT模型的词汇量，定义了能被inputs_ids表示的token数量。</li><li>hidden_size: </li></ul><h2 id="BertTokenizer"><a href="#BertTokenizer" class="headerlink" title="BertTokenizer"></a>BertTokenizer</h2><h2 id="BertModel"><a href="#BertModel" class="headerlink" title="BertModel"></a>BertModel</h2><h1 id="BERT的应用"><a href="#BERT的应用" class="headerlink" title="BERT的应用"></a>BERT的应用</h1><h2 id="BertForPreTraining"><a href="#BertForPreTraining" class="headerlink" title="BertForPreTraining"></a>BertForPreTraining</h2><h2 id="BertForNextSentencePrediction"><a href="#BertForNextSentencePrediction" class="headerlink" title="BertForNextSentencePrediction"></a>BertForNextSentencePrediction</h2><h2 id="BertForSequenceClassification"><a href="#BertForSequenceClassification" class="headerlink" title="BertForSequenceClassification"></a>BertForSequenceClassification</h2><h2 id="BertForMultipleChoice"><a href="#BertForMultipleChoice" class="headerlink" title="BertForMultipleChoice"></a>BertForMultipleChoice</h2><h2 id="BertForTokenClassification"><a href="#BertForTokenClassification" class="headerlink" title="BertForTokenClassification"></a>BertForTokenClassification</h2><h2 id="BertForQuestionAnswering"><a href="#BertForQuestionAnswering" class="headerlink" title="BertForQuestionAnswering"></a>BertForQuestionAnswering</h2><h1 id="BERT的训练和优化"><a href="#BERT的训练和优化" class="headerlink" title="BERT的训练和优化"></a>BERT的训练和优化</h1><h2 id="Pre-Training"><a href="#Pre-Training" class="headerlink" title="Pre-Training"></a>Pre-Training</h2><h2 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine-Tuning"></a>Fine-Tuning</h2><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>HuggingFace/transfomers/BERT <a href="https://huggingface.co/transformers/model_doc/bert.html#">https://huggingface.co/transformers/model_doc/bert.html#</a></li><li>基于transformers的自然语言处理(NLP)入门–在线阅读 <a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/">https://datawhalechina.github.io/learn-nlp-with-transformers/#/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h1&gt;&lt;p&gt;本部分是BERT源码的解读，来自HuggingFace/transfomers/BERT[1</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="第29期 基于transformer的NLP" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="预训练模型" scheme="http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="BERT" scheme="http://example.com/tags/BERT/"/>
    
    <category term="transfomer" scheme="http://example.com/tags/transfomer/"/>
    
  </entry>
  
  <entry>
    <title>task04 学习GPT</title>
    <link href="http://example.com/nlp-transformer-task04/"/>
    <id>http://example.com/nlp-transformer-task04/</id>
    <published>2021-09-19T12:02:17.000Z</published>
    <updated>2021-09-20T19:08:17.312Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从语言模型说起"><a href="#从语言模型说起" class="headerlink" title="从语言模型说起"></a>从语言模型说起</h1><h2 id="自编码语言模型（auto-encoder）"><a href="#自编码语言模型（auto-encoder）" class="headerlink" title="自编码语言模型（auto-encoder）"></a>自编码语言模型（auto-encoder）</h2><p>自编码语言模型通过随机Mask输入的部分单词，然后预训练的目标是预测被Mask的单词，不仅可以融入上文信息，还可以自然的融入下文信息。ex. BERT.</p><ul><li>优点：自然地融入双向语言模型，同时看到被预测单词的上文和下文</li><li>缺点：训练和预测不一致。训练的时候输入引入了[Mask]标记，但是在预测阶段往往没有这个[Mask]标记，导致预训练阶段和Fine-tuning阶段不一致。</li></ul><h2 id="自回归语言模型（auto-regressive）"><a href="#自回归语言模型（auto-regressive）" class="headerlink" title="自回归语言模型（auto-regressive）"></a>自回归语言模型（auto-regressive）</h2><p>语言模型根据输入句子的一部分文本来预测下一个词。ex. GPT-2</p><ul><li>优点：对于生成类的NLP任务，比如文本摘要，机器翻译等，从左向右的生成内容，天然和自回归语言模型契合。</li><li>缺点：由于一般是从左到右（当然也可能从右到左），所以只能利用上文或者下文的信息，不能同时利用上文和下文的信息。</li></ul><h1 id="Transformer-BERT-GPT-2的关系"><a href="#Transformer-BERT-GPT-2的关系" class="headerlink" title="Transformer, BERT, GPT-2的关系"></a>Transformer, BERT, GPT-2的关系</h1><p>Transformer的Encoder进化成了BERT，Decoder进化成了GPT2。</p><p>如果要使用Transformer来解决语言模型任务，并不需要完整的Encoder部分和Decoder部分，于是在原始Transformer之后的许多研究工作中，人们尝试只使用Transformer Encoder或者Decoder进行预训练。比如BERT只使用了Encoder部分进行masked language model（自编码）训练，GPT-2便是只使用了Decoder部分进行自回归（auto regressive）语言模型训练。</p><h1 id="GPT-2概述"><a href="#GPT-2概述" class="headerlink" title="GPT-2概述"></a>GPT-2概述</h1><h2 id="模型的输入"><a href="#模型的输入" class="headerlink" title="模型的输入"></a>模型的输入</h2><p>输入的处理分为两步：token embedding + position encoding。即:</p><ol><li>在嵌入矩阵中查找输入的单词的对应的embedding向量</li><li>融入位置编码</li></ol><h2 id="Decoder层"><a href="#Decoder层" class="headerlink" title="Decoder层"></a>Decoder层</h2><p>每一层decoder的组成：Masked Self-Attention + Feed Forward Neural Network</p><p>Self-Attention所做的事情是：它通过对句子片段中每个词的相关性打分，并将这些词的表示向量根据相关性加权求和，从而让模型能够将词和其他相关词向量的信息融合起来。</p><p>Masked Self-Attention做的是：将mask位置对应的的attention score变成一个非常小的数字或者0，让其他单词再self attention的时候（加权求和的时候）不考虑这些单词。</p><h2 id="模型的输出"><a href="#模型的输出" class="headerlink" title="模型的输出"></a>模型的输出</h2><p>当模型顶部的Decoder层产生输出向量时，模型会将这个向量乘以一个巨大的嵌入矩阵（vocab size x embedding size）来计算该向量和所有单词embedding向量的相关得分。这个相乘的结果，被解释为模型词汇表中每个词的分数，经过softmax之后被转换成概率。</p><p>我们可以选择最高分数的 token（top_k=1），也可以同时考虑其他词（top k）。假设每个位置输出k个token，假设总共输出n个token，那么基于n个单词的联合概率选择的输出序列会更好。</p><p>模型完成一次迭代，输出一个单词。模型会继续迭代，直到所有的单词都已经生成，或者直到输出了表示句子末尾的token。</p><h1 id="关于Self-Attention-Masked-Self-Attention"><a href="#关于Self-Attention-Masked-Self-Attention" class="headerlink" title="关于Self-Attention, Masked Self-Attention"></a>关于Self-Attention, Masked Self-Attention</h1><h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>Self-Attention 主要通过 3 个步骤来实现：</p><ol><li>为每个路径创建 Query、Key、Value 矩阵。</li><li>对于每个输入的token，使用它的Query向量为所有其他的Key向量进行打分。</li><li>将 Value 向量乘以它们对应的分数后求和。</li></ol><h2 id="Masked-Self-Attention"><a href="#Masked-Self-Attention" class="headerlink" title="Masked Self-Attention"></a>Masked Self-Attention</h2><p>在Self-Attention的第2步，把未来的 token 评分设置为0，因此模型不能看到未来的词。</p><p>这个屏蔽（masking）经常用一个矩阵来实现，称为 attention mask矩阵。<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/mask_1.jpg?raw=true" width="600" alt="" align="center" /><br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/mask_2.jpg?raw=true" width="600" alt="" align="center" /></p><h2 id="GPT-2中的Self-Attention"><a href="#GPT-2中的Self-Attention" class="headerlink" title="GPT-2中的Self-Attention"></a>GPT-2中的Self-Attention</h2><p>(skip)</p><h1 id="自回归语言模型的应用"><a href="#自回归语言模型的应用" class="headerlink" title="自回归语言模型的应用"></a>自回归语言模型的应用</h1><p>应用在下游并取得不错效果的NLP任务有：机器翻译、摘要生成、音乐生成。<em>（可见，主要是跟预训练任务相似的生成类任务。）</em></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>基于transformers的自然语言处理(NLP)入门–在线阅读 <a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/">https://datawhalechina.github.io/learn-nlp-with-transformers/#/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;从语言模型说起&quot;&gt;&lt;a href=&quot;#从语言模型说起&quot; class=&quot;headerlink&quot; title=&quot;从语言模型说起&quot;&gt;&lt;/a&gt;从语言模型说起&lt;/h1&gt;&lt;h2 id=&quot;自编码语言模型（auto-encoder）&quot;&gt;&lt;a href=&quot;#自编码语言模型（auto</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="第29期 基于transformer的NLP" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="预训练模型" scheme="http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="GPT" scheme="http://example.com/tags/GPT/"/>
    
  </entry>
  
  <entry>
    <title>chap01-02 PyTorch的简介和安装、PyTorch基础知识</title>
    <link href="http://example.com/pytorch-chap01-02/"/>
    <id>http://example.com/pytorch-chap01-02/</id>
    <published>2021-09-18T02:26:03.000Z</published>
    <updated>2021-09-21T17:58:33.143Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第一章-PyTorch的简介和安装"><a href="#第一章-PyTorch的简介和安装" class="headerlink" title="第一章 PyTorch的简介和安装"></a>第一章 PyTorch的简介和安装</h1><h2 id="PyTorch简介"><a href="#PyTorch简介" class="headerlink" title="PyTorch简介"></a>PyTorch简介</h2><p>PyTorch是由Facebook人工智能研究小组开发的一种基于Lua编写的Torch库的Python实现的深度学习库，目前被广泛应用于学术界和工业界，而随着Caffe2项目并入Pytorch， Pytorch开始影响到TensorFlow在深度学习应用框架领域的地位。总的来说，PyTorch是当前难得的简洁优雅且高效快速的框架。因此本课程我们选择了PyTorch来进行开源学习。</p><h2 id="PyTorch的安装"><a href="#PyTorch的安装" class="headerlink" title="PyTorch的安装"></a>PyTorch的安装</h2><p>PyTorch官网：<a href="https://pytorch.org/">https://pytorch.org/</a></p><h2 id="PyTorch的发展和优势"><a href="#PyTorch的发展和优势" class="headerlink" title="PyTorch的发展和优势"></a>PyTorch的发展和优势</h2><p>“All in Pytorch”.</p><h1 id="第二章-PyTorch的基础知识"><a href="#第二章-PyTorch的基础知识" class="headerlink" title="第二章 PyTorch的基础知识"></a>第二章 PyTorch的基础知识</h1><h2 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h2><p>张量是基于向量和矩阵的推广，比如我们可以将标量视为零阶张量，矢量可以视为一阶张量，矩阵就是二阶张量。</p><ul><li>0维张量/标量 标量是1个数字</li><li>1维张量/向量 1维张量称为“向量”</li><li>2维张量 2维张量称为“矩阵”</li><li>3维张量 时间序列数据、股价、文本数据、彩色图片(RGB)</li><li>4维=图像</li><li>5维=视频</li></ul><p>在PyTorch中， torch.Tensor 是存储和变换数据的主要工具。</p><h3 id="构造"><a href="#构造" class="headerlink" title="构造"></a>构造</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">4</span>, <span class="number">3</span>)  <span class="comment"># 构造张量</span></span><br><span class="line"><span class="built_in">print</span>(x.size())  <span class="comment"># 获取维度信息</span></span><br><span class="line"><span class="built_in">print</span>(x.shape)  <span class="comment"># 获取维度信息</span></span><br></pre></td></tr></table></figure><p>还有一些常见的构造Tensor的函数：<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/01_tensor_1.jpg?raw=true" width="600" alt="" align="center" /></p><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x[:, <span class="number">1</span>])  <span class="comment"># 取第2列</span></span><br><span class="line">y = torch.view(<span class="number">1</span>)  <span class="comment"># 改变维度信息</span></span><br></pre></td></tr></table></figure><p>PyTorch中的Tensor支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考官方文档。</p><h2 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h2><p>PyTorch中，所有神经网络的核心是autograd包。autograd包为张量上的所有操作提供了自动求导机制。</p><h2 id="并行计算简介"><a href="#并行计算简介" class="headerlink" title="并行计算简介"></a>并行计算简介</h2><h1 id="课程问题反馈"><a href="#课程问题反馈" class="headerlink" title="课程问题反馈"></a>课程问题反馈</h1><ul><li>教程中的链接最好在新标签页中打开</li><li></li></ul><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>Datawhale开源项目：深入浅出PyTorch <a href="https://github.com/datawhalechina/thorough-pytorch/">https://github.com/datawhalechina/thorough-pytorch/</a></li><li></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;第一章-PyTorch的简介和安装&quot;&gt;&lt;a href=&quot;#第一章-PyTorch的简介和安装&quot; class=&quot;headerlink&quot; title=&quot;第一章 PyTorch的简介和安装&quot;&gt;&lt;/a&gt;第一章 PyTorch的简介和安装&lt;/h1&gt;&lt;h2 id=&quot;PyTor</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="第30期 深入浅出PyTorch" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC30%E6%9C%9F-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAPyTorch/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="PyTorch" scheme="http://example.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>task03 学习BERT</title>
    <link href="http://example.com/nlp-transformer-task03/"/>
    <id>http://example.com/nlp-transformer-task03/</id>
    <published>2021-09-17T01:44:18.000Z</published>
    <updated>2021-09-20T19:08:38.783Z</updated>
    
    <content type="html"><![CDATA[<h1 id="BERT简介"><a href="#BERT简介" class="headerlink" title="BERT简介"></a>BERT简介</h1><p>BERT首先在大规模无监督语料上进行预训练，然后在预训练好的参数基础上增加一个与任务相关的神经网络层，并在该任务的数据上进行微调训，最终取得很好的效果。<strong>BERT的这个训练过程可以简述为：预训练（pre-train）+微调（fine-tune/fine-tuning），已经成为最近几年最流行的NLP解决方案的范式。</strong></p><h2 id="如何直接应用BERT"><a href="#如何直接应用BERT" class="headerlink" title="如何直接应用BERT"></a>如何直接应用BERT</h2><ol><li>下载在无监督语料上预训练好的BERT模型，一般来说对应了3个文件：BERT模型配置文件（用来确定Transformer的层数，隐藏层大小等），BERT模型参数，BERT词表（BERT所能处理的所有token）。</li><li>针对特定任务需要，在BERT模型上增加一个任务相关的神经网络，比如一个简单的分类器，然后在特定任务监督数据上进行微调训练。（微调的一种理解：学习率较小，训练epoch数量较少，对模型整体参数进行轻微调整）</li></ol><h2 id="BERT的结构"><a href="#BERT的结构" class="headerlink" title="BERT的结构"></a>BERT的结构</h2><p><strong>BERT模型结构基本上就是Transformer的encoder部分。</strong></p><h2 id="BERT的输入和输出"><a href="#BERT的输入和输出" class="headerlink" title="BERT的输入和输出"></a>BERT的输入和输出</h2><p>BERT模型输入有一点特殊的地方是在一句话最开始拼接了一个[CLS] token，如下图所示。这个特殊的[CLS] token经过BERT得到的向量表示通常被用作当前的句子表示。我们直接使用第1个位置的向量输出（对应的是[CLS]）传入classifier网络，然后进行分类任务。</p><h1 id="BERT的预训练任务"><a href="#BERT的预训练任务" class="headerlink" title="BERT的预训练任务"></a>BERT的预训练任务</h1><p>BERT是一个多任务模型，它的任务是由两个自监督任务组成。</p><h2 id="Masked-Language-Model（MLM）"><a href="#Masked-Language-Model（MLM）" class="headerlink" title="Masked Language Model（MLM）"></a>Masked Language Model（MLM）</h2><p>MLM：将输入文本序列的部分（15%）单词随机Mask掉，让BERT来预测这些被Mask的词语。<em>（可以说是完形填空）</em></p><blockquote><p>Masked Language Model（MLM）和核心思想取自Wilson Taylor在1953年发表的一篇论文《cloze procedure: A new tool for measuring readability》。所谓MLM是指在训练的时候随即从输入预料上mask掉一些单词，然后通过的上下文预测该单词，该任务非常像我们在中学时期经常做的完形填空。正如传统的语言模型算法和RNN匹配那样，MLM的这个性质和Transformer的结构是非常匹配的。</p></blockquote><h2 id="Next-Sentence-Prediction（NSP）"><a href="#Next-Sentence-Prediction（NSP）" class="headerlink" title="Next Sentence Prediction（NSP）"></a>Next Sentence Prediction（NSP）</h2><p>NSP：判断两个句子是否是相邻句子。即，输入是sentence A和sentence B，经过BERT编码之后，使用CLS token的向量表示来预测两个句子是否是相邻句子。</p><blockquote><p>Next Sentence Prediction（NSP）的任务是判断句子B是否是句子A的下文。如果是的话输出’IsNext‘，否则输出’NotNext‘。训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在[CLS]符号中。</p></blockquote><h1 id="BERT的应用"><a href="#BERT的应用" class="headerlink" title="BERT的应用"></a>BERT的应用</h1><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><p>由于BERT模型可以得到输入序列所对应的所有token的向量表示，因此不仅可以使用最后一程BERT的输出连接上任务网络进行微调，还可以直接使用这些token的向量当作特征。比如，可以直接提取每一层encoder的token表示当作特征，输入现有的特定任务神经网络中进行训练。</p><h2 id="Pretrain-Fine-tune"><a href="#Pretrain-Fine-tune" class="headerlink" title="Pretrain + Fine tune"></a>Pretrain + Fine tune</h2><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>基于transformers的自然语言处理(NLP)入门–在线阅读 <a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/">https://datawhalechina.github.io/learn-nlp-with-transformers/#/</a></li><li>李宏毅机器学习2019-ELMO,BERT,GPT <a href="https://www.bilibili.com/video/BV1Gb411n7dE?p=61">https:// www.bilibili.com/video/BV1Gb411n7dE?p=61</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;BERT简介&quot;&gt;&lt;a href=&quot;#BERT简介&quot; class=&quot;headerlink&quot; title=&quot;BERT简介&quot;&gt;&lt;/a&gt;BERT简介&lt;/h1&gt;&lt;p&gt;BERT首先在大规模无监督语料上进行预训练，然后在预训练好的参数基础上增加一个与任务相关的神经网络层，并在该</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="第29期 基于transformer的NLP" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="预训练模型" scheme="http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="BERT" scheme="http://example.com/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>task02 学习Attentioin和Transformer</title>
    <link href="http://example.com/nlp-transformer-task02/"/>
    <id>http://example.com/nlp-transformer-task02/</id>
    <published>2021-09-17T01:09:24.000Z</published>
    <updated>2021-09-20T19:08:48.847Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><h2 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h2><p>seq2seq是一种常见的NLP模型结构，全称是：sequence to sequence，翻译为“序列到序列”。顾名思义：从一个文本序列得到一个新的文本序列。典型的任务有：机器翻译任务，文本摘要任务。</p><p>seq2seq模型由编码器（encoder）和解码器（decoder）组成，编码器用来分析输入序列，解码器用来生成输出序列。编码器会处理输入序列中的每个元素，把这些信息转换成为一个背景向量（context vector）。当我们处理完整个输入序列后，编码器把背景向量发送给解码器，解码器通过背景向量中的信息，逐个元素输出新的序列。</p><p><strong>在transformer模型之前，seq2seq中的编码器和解码器一般采用循环神经网络（RNN）</strong>，虽然非常经典，但是局限性也非常大。最大的局限性就在于编码器和解码器之间的唯一联系就是一个固定长度的context向量。也就是说，编码器要将整个序列的信息压缩进一个固定长度的向量中。这样做存在两个弊端：</p><ul><li>语义向量可能无法完全表示整个序列的信息</li><li>先输入到网络的内容携带的信息会被后输入的信息覆盖掉，输入序列越长，这个现象就越严重</li></ul><h2 id="Attention-1"><a href="#Attention-1" class="headerlink" title="Attention"></a>Attention</h2><p>为了解决seq2seq模型中的两个弊端，Bahdanau等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》中提出使用Attention机制，使得seq2seq模型可以有区分度、有重点地关注输入序列，从而极大地提高了机器翻译的质量。</p><p>一个有注意力机制的seq2seq与经典的seq2seq主要有2点不同：</p><ol><li>首先，编码器会把更多的数据传递给解码器。编码器把所有时间步的 hidden state（隐藏层状态）传递给解码器，而不是只传递最后一个 hidden state（隐藏层状态）</li><li>注意力模型的解码器在产生输出之前，做了一个额外的attention处理</li></ol><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>transformer原论文的架构图：</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/trm_1.png?raw=true" width="400" alt="" align="center" /><p>一个更清晰的架构图：<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/trm_2.png?raw=true" width="600" alt="" align="center" /></p><p>从输入到输出拆开看就是：</p><ul><li>INPUT：input vector + position encoding</li><li>ENCODERs（×6），and each encoder includes：<ul><li>input</li><li>multi-head self-attention</li><li>residual connection&amp;norm</li><li>full-connected network</li><li>residual connection&amp;norm</li><li>output</li></ul></li><li>DECODERs（×6），and each decoder includes：<ul><li>input </li><li>Masked multihead self-attention</li><li>residual connection&amp;norm</li><li>multi-head self-attention</li><li>residual connection&amp;norm</li><li>full-connected network</li><li>residual connection&amp;norm</li><li>output</li></ul></li><li>OUTPUT：<ul><li>output (decoder’s)</li><li>linear layer</li><li>softmax layer</li><li>output</li></ul></li></ul><h2 id="模型输入"><a href="#模型输入" class="headerlink" title="模型输入"></a>模型输入</h2><h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>和常见的NLP任务一样，我们首先会使用词嵌入算法（embedding），将输入文本序列的每个词转换为一个词向量。</p><h3 id="位置向量"><a href="#位置向量" class="headerlink" title="位置向量"></a>位置向量</h3><p>Transformer模型对每个输入的词向量都加上了一个位置向量。这些向量有助于确定每个单词的位置特征，或者句子中不同单词之间的距离特征。词向量加上位置向量背后的直觉是：将这些表示位置的向量添加到词向量中，得到的新向量，可以为模型提供更多有意义的信息，比如词的位置，词之间的距离等。</p><p><em>（生成位置编码向量的方法有很多种）</em></p><h2 id="编码器和解码器"><a href="#编码器和解码器" class="headerlink" title="编码器和解码器"></a>编码器和解码器</h2><p><em>注：1. 编码器和解码器中有相似的模块和结构，所以合并到一起介绍。</em><br><em>2. 本部分按照李宏毅老师的Attention，Transformer部分的课程PPT来，因为lee的课程对新手更友好。</em></p><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>self-attention对于每个向量都会考虑整个sequence的信息后输出一个向量，self-attention结构如下：<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/04_attention_1.png?raw=true" width="600" alt="" align="center" /><br>FC：Fully-connected network 全连接网络<br>ai: 输入变量。可能是整个网络的输入，也可能是某个隐藏层的输出<br>bi: 考虑整个sequence信息后的输出变量</p><p>矩阵计算：<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/13_matrix_4.jpg?raw=true" width="300" alt="" align="center" /><br>目标：根据输入向量矩阵I，计算输出向量矩阵O。矩阵运算过程：</p><ol><li>矩阵I分别乘以Wq, Wk, Wv（参数矩阵，需要模型进行学习），得到矩阵Q, K, V。</li><li>矩阵K的转置乘以Q，得到注意力权重矩阵A，归一化得到矩阵A’。</li><li>矩阵V乘矩阵A‘，得到输出向量矩阵O。</li></ol><h3 id="Multi-Head-Self-Attention"><a href="#Multi-Head-Self-Attention" class="headerlink" title="Multi Head Self-Attention"></a>Multi Head Self-Attention</h3><p><em>简单地说，多了几组Q，K，V。在Self-Attention中，我们是使用𝑞去寻找与之相关的𝑘，但是这个相关性并不一定有一种。那多种相关性体现到计算方式上就是有多个矩阵𝑞，不同的𝑞负责代表不同的相关性。</em></p><p>Transformer 的论文通过增加多头注意力机制（一组注意力称为一个 attention head），进一步完善了Self-Attention。这种机制从如下两个方面增强了attention层的能力：</p><ul><li>它扩展了模型关注不同位置的能力。</li><li>多头注意力机制赋予attention层多个“子表示空间”。</li></ul><h3 id="残差链接和归一化"><a href="#残差链接和归一化" class="headerlink" title="残差链接和归一化"></a>残差链接和归一化</h3><p>残差链接：一种把input向量和output向量直接加起来的架构。<br>归一化：把数据映射到0～1范围之内处理。</p><h2 id="模型输出"><a href="#模型输出" class="headerlink" title="模型输出"></a>模型输出</h2><h3 id="线性层和softmax"><a href="#线性层和softmax" class="headerlink" title="线性层和softmax"></a>线性层和softmax</h3><p>Decoder 最终的输出是一个向量，其中每个元素是浮点数。我们怎么把这个向量转换为单词呢？这是线性层和softmax完成的。</p><p>线性层就是一个普通的全连接神经网络，可以把解码器输出的向量，映射到一个更大的向量，这个向量称为 logits 向量：假设我们的模型有 10000 个英语单词（模型的输出词汇表），此 logits 向量便会有 10000 个数字，每个数表示一个单词的分数。</p><p>然后，Softmax 层会把这些分数转换为概率（把所有的分数转换为正数，并且加起来等于 1）。然后选择最高概率的那个数字对应的词，就是这个时间步的输出单词。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>Transformer训练的时候，需要将解码器的输出和label一同送入损失函数，以获得loss，最终模型根据loss进行方向传播。</p><p>只要Transformer解码器预测了组概率，我们就可以把这组概率和正确的输出概率做对比，然后使用反向传播来调整模型的权重，使得输出的概率分布更加接近整数输出。</p><p>那我们要怎么比较两个概率分布呢？：我们可以简单的用两组概率向量的的空间距离作为loss（向量相减，然后求平方和，再开方），当然也可以使用交叉熵(cross-entropy)]和KL 散度(Kullback–Leibler divergence)。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><strong>理论部分</strong><br>[1] (强推)李宏毅2021春机器学习课程 <a href="https://www.bilibili.com/video/BV1Wv411h7kN?from=search&seid=17090062977285779802&spm_id_from=333.337.0.0">https://www.bilibili.com/video/BV1Wv411h7kN?from=search&amp;seid=17090062977285779802&amp;spm_id_from=333.337.0.0</a><br>[2] <strong>基于transformers的自然语言处理(NLP)入门（涵盖了图解系列、annotated transformer、huggingface）</strong> <a href="https://github.com/datawhalechina/learn-nlp-with-transformers">https://github.com/datawhalechina/learn-nlp-with-transformers</a><br>[3] 图解transformer|The Illustrated Transformer <a href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a><br>[4] 图解seq2seq, attention|Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a></p><p><strong>代码部分</strong><br>[5] The Annotated Transformer <a href="http://nlp.seas.harvard.edu//2018/04/03/attention.html">http://nlp.seas.harvard.edu//2018/04/03/attention.html</a><br>[6] Huggingface/transformers <a href="https://github.com/huggingface/transformers/blob/master/README_zh-hans.md">https://github.com/huggingface/transformers/blob/master/README_zh-hans.md</a></p><p><strong>论文部分</strong><br>Attention is all “we” need.</p><p><strong>其他不错的博客或教程</strong><br>[7] 基于transformers的自然语言处理(NLP)入门–在线阅读 <a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/">https://datawhalechina.github.io/learn-nlp-with-transformers/#/</a><br>[8] 李宏毅2021春机器学习课程笔记——自注意力机制 <a href="https://www.cnblogs.com/sykline/p/14730088.html">https://www.cnblogs.com/sykline/p/14730088.html</a><br>[9] 李宏毅2021春机器学习课程笔记——Transformer模型 <a href="https://www.cnblogs.com/sykline/p/14785552.html">https://www.cnblogs.com/sykline/p/14785552.html</a><br>[10] 李宏毅机器学习学习笔记——自注意力机制 <a href="https://blog.csdn.net/p_memory/article/details/116271274">https://blog.csdn.net/p_memory/article/details/116271274</a><br>[11] 车万翔-自然语言处理新范式：基于预训练的方法【讲座+PPT】 <a href="https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true">https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true</a><br>[12] 苏剑林-《Attention is All You Need》浅读（简介+代码）<a href="https://spaces.ac.cn/archives/4765">https://spaces.ac.cn/archives/4765</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Attention&quot;&gt;&lt;a href=&quot;#Attention&quot; class=&quot;headerlink&quot; title=&quot;Attention&quot;&gt;&lt;/a&gt;Attention&lt;/h1&gt;&lt;h2 id=&quot;seq2seq&quot;&gt;&lt;a href=&quot;#seq2seq&quot; class=&quot;he</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="第29期 基于transformer的NLP" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="预训练模型" scheme="http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="attention" scheme="http://example.com/tags/attention/"/>
    
    <category term="transfomer" scheme="http://example.com/tags/transfomer/"/>
    
  </entry>
  
  <entry>
    <title>task01 NLP学习概览</title>
    <link href="http://example.com/nlp-transformer-task01/"/>
    <id>http://example.com/nlp-transformer-task01/</id>
    <published>2021-09-12T16:14:06.000Z</published>
    <updated>2021-09-20T19:08:32.412Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NLP思维导图-最近更新日期：2021-09-13"><a href="#NLP思维导图-最近更新日期：2021-09-13" class="headerlink" title="NLP思维导图(最近更新日期：2021-09-13)"></a>NLP思维导图(最近更新日期：2021-09-13)</h1><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/NLP.png?raw=true" width="900" alt="" align="center" /><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>Datawhale-基于transformers的自然语言处理(NLP)入门 <a href="https://github.com/datawhalechina/learn-nlp-with-transformers">https://github.com/datawhalechina/learn-nlp-with-transformers</a></li><li>《自然语言处理-基于预训练模型的方法》 <a href="https://item.jd.com/13344628.html">https://item.jd.com/13344628.html</a></li><li>刘知远老师-NLP研究入门之道 <a href="https://github.com/zibuyu/research_tao">https://github.com/zibuyu/research_tao</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;NLP思维导图-最近更新日期：2021-09-13&quot;&gt;&lt;a href=&quot;#NLP思维导图-最近更新日期：2021-09-13&quot; class=&quot;headerlink&quot; title=&quot;NLP思维导图(最近更新日期：2021-09-13)&quot;&gt;&lt;/a&gt;NLP思维导图(最近</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="第29期 基于transformer的NLP" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%AC%AC29%E6%9C%9F-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="预训练模型" scheme="http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>CS61A Week1 Comupter_Science, Functions</title>
    <link href="http://example.com/cs61a-week1/"/>
    <id>http://example.com/cs61a-week1/</id>
    <published>2021-09-01T08:46:52.000Z</published>
    <updated>2021-09-17T01:51:24.035Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>CS61A作为61系列基础课程的第一门课程，是一门计算机入门导论课程，伯克利大一新生的第一门计算机课程。该课程主要使用Python语言，简要介绍了计算机的各种概念，范围广而涉猎不深，包括高阶函数，抽象，递归和树，OOP，简单的SQL语句，Scheme语法和解释器等概念。</p><p>目前推荐的课程是20年秋季学期(fa20)的课程。</p><p>——名校公开课程评价网 <a href="https://conanhujinming.github.io/comments-for-awesome-courses/UC%20BerkeleyCS61A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9E%84%E9%80%A0%E4%B8%8E%E8%A7%A3%E9%87%8A/">名校公开课程评价网-cs61a</a></p></blockquote><h2 id="文档组织"><a href="#文档组织" class="headerlink" title="文档组织"></a>文档组织</h2><p>对应不同教学内容，文档组织如下：</p><img src="/images/cs61a/01/zuzhi.jpg" width = "200" alt="" align="center" /><ul><li>0_课件：lecture</li><li>1_代码：lecture代码</li><li>2_笔记：学习笔记<ul><li>使用markdown</li><li>内容包括：Weekx内容(x=week number), Lecture Notes, Lab Notes, Homework Notes</li></ul></li><li>3_实验：lab</li><li>4_作业：homework</li><li>5_项目：project</li></ul><h2 id="Week1内容"><a href="#Week1内容" class="headerlink" title="Week1内容"></a>Week1内容</h2><img src="/images/cs61a/01/00.jpg" alt="" align="center" /><p>Week1主要内容：</p><ul><li><p>Lecture01 Computer Science; Lecture02 Functions 介绍计算机科学和函数基础知识</p></li><li><p>Lab00: Getting Started 安装Python3，终端的使用，常用命令行，文档测试（doctest）的使用，测试和提交使用OK系统</p><img src="/images/cs61a/01/02.jpg" width = "500" alt="" align="center" /></li><li><p>HW01: Variables &amp; Functions, Control。掌握函数特性</p></li></ul><h2 id="Lecture-Notes"><a href="#Lecture-Notes" class="headerlink" title="Lecture Notes"></a>Lecture Notes</h2><h3 id="What-is-Computer-Science"><a href="#What-is-Computer-Science" class="headerlink" title="What is Computer Science"></a>What is Computer Science</h3><img src="/images/cs61a/01/01.jpg" alt="" width = "700" align="center" /><p>计算机科学是一门定义和解决计算问题的方法和技术的学科。它的分支结构参考CSRanking的分类方式（ <a href="http://csrankings.org/#/index?all&world">http://csrankings.org/#/index?all&amp;world</a>），大概可以分为人工智能（计算机视觉、机器学习、自然语言处理、信息检索…）、系统（计算机结构、网络、安全、数据库、操作系统、分布式…）、理论（算法和复杂度、formal method…）、交叉（计算生物/生物计算、人机交互、机器人…）等方向。</p><h3 id="Anatomy-of-a-Call-Expression-Operator-Operand"><a href="#Anatomy-of-a-Call-Expression-Operator-Operand" class="headerlink" title="Anatomy of a Call Expression: Operator, Operand"></a>Anatomy of a Call Expression: Operator, Operand</h3><img src="/images/cs61a/01/03.jpg" width = "700" alt="" align="center" /><blockquote><p>在程式語言中, 指示程式進行運算(計算、比較或連結) 的符號, 稱為operators (運算子), 被運算的資料稱為operands (運算元), 一句中有operators 及operands 就稱為expression。</p></blockquote><h3 id="Environment-Diagrams"><a href="#Environment-Diagrams" class="headerlink" title="Environment Diagrams"></a>Environment Diagrams</h3><img src="/images/cs61a/01/04.jpg" width = "700" alt="" align="center" /><p>Environment Diagrams Tools: <a href="http://pythontutor.com/composingprograms.html#mode=edit">http://pythontutor.com/composingprograms.html#mode=edit</a></p><h3 id="Defining-Functions"><a href="#Defining-Functions" class="headerlink" title="Defining Functions"></a>Defining Functions</h3><img src="/images/cs61a/01/05.jpg" alt="" width = "700" align="center" /><img src="/images/cs61a/01/06.jpg" alt="" width = "700" align="center" /><img src="/images/cs61a/01/07.jpg" alt="" width = "700" align="center" /><p>这里涉及到全局变量（Global Variable）和局部变量（Local Variable）的问题，全局变量是整个程序都可访问的变量，生存期从程序开始到程序结束；局部变量存在于模块中(比如某个函数)，只有在模块中才可以访问，生存期从模块开始到模块结束。简单的说，</p><ul><li>全局变量：在模块内、在所有函数的外面、在class外面</li><li>局部变量：在函数内、在class的方法内</li></ul><h2 id="Lab-Notes"><a href="#Lab-Notes" class="headerlink" title="Lab Notes"></a>Lab Notes</h2><p>常用命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ls: lists all files in the current directory</span><br><span class="line">cd &lt;path to directory&gt;: change into the specified directory</span><br><span class="line">mkdir &lt;directory name&gt;: make a new directory with the given name</span><br><span class="line">mv &lt;source path&gt; &lt;destination path&gt;: move the file at the given source to the given destination</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python3 xxx.py # 运行程序</span><br><span class="line">python3 -i xxx.py # 运行程序并打开交互式会话</span><br><span class="line">python3 -m doctest xxx.py # 运行文档测试</span><br><span class="line">python3 -m doctest - xxx.py # 运行文档测试并显示样例</span><br></pre></td></tr></table></figure><h2 id="Homework-Notes"><a href="#Homework-Notes" class="headerlink" title="Homework Notes"></a>Homework Notes</h2><h3 id="Bug"><a href="#Bug" class="headerlink" title="Bug"></a>Bug</h3><p><code>TypeError: &#39;int&#39; object is not callable</code></p><p>修改程序名就可以了。</p><h3 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h3><p>Q5: If Function vs Statement</p><img src="/images/cs61a/01/hwp01.jpg" alt="" align="center" /><p>while…<br><img src="/images/cs61a/01/hwp02.jpg" alt=""  width="200" align="left" /><br><br><br><br></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li>cs61a 20fall 官网 <a href="https://inst.eecs.berkeley.edu/~cs61a/fa20/">https://inst.eecs.berkeley.edu/~cs61a/fa20/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;CS61A作为61系列基础课程的第一门课程，是一门计算机入门导论课程，伯克利大一新生的第一门计算机课程。该课程主要</summary>
      
    
    
    
    <category term="01 计算机基础" scheme="http://example.com/categories/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
    <category term="CS61A 计算机程序的构造与解释" scheme="http://example.com/categories/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/CS61A-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9E%84%E9%80%A0%E4%B8%8E%E8%A7%A3%E9%87%8A/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="CS公开课" scheme="http://example.com/tags/CS%E5%85%AC%E5%BC%80%E8%AF%BE/"/>
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="function" scheme="http://example.com/tags/function/"/>
    
  </entry>
  
  <entry>
    <title>公式之美：EVERYTHING IS EPHEMERAL BUT FORMULA IS ETERNAL</title>
    <link href="http://example.com/formula/"/>
    <id>http://example.com/formula/</id>
    <published>2021-08-04T12:27:54.000Z</published>
    <updated>2021-09-20T19:20:57.054Z</updated>
    
    <content type="html"><![CDATA[<p>一个有点意思的科普书，尤其在不想写论文的时候，宁愿去看勾股定理的N种推导也不愿意碰论文。更好玩的是，这里面的插图要比内容更没有争议的获得一致好评。</p><blockquote><p>1854年之前，欧洲数学家灿若星辰，笛卡儿、拉格朗日、牛顿、贝叶斯、拉普拉斯、柯西、傅里叶、伽罗瓦等，无一不是数学天才。<br>1854—1935年，高斯、黎曼等人在数学界领袖群伦，德国取代英法成为世界的数学中心。<br>1935年之后，希特勒给美国送上“科学大礼包”：哥德尔、爱因斯坦、德拜、冯.诺依曼、费米、冯.卡门、外尔……很多科学家逃至北美，数学大本营从德国转向美国，美国成为世界的数学中心。</p></blockquote><blockquote><p>古希腊几何学家阿波洛尼乌斯总结了圆锥曲线理论，一千多年后，德国天文学家开普勒才将其应用于行星轨道；高斯被认为最早发现非欧几何，半个世纪后，由他弟子创立的黎曼几何成为广义相对论的数学基础。伴随着杠杆原理、牛顿三大定律、麦克斯韦方程、香农公式、贝叶斯定理等，人类向蒸汽时代、电力时代、信息时代乃至人工智能时代徐徐迈进。</p></blockquote><h2 id="1-1-2：数学的溯源"><a href="#1-1-2：数学的溯源" class="headerlink" title="1+1=2：数学的溯源"></a>1+1=2：数学的溯源</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/00-1plus1.jpg?raw=true" width="600" alt="" align="center" /><h2 id="勾股定理：数与形的结合"><a href="#勾股定理：数与形的结合" class="headerlink" title="勾股定理：数与形的结合"></a>勾股定理：数与形的结合</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/01-gougudingli.jpg?raw=true" width="600" alt="" align="center" /><h2 id="费马大定理：困扰人类358年"><a href="#费马大定理：困扰人类358年" class="headerlink" title="费马大定理：困扰人类358年"></a>费马大定理：困扰人类358年</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/02-feima.jpg?raw=true" width="600" alt="" align="center" /><h2 id="牛顿-莱布尼茨公式：无穷小的秘密"><a href="#牛顿-莱布尼茨公式：无穷小的秘密" class="headerlink" title="牛顿-莱布尼茨公式：无穷小的秘密"></a>牛顿-莱布尼茨公式：无穷小的秘密</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/03-weijifen.jpg?raw=true" width="600" alt="" align="center" /><h2 id="万有引力：从混沌到光明"><a href="#万有引力：从混沌到光明" class="headerlink" title="万有引力：从混沌到光明"></a>万有引力：从混沌到光明</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/04-wanyouyinli.jpg?raw=true" width="600" alt="" align="center" /><h2 id="欧拉公式：最美的等式"><a href="#欧拉公式：最美的等式" class="headerlink" title="欧拉公式：最美的等式"></a>欧拉公式：最美的等式</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/05-oula.jpg?raw=true" width="600" alt="" align="center" /><h2 id="伽罗瓦理论：无解的方程"><a href="#伽罗瓦理论：无解的方程" class="headerlink" title="伽罗瓦理论：无解的方程"></a>伽罗瓦理论：无解的方程</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/06-jialuowa.jpg?raw=true" width="600" alt="" align="center" /><h2 id="危险的黎曼猜想"><a href="#危险的黎曼猜想" class="headerlink" title="危险的黎曼猜想"></a>危险的黎曼猜想</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/07-liman.jpg?raw=true" width="600" alt="" align="center" /><h2 id="熵增定律：寂灭是宇宙宿命？"><a href="#熵增定律：寂灭是宇宙宿命？" class="headerlink" title="熵增定律：寂灭是宇宙宿命？"></a>熵增定律：寂灭是宇宙宿命？</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/08-shang.jpg?raw=true" width="600" alt="" align="center" /><h2 id="麦克斯韦方程组：让黑暗消失"><a href="#麦克斯韦方程组：让黑暗消失" class="headerlink" title="麦克斯韦方程组：让黑暗消失"></a>麦克斯韦方程组：让黑暗消失</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/09-mksw.jpg?raw=true" width="600" alt="" align="center" /><h2 id="质能方程：开启潘多拉的魔盒"><a href="#质能方程：开启潘多拉的魔盒" class="headerlink" title="质能方程：开启潘多拉的魔盒"></a>质能方程：开启潘多拉的魔盒</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/10-zhineng.jpg?raw=true" width="600" alt="" align="center" /><h2 id="薛定谔方程：猫与量子世界"><a href="#薛定谔方程：猫与量子世界" class="headerlink" title="薛定谔方程：猫与量子世界"></a>薛定谔方程：猫与量子世界</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/11-uedinge.jpg?raw=true" width="600" alt="" align="center" /><h2 id="狄拉克方程：反物质的“先知”"><a href="#狄拉克方程：反物质的“先知”" class="headerlink" title="狄拉克方程：反物质的“先知”"></a>狄拉克方程：反物质的“先知”</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/12-dilake.jpg?raw=true" width="600" alt="" align="center" /><h2 id="杨-米尔斯规范场论：大统一之路"><a href="#杨-米尔斯规范场论：大统一之路" class="headerlink" title="杨-米尔斯规范场论：大统一之路"></a>杨-米尔斯规范场论：大统一之路</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/13-yang.jpg?raw=true" width="600" alt="" align="center" /><h2 id="香农公式：5G背后的主宰"><a href="#香农公式：5G背后的主宰" class="headerlink" title="香农公式：5G背后的主宰"></a>香农公式：5G背后的主宰</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/14-xiangnong.jpg?raw=true" width="600" alt="" align="center" /><h2 id="布莱克-斯科尔斯方程：金融“巫师”"><a href="#布莱克-斯科尔斯方程：金融“巫师”" class="headerlink" title="布莱克-斯科尔斯方程：金融“巫师”"></a>布莱克-斯科尔斯方程：金融“巫师”</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/15-bulaike.jpg?raw=true" width="600" alt="" align="center" /><h2 id="枪械：弹道里的“技术哲学”"><a href="#枪械：弹道里的“技术哲学”" class="headerlink" title="枪械：弹道里的“技术哲学”"></a>枪械：弹道里的“技术哲学”</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/16-qiangxie.jpg?raw=true" width="600" alt="" align="center" /><h2 id="胡克定律：机械表的心脏"><a href="#胡克定律：机械表的心脏" class="headerlink" title="胡克定律：机械表的心脏"></a>胡克定律：机械表的心脏</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/17-huke.jpg?raw=true" width="600" alt="" align="center" /><h2 id="混沌理论：一只蝴蝶引发的思考"><a href="#混沌理论：一只蝴蝶引发的思考" class="headerlink" title="混沌理论：一只蝴蝶引发的思考"></a>混沌理论：一只蝴蝶引发的思考</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/18-hundun.jpg?raw=true" width="600" alt="" align="center" /><h2 id="凯利公式：赌场上的最大赢家"><a href="#凯利公式：赌场上的最大赢家" class="headerlink" title="凯利公式：赌场上的最大赢家"></a>凯利公式：赌场上的最大赢家</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/19-kaili.jpg?raw=true" width="600" alt="" align="center" /><h2 id="贝叶斯定理：AI如何思考？"><a href="#贝叶斯定理：AI如何思考？" class="headerlink" title="贝叶斯定理：AI如何思考？"></a>贝叶斯定理：AI如何思考？</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/20-beiyesi.jpg?raw=true" width="600" alt="" align="center" /><h2 id="三体问题：挥之不去的乌云"><a href="#三体问题：挥之不去的乌云" class="headerlink" title="三体问题：挥之不去的乌云"></a>三体问题：挥之不去的乌云</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/21-santi.jpg?raw=true" width="600" alt="" align="center" /><h2 id="椭圆曲线方程：比特币的基石"><a href="#椭圆曲线方程：比特币的基石" class="headerlink" title="椭圆曲线方程：比特币的基石"></a>椭圆曲线方程：比特币的基石</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/22-bitcoin.jpg?raw=true" width="600" alt="" align="center" /><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>《公式之美》<a href="https://book.douban.com/subject/35218287/">https://book.douban.com/subject/35218287/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;一个有点意思的科普书，尤其在不想写论文的时候，宁愿去看勾股定理的N种推导也不愿意碰论文。更好玩的是，这里面的插图要比内容更没有争议的获得一致好评。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1854年之前，欧洲数学家灿若星辰，笛卡儿、拉格朗日、牛顿、贝叶斯、拉普拉斯、柯西、傅</summary>
      
    
    
    
    <category term="沉思录" scheme="http://example.com/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"/>
    
    
    <category term="阅读" scheme="http://example.com/tags/%E9%98%85%E8%AF%BB/"/>
    
    <category term="math" scheme="http://example.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>NLP中的文本表示方法</title>
    <link href="http://example.com/nlp-text-representation/"/>
    <id>http://example.com/nlp-text-representation/</id>
    <published>2021-07-31T11:48:54.000Z</published>
    <updated>2021-09-17T01:53:35.231Z</updated>
    
    <content type="html"><![CDATA[<p>TODO</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href=""></a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;TODO&lt;/p&gt;
&lt;h1 id=&quot;参考资料&quot;&gt;&lt;a href=&quot;#参考资料&quot; class=&quot;headerlink&quot; title=&quot;参考资料&quot;&gt;&lt;/a&gt;参考资料&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</summary>
      
    
    
    
    <category term="02 人工智能" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="自然语言处理" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="文本表示" scheme="http://example.com/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"/>
    
  </entry>
  
  <entry>
    <title>小狗钱钱：理财启蒙的童话</title>
    <link href="http://example.com/%E5%B0%8F%E7%8B%97%E9%92%B1%E9%92%B1/"/>
    <id>http://example.com/%E5%B0%8F%E7%8B%97%E9%92%B1%E9%92%B1/</id>
    <published>2021-07-28T18:06:58.000Z</published>
    <updated>2021-09-20T19:20:46.930Z</updated>
    
    <content type="html"><![CDATA[<img src="/images/xgqq/xgqq.jpeg" width = "200" alt="" align="center" /><blockquote><p>《小狗钱钱》是一本理财的启蒙书籍，讲述的是一个会理财的小狗的童话故事：从这个童话故事里可以了解一些金钱的秘密和真相，以及投资、理财的办法。这个故事描述了在实施这些方法的过程中可能遇到的挑战，并且说明了一些令人难以置信的结果。</p></blockquote><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><blockquote><p><strong>认为我们必须忍受拮据的生活，甚至认为这样才是高尚的——这种想法是人类犯下的最重大的错误之一。</strong></p></blockquote><p>有一些批判的声音说，理财是一个很资本主义的概念。把家庭或者生活当做一个公司去经营，过于理性、物质、精明，以至于丧失了生活中的乐趣，因为生活是体验和感受为主的。<br>实际上金钱带来了诸多限制，不然为什么大家整天想着财务自由呢。据说犹太人擅长经商的原因之一就是不掩盖自己对财富的欲望。所以，坦白的说，我很穷，我想赚钱。</p><h1 id="第二章-梦想储蓄罐和梦想相册"><a href="#第二章-梦想储蓄罐和梦想相册" class="headerlink" title="第二章 梦想储蓄罐和梦想相册"></a>第二章 梦想储蓄罐和梦想相册</h1><blockquote><p>太多的人做事犹豫不决，就是因为他们觉得没有完全弄懂这件事。真正付诸实践要比纯粹的思考有用多了。</p></blockquote><p>光想是没用的，just do it.</p><h1 id="第三章-达瑞，一个很会挣钱的男孩"><a href="#第三章-达瑞，一个很会挣钱的男孩" class="headerlink" title="第三章 达瑞，一个很会挣钱的男孩"></a>第三章 达瑞，一个很会挣钱的男孩</h1><blockquote><p>你去准备一个本子，给它取名叫‘成功日记’，然后把所有做成功的事情记录进去。你最好每天都做这件事，每次都写至少5条你的个人成果，任何小事都可以。开始的时候也许你觉得不太容易，可能会问自己，这件或那件事情是否真的可以算作成果。在这种情况下，你的回答应该是肯定的。过于自信比不够自信要好得多。</p></blockquote><p>成功日记，对于一个谦虚的典型中国人来说也太羞耻了吧。但是，最后一句绝对是真理。过于自信或许会经常丢面子，但会抓住很多意想不到的机会，其中某个机会或许就改变了人生轨迹。一定要多做尝试。</p><h1 id="第四章-堂兄的挣钱之道"><a href="#第四章-堂兄的挣钱之道" class="headerlink" title="第四章 堂兄的挣钱之道"></a>第四章 堂兄的挣钱之道</h1><blockquote><p>第一，无论在什么时候都不能把希望只寄托在一份工作上，它持续的时间不会像你设想的那么长，所以你要立即寻找另一份替代的工作。</p></blockquote><p>生活不易，多才多艺。打工人要想生存，就得多掌握点技能。</p><h1 id="第五章-钱钱以前的主人"><a href="#第五章-钱钱以前的主人" class="headerlink" title="第五章 钱钱以前的主人"></a>第五章 钱钱以前的主人</h1><blockquote><p>困难总是在不断地出现。尽管如此，你每天还是要不间断地去做对你的未来意义重大的事情。你为此花费的时间不会超过10分钟，但是就是这10分钟会让一切变得不同。大多数人总是在现有的水平上停滞不前，就是因为他们没有拿出这10分钟。他们总是期望情况能向有利于自己的方向转变，但是他们忽视了一点，那就是他们首先必须改变自己。<br>当你决定做一件事情的时候，你必须在72小时之内完成，否则你很可能永远不会再做了。</p></blockquote><p>个人来说，对未来意义重大的事情就是每天花10分钟记账。</p><h1 id="第六章-爸爸妈妈犯下的错误"><a href="#第六章-爸爸妈妈犯下的错误" class="headerlink" title="第六章 爸爸妈妈犯下的错误"></a>第六章 爸爸妈妈犯下的错误</h1><blockquote><p>根本上说，那些陷入债务的人只要听从4个忠告就可以解决负债问题。一切都很简单。”</p></blockquote><h2 id="欠债的人应当毁掉所有的信用卡。"><a href="#欠债的人应当毁掉所有的信用卡。" class="headerlink" title="欠债的人应当毁掉所有的信用卡。"></a>欠债的人应当毁掉所有的信用卡。</h2><blockquote><p>“因为大多数人在使用信用卡的时候，会比使用现金时花的钱要多得多。”钱钱答道。</p></blockquote><h2 id="应当尽可能少地偿还贷款——也就是大人们说的分期付款。"><a href="#应当尽可能少地偿还贷款——也就是大人们说的分期付款。" class="headerlink" title="应当尽可能少地偿还贷款——也就是大人们说的分期付款。"></a>应当尽可能少地偿还贷款——也就是大人们说的分期付款。</h2><blockquote><p>也许这个忠告听起来有点儿可笑，但你要知道，分期付款额越高，每个月剩下的生活费就越少。</p></blockquote><h2 id="将扣除生活费后剩下的钱的一半存起来，剩下的一半用于支付消费贷款。最好根本不申请消费贷款。"><a href="#将扣除生活费后剩下的钱的一半存起来，剩下的一半用于支付消费贷款。最好根本不申请消费贷款。" class="headerlink" title="将扣除生活费后剩下的钱的一半存起来，剩下的一半用于支付消费贷款。最好根本不申请消费贷款。"></a><strong>将扣除生活费后剩下的钱的一半存起来，剩下的一半用于支付消费贷款。最好根本不申请消费贷款。</strong></h2><blockquote><p>消费贷款是与住房无关的贷款。假如人们为了购置新的汽车、家具、电视机或其他用于生活的商品而贷款，就是消费贷款。<strong>这时候贷款的人应当遵守的一个原则，就是将不用于生活的那部分钱的一半存起来，另一半用于偿还贷款。</strong><br>你的爸爸妈妈应当开始攒钱，他们不需要等到还清债务以后再开始存钱，他们可以现在立即开始，<strong>只有这样，他们才有能力在不申请新的贷款的情况下，满足自己的愿望。</strong>那样他们也才能心安理得地、更好地享用这些东西。<br>钱钱点头说：“这个主意真不错。此外，所有的消费贷款都是不明智的。聪明的做法是只把以前积攒起来的财富用于支出。”</p></blockquote><h2 id="绝不借债。"><a href="#绝不借债。" class="headerlink" title="绝不借债。"></a>绝不借债。</h2><blockquote><p>我作了一个决定：我绝不借债。为实现一个愿望，我要提前开始储蓄。我绝不要陷入和爸爸妈妈一样的困境。</p></blockquote><h1 id="第七章-在金先生家"><a href="#第七章-在金先生家" class="headerlink" title="第七章 在金先生家"></a>第七章 在金先生家</h1><blockquote><p>金先生慢悠悠地说，“鹅代表你的钱。如果你存钱，你就会得到利息。利息就相当于金蛋。”<br>金先生接着说：“大多数人生来并没有‘鹅’。这就是说，他们的钱不足以让他们依靠利息来生活。”<br>这完全要根据你的目标来决定。如果你总是把10%的钱变成‘鹅’，那么你一定会变得富有。但如果你想有一天真的非常有钱的话，你存的比例可能得再高一些。我的习惯是把我收入的50%变成我的‘鹅’。”</p></blockquote><p>这里涉及到经济学中生产要素的概念。生产要素包括劳动、土地、资本、信息、技术等。劳动的收入是工资，土地的收入是地租，资本的收入是利息…</p><h1 id="第八章-陶穆太太"><a href="#第八章-陶穆太太" class="headerlink" title="第八章 陶穆太太"></a>第八章 陶穆太太</h1><blockquote><p>我不同意她的话。金先生看上去十分快乐，而爸爸妈妈却恰恰相反，总是不太高兴。直觉告诉我，贫穷更容易产生不幸。</p></blockquote><p>你以为有钱人真的很快乐吗？是的，他们的快乐你根本想象不到。所以，停止自我安慰吧hh</p><h1 id="第十二章-陶穆太太归来"><a href="#第十二章-陶穆太太归来" class="headerlink" title="第十二章 陶穆太太归来"></a>第十二章 陶穆太太归来</h1><blockquote><p>要想过更幸福、更满意的生活，人就得改变自身。这和钱无关，金钱本身既不会使人幸福，也不会带来不幸。金钱是中性的，既不好，也不坏。只有当钱属于某一个人的时候，它才会对这个人产生好的影响或者坏的影响。钱可以被用于好的用途，也可以被用于坏的用途。一个幸福的人有了钱会更幸福；而一个悲观忧虑的人，钱越多，烦恼就越多。</p></blockquote><h1 id="第十三章-巨大的危机"><a href="#第十三章-巨大的危机" class="headerlink" title="第十三章 巨大的危机"></a>第十三章 巨大的危机</h1><blockquote><p>“这不像你现在想的那么简单。”我听见钱钱用恳切的口气接着说，“成功会使人骄傲。如果你骄傲自大，你就会停止学习。不学习，人就不会再进步。”<br>它停顿了一下，接着说：“当你写成功日记的时候，你会对自己，对世界，还有对成功的规律作更深入的思考，会越来越多地了解自己和自己的愿望，这样你才会有能力去理解别人。彻底了解自己和世界上的所有秘密，是我们无法完全实现的一种理想，但我们可以一步一步地慢慢接近这种理想。”<br>恐惧总是出现在我们设想事情会如何不顺的时候。我们对失败的可能性想得越多，就会越害怕。而当你看着自己的成功日记时，你就会注意到那些成功的事情，自然而然也就会想到应该怎样去做。<br>当你朝着积极的目标去思考的时候，就不会心生畏惧。</p></blockquote><p>首先，人是不能停止学习的，无论到什么阶段，停下来马上会变得迟钝、狭隘。<br>其次，在另一本流行的理财书籍《穷查理宝典》中，作者查理芒格的理念是，“生活上的大多数成功来自于你应该知道避免哪些事情”。虽然作者很牛逼，但我很难认同这一点。因为失败的原因实在太多了，一件事情可以找一万个理由做不到，但只要找到一条路这事就能做成。正如幸福的家庭都是相似的，不幸的家庭各有各的不幸。</p><h1 id="第十四章-投资俱乐部"><a href="#第十四章-投资俱乐部" class="headerlink" title="第十四章 投资俱乐部"></a>第十四章 投资俱乐部</h1><blockquote><p>我们一致认为，只要学会了我们的咒语，就可以从无到有地变出钱来。<br>我们的咒语是：<br>1．确定自己希望获得财务上的成功。<br>2．自信，有想法，做自己喜欢做的事。<br>3．把钱分成日常开销、梦想目标和金鹅账户三部分。<br>4．进行明智的投资。<br>5．享受生活。</p></blockquote><p>第3点，分割账户的理念。我觉得应该分为，日常开销、负债、储蓄、理财。就是说，一个人的净收入大概用于这4个方面的预算是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">I = 收入</span><br><span class="line"><span class="keyword">if</span> 负债:</span><br><span class="line">    日常开销 = <span class="number">50</span>%*I</span><br><span class="line">    负债 = <span class="number">25</span>%*I</span><br><span class="line">    储蓄 = <span class="number">15</span>%*I</span><br><span class="line">    理财 = <span class="number">10</span>%*I</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    日常开销 = <span class="number">50</span>%*I</span><br><span class="line">    储蓄 = <span class="number">30</span>%*I</span><br><span class="line">    理财 = <span class="number">20</span>%*I</span><br></pre></td></tr></table></figure><p>这是我目前的分配偏好。</p><h1 id="第十七章-爷爷奶奶害怕风险"><a href="#第十七章-爷爷奶奶害怕风险" class="headerlink" title="第十七章 爷爷奶奶害怕风险"></a>第十七章 爷爷奶奶害怕风险</h1><blockquote><p>“只有当我们把它卖出的时候，才会有亏损。可是我们并没有这么做。”</p></blockquote><p>每当股票和基金跌了的时候，很适合自我安慰。这句话其实是经济学或者说会计记账的把戏，因为不卖的话，没有产生经济活动，账本没有变化。:)</p><h1 id="自力更生——写给成年人的后记"><a href="#自力更生——写给成年人的后记" class="headerlink" title="自力更生——写给成年人的后记"></a>自力更生——写给成年人的后记</h1><blockquote><p>我们推崇一种聪明的、简朴的生活方式。也就是说，宁愿购买一件一流产品，也不要不停地买许多的二流产品。而且，不要仅仅因为一件产品的外观不再时髦而新产品正在流行，就不断追逐新鲜的东西。<br>生活质量不是由越来越多的高科技产品堆砌而成的，而体现在一些别的方面，比如悠闲地享受一下生活，增进邻里关系，表达感情或者从事艺术性和创造性的活动。</p></blockquote><p>这也是我开始喜欢极简主义的原因吧。开始极简后，才发现自己买了太多没用的东西。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>《小狗钱钱》<a href="https://book.douban.com/subject/1095634/">https://book.douban.com/subject/1095634/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;img src=&quot;/images/xgqq/xgqq.jpeg&quot; width = &quot;200&quot; alt=&quot;&quot; align=&quot;center&quot; /&gt;

&lt;blockquote&gt;
&lt;p&gt;《小狗钱钱》是一本理财的启蒙书籍，讲述的是一个会理财的小狗的童话故事：从这个童话故事里可以了解一些金</summary>
      
    
    
    
    <category term="沉思录" scheme="http://example.com/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"/>
    
    
    <category term="阅读" scheme="http://example.com/tags/%E9%98%85%E8%AF%BB/"/>
    
    <category term="财务管理" scheme="http://example.com/tags/%E8%B4%A2%E5%8A%A1%E7%AE%A1%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>SQL表连接&amp;聚合函数&amp;窗口函数</title>
    <link href="http://example.com/SQL%E9%87%8D%E7%82%B9/"/>
    <id>http://example.com/SQL%E9%87%8D%E7%82%B9/</id>
    <published>2021-07-26T19:33:19.000Z</published>
    <updated>2021-09-17T01:53:20.789Z</updated>
    
    <content type="html"><![CDATA[<h1 id="表连接-join"><a href="#表连接-join" class="headerlink" title="表连接 join"></a>表连接 join</h1><p>join: 以字段（列）为单位进行多表连接。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Inner</span> <span class="keyword">join</span> # 只保留两个表中同时存在的记录。</span><br><span class="line"><span class="keyword">Left</span> <span class="keyword">join</span> # 保留左表所有的记录，无论其是否能够在右表中匹配到对应的记录。若无匹配记录，则需要用<span class="keyword">NULL</span>填补。</span><br><span class="line"><span class="keyword">Right</span> <span class="keyword">join</span> # 保留右表所有的记录，无论其是否能够在左表中匹配到对应的记录。若无匹配记录，则需要用<span class="keyword">NULL</span>填补。</span><br><span class="line"><span class="keyword">Full</span> <span class="keyword">join</span> # 左表和右表所有的记录都会保留，没有匹配记录的用<span class="keyword">NULL</span>填补。</span><br></pre></td></tr></table></figure><h1 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sum</span>() # 返回分组后组内所有记录的和</span><br><span class="line"><span class="built_in">avg</span>() # 返回分组后组内所有记录的均值</span><br><span class="line"><span class="built_in">count</span>() # 返回分组后组内所有记录的计数</span><br><span class="line"><span class="built_in">max</span>()<span class="operator">/</span><span class="built_in">min</span>() # 返回分组后组内所有记录的最大值、最小值</span><br></pre></td></tr></table></figure><h1 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h1><p>窗口函数对记录分组之后进行聚合计算，为分组中的每条记录返回特定值。</p><p>窗口函数的基本结构是：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&lt;</span>窗口函数<span class="operator">&gt;</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> <span class="operator">&lt;</span>col1, col2<span class="operator">&gt;</span> </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> <span class="operator">&lt;</span>col3 <span class="keyword">desc</span><span class="operator">/</span><span class="keyword">asc</span>, col4 <span class="keyword">asc</span><span class="operator">/</span><span class="keyword">desc</span><span class="operator">&gt;</span>)</span><br></pre></td></tr></table></figure><table><thead><tr><th>窗口函数</th><th>介绍</th></tr></thead><tbody><tr><td><code>rank() over()</code></td><td>返回记录在同一分组内的排序，如果有并列名次的行，会占用下一名次的位置</td></tr><tr><td><code>dense_rank() over()</code></td><td>返回记录在同一分组内的排序，如果有并列名次的行，不占用下一名次的位置</td></tr><tr><td><code>row_number() over()</code></td><td>返回记录在同一分组内的排序，不考虑并列名次的情况</td></tr><tr><td><code>percent_rank() over()</code></td><td>返回记录在同一分组内排序的分位数，为0~1</td></tr><tr><td><code>sum(col) over()</code></td><td>返回同一分组内所有记录col值的和，同一分组内记录的返回值相同</td></tr><tr><td><code>avg(col) over()</code></td><td>返回同一分组内所有记录col值的平均值，同一分组内记录的返回值相同</td></tr><tr><td><code>max/min(col) over()</code></td><td>返回同一分组内所有记录col值的最大值/最小值，同一分组内记录的返回值相同</td></tr></tbody></table><p>聚合函数在窗口函数中，是对自身记录、及位于自身记录以上的数据进行运算的结果。聚合函数作为窗口函数，可以在每一行的数据里直观的看到，截止到本行数据，统计数据是多少（最大值、最小值等）。同时可以看出每一行数据，对整体统计数据的影响。</p><h1 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h1><p>索引用来排序数据以加快搜索和排序操作的速度。可以在一个或多个列上定义索引，使DBMS保存其内容的一个排过序的列表。在定义了索引后，DBMS以使用书的索引类似的方法使用它。DBMS 搜索排过序的索引，找出匹配的位置，然后检索这些行。</p><p>索引是关系数据库中对某一列或多个列的值进行预排序的数据结构。通过使用索引，可以让数据库系统不必扫描整个表，而是直接定位到符合条件的记录，这样就大大加快了查询速度。</p><p>索引用CREATE INDEX 语句创建（不同DBMS创建索引的语句变化很大）。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1]通俗易懂的学会：SQL窗口函数 <a href="https://zhuanlan.zhihu.com/p/92654574">https://zhuanlan.zhihu.com/p/92654574</a><br>[2]《拿下Offer:数据分析师求职面试指南》 <a href="https://item.jd.com/12686131.html">https://item.jd.com/12686131.html</a><br>[3]《SQL必知必会》 <a href="https://book.douban.com/subject/24250054/">https://book.douban.com/subject/24250054/</a><br>[4]廖雪峰的官方网站-SQL教程 <a href="https://www.liaoxuefeng.com/wiki/1177760294764384">https://www.liaoxuefeng.com/wiki/1177760294764384</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;表连接-join&quot;&gt;&lt;a href=&quot;#表连接-join&quot; class=&quot;headerlink&quot; title=&quot;表连接 join&quot;&gt;&lt;/a&gt;表连接 join&lt;/h1&gt;&lt;p&gt;join: 以字段（列）为单位进行多表连接。&lt;/p&gt;
&lt;figure class=&quot;high</summary>
      
    
    
    
    <category term="02 人工智能" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="数据分析" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
    <category term="SQL" scheme="http://example.com/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>5h打通Git全套教程</title>
    <link href="http://example.com/git/"/>
    <id>http://example.com/git/</id>
    <published>2021-07-23T13:01:25.000Z</published>
    <updated>2021-09-17T01:53:30.399Z</updated>
    
    <content type="html"><![CDATA[<p>本文是以下课程的笔记：</p><ul><li>【尚硅谷】5h打通Git全套教程丨2021最新IDEA版（涵盖GitHub\Gitee码云\GitLab）<a href="https://www.bilibili.com/video/BV1vy4y1s7k6?p=41">https://www.bilibili.com/video/BV1vy4y1s7k6?p=41</a></li></ul><h1 id="课程结构"><a href="#课程结构" class="headerlink" title="课程结构"></a>课程结构</h1><p>本套视频从基础的常用命令开始讲起，到开发工具集成Git 、GitHub如何进行团队协作、国内代码托管中心Gitee码云的使用、局域网自建代码托管平台GitLab服务器的部署。<strong>（本文主要是P1-P26的笔记。）</strong></p><ul><li>P1-P2 Git</li><li>P3-P6 Git概述</li><li>P7-P14 Git命令</li><li>P15-P18 Git分支</li><li>P19 Git团队协作</li><li>P20-P26 Git&amp;GitHub</li><li>P27-P37 IDEA集成GitHub</li><li>P38-P40 Gitee码云</li><li>P41-P44 GitLab</li><li>P45 课程总结</li></ul><h1 id="Git介绍"><a href="#Git介绍" class="headerlink" title="Git介绍"></a>Git介绍</h1><h2 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h2><p>Git是一个<strong>免费的、开源的分布式版本控制系统</strong>，可以快速高效地处理从小型到大型的各种项目。</p><h2 id="版本控制"><a href="#版本控制" class="headerlink" title="版本控制"></a>版本控制</h2><p>版本控制是一种记录文件内容变化，以便将来查阅特定版本修订情况的系统。版本控制其实最重要的是可以记录文件修改历史记录，从而让用户能够查看历史版本，方便版本切换。<br><img src="/images/git/v_ctrl.jpg" width = "500" alt="一个糟糕的版本控制" align="center" /></p><h2 id="版本控制工具"><a href="#版本控制工具" class="headerlink" title="版本控制工具"></a>版本控制工具</h2><ul><li>集中式版本控制工具：CVS、SVN(Subversion)、VSS……</li><li>分布式版本控制工具：Git、 Mercurial、 Bazaar、 Darcs……</li></ul><h2 id="Git简史"><a href="#Git简史" class="headerlink" title="Git简史"></a>Git简史</h2><p>Git是Linus大神写的，所以和Linux一套命令<br><img src="/images/git/git_history.jpg" width = "800" alt="Git简史" align="center" /></p><h2 id="Git机制"><a href="#Git机制" class="headerlink" title="Git机制"></a>Git机制</h2><p>在工作区写代码，通过git add添加到暂存区，再通过git commit提交到本地库，生成历史版本。本地库可以push代码到远程库，也可以从远程库pull拉取代码，不同版本的代码可以进行merge。<br><img src="/images/git/git_strcture.jpg" width = "300" alt="Git工作机制" align="center" /></p><h2 id="代码托管中心-远程库"><a href="#代码托管中心-远程库" class="headerlink" title="代码托管中心-远程库"></a>代码托管中心-远程库</h2><p>代码托管中心是基于网络服务器的远程代码仓库，一般我们简单称为远程库。</p><ul><li>局域网：GitLab</li><li>互联网：GitHub(国外)，Gitee（国内）</li></ul><h1 id="Git常用命令"><a href="#Git常用命令" class="headerlink" title="Git常用命令"></a>Git常用命令</h1><p>一图以蔽之，<br><img src="/images/git/git_command.jpg" width = "600" alt="Git常用命令" align="center" /></p><h1 id="Git分支"><a href="#Git分支" class="headerlink" title="Git分支"></a>Git分支</h1><h2 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h2><p>在版本控制过程中，同时推进多个任务，为每个任务，我们就可以创建每个任务的单独分支。使用分支意味着程序员可以把自己的工作从开发主线上分离开来，开发自己分支的时候，不会影响主线分支的运行。对于初学者而言，分支可以简单理解为副本，一个分支就是一个单独的副本。（分支底层其实也是指针的引用）</p><p>同时并行推进多个功能开发，可以提高开发效率。各个分支在开发过程中，如果某一个分支开发失败，不会对其他分支有任何影响。失败的分支删除重新开始即可。</p><img src="/images/git/branch.jpg" width = "600" alt="分支示意图" align="center" /><h2 id="Git分支命令"><a href="#Git分支命令" class="headerlink" title="Git分支命令"></a>Git分支命令</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git branch -v # 查看分支</span><br><span class="line">git branch 分支名 # 创建分支</span><br><span class="line">git checkout 分支名 # 切换分支</span><br><span class="line">git merge 分支名 # 合并分支</span><br></pre></td></tr></table></figure><h2 id="合并冲突"><a href="#合并冲突" class="headerlink" title="合并冲突"></a>合并冲突</h2><p>合并分支时，两个分支在同一个文件的同一个位置有两套完全不同的修改。Git无法替我们决定使用哪一个。必须人为决定新代码内容。</p><h1 id="Git团队合作"><a href="#Git团队合作" class="headerlink" title="Git团队合作"></a>Git团队合作</h1><p>两个非常形象化的图和生动的例子！</p><h2 id="团队内合作"><a href="#团队内合作" class="headerlink" title="团队内合作"></a>团队内合作</h2><img src="/images/git/inside.jpg" width = "600" alt="团队内合作" align="center" /><h2 id="跨团队合作"><a href="#跨团队合作" class="headerlink" title="跨团队合作"></a>跨团队合作</h2><img src="/images/git/outside.jpg" width = "600" alt="跨团队合作" align="center" /><h1 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h1><h2 id="远程仓库操作"><a href="#远程仓库操作" class="headerlink" title="远程仓库操作"></a>远程仓库操作</h2><img src="/images/git/github_command.jpg" width = "600" alt="远程仓库操作" align="center" /><p>clone会做如下操作：</p><ol><li>拉取代码。 </li><li>初始化本地仓库。 </li><li>创建别名（origin）</li></ol><h2 id="邀请合作者"><a href="#邀请合作者" class="headerlink" title="邀请合作者"></a>邀请合作者</h2><img src="/images/git/invitation.jpg" width = "600" alt="邀请合作者" align="center" /><h2 id="SSH登录"><a href="#SSH登录" class="headerlink" title="SSH登录"></a>SSH登录</h2><img src="/images/git/ssh.jpg" width = "600" alt="SSH登录" align="center" /><h1 id="Git与其他环境集成"><a href="#Git与其他环境集成" class="headerlink" title="Git与其他环境集成"></a>Git与其他环境集成</h1><p>官方文档-Git与各种IDE的集成：<a href="https://git-scm.com/book/en/v2">https://git-scm.com/book/en/v2</a></p><h1 id="码云Gitee"><a href="#码云Gitee" class="headerlink" title="码云Gitee"></a>码云Gitee</h1><p>众所周知，GitHub服务器在国外，使用GitHub作为项目托管网站，如果网速不好的话，严重影响使用体验，甚至会出现登录不上的情况。针对这个情况，大家也可以使用国内的项目托管网站-码云 <a href="https://gitee.com/">https://gitee.com/</a>。</p><h1 id="GitLab"><a href="#GitLab" class="headerlink" title="GitLab"></a>GitLab</h1><p>GitLab <a href="https://about.gitlab.com/">https://about.gitlab.com/</a>是由GitLabInc.开发，使用MIT许可证的基于网络的Git仓库管理工具，且具有wiki和issue跟踪功能。使用Git作为代码管理工具，并在此基础上搭建起来的web服务。<br>GitLab由乌克兰程序员DmitriyZaporozhets和ValerySizov开发，它使用Ruby语言写成。后来,一些部分用Go语言重写。截止2018年5月，该公司约有290名团队成员，以及2000多名开源贡献者。GitLab被IBM, Sony, JülichResearchCenter, NASA, Alibaba, Invincea, O’Reilly Media, Leibniz-Rechenzentrum, CERN, SpaceX等组织使用。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文是以下课程的笔记：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;【尚硅谷】5h打通Git全套教程丨2021最新IDEA版（涵盖GitHub\Gitee码云\GitLab）&lt;a href=&quot;https://www.bilibili.com/video/BV1vy4y1s7k6?p=41&quot;&gt;</summary>
      
    
    
    
    <category term="03 工具箱" scheme="http://example.com/categories/03-%E5%B7%A5%E5%85%B7%E7%AE%B1/"/>
    
    
    <category term="git" scheme="http://example.com/tags/git/"/>
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>大理：在民宿打工的日子</title>
    <link href="http://example.com/%E5%A4%A7%E7%90%86/"/>
    <id>http://example.com/%E5%A4%A7%E7%90%86/</id>
    <published>2021-07-21T14:49:49.000Z</published>
    <updated>2021-09-20T19:19:57.835Z</updated>
    
    <content type="html"><![CDATA[<p>生活就是换个地方打工。</p><p>然而田园的生活也不是那么美好的，有很多想象不到的问题。比如，随时随地出现的各种蚊虫、大蜘蛛、蛇，花花草草都要每天浇水和定时剪枝，下雨天泳池会有很多脏东西需要处理，垃圾必须每天清空不然会有老鼠光顾，厨房排水做不好的话会倒流没法用（怪不得要专业疏通下水道），还有电路也时不时的出点毛病。</p><p>由于每天都在处理这种琐碎的事情，这段时间最常去的地方就是仓库了。仓库就像哆啦A梦的口袋一样，变出我以前不知道的各种工具来，有些我即使认识也不知道名字，但是这些小玩意儿在生活中如此有用，比如玻璃胶、磨砂纸、扎带种种。有一次，洗衣机旁的电源开关的接触不好，电工来修，把电线直接接到了一个插排上，虽然我对这个做法的安全性和易用性保持怀疑，但足够让物理一向不好的我很是佩服——毕竟我连火线零线都分不清，高中物理连接小灯泡的题也得想半天。</p><p>后来，就匆匆离开了。就像Harry说的，It is not better or worse than any place else - just different. </p><img src="/images/dali/1.JPG" width="300" alt="" align="left" /><img src="/images/dali/2.JPG" width="300" alt="" align="left" /><img src="/images/dali/3.JPG" width="300" alt="" align="left" /><img src="/images/dali/4.JPG" width="300" alt="" align="left" /><img src="/images/dali/5.JPG" width="300" alt="" align="left" /><img src="/images/dali/8.JPG" width="300" alt="" align="left" /><img src="/images/dali/6.jpeg" width="300" alt="" align="left" /><img src="/images/dali/7.jpeg" width="300" alt="" align="left" /><img src="/images/dali/9.jpeg" width="300" alt="" align="left" />]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;生活就是换个地方打工。&lt;/p&gt;
&lt;p&gt;然而田园的生活也不是那么美好的，有很多想象不到的问题。比如，随时随地出现的各种蚊虫、大蜘蛛、蛇，花花草草都要每天浇水和定时剪枝，下雨天泳池会有很多脏东西需要处理，垃圾必须每天清空不然会有老鼠光顾，厨房排水做不好的话会倒流没法用（怪不得要</summary>
      
    
    
    
    <category term="沉思录" scheme="http://example.com/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"/>
    
    
    <category term="大理" scheme="http://example.com/tags/%E5%A4%A7%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>校准对世界的预期</title>
    <link href="http://example.com/%E6%A0%A1%E5%87%86%E5%AF%B9%E4%B8%96%E7%95%8C%E7%9A%84%E9%A2%84%E6%9C%9F/"/>
    <id>http://example.com/%E6%A0%A1%E5%87%86%E5%AF%B9%E4%B8%96%E7%95%8C%E7%9A%84%E9%A2%84%E6%9C%9F/</id>
    <published>2021-07-15T09:34:54.000Z</published>
    <updated>2021-09-17T01:53:48.924Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>校准对世界的预期，这个题目出自于播客里听到的一段话：</p><blockquote><p>从学校步入职场，要做的第一件事是重新校准你对世界的预期。这世界就是很糟糕的（或者说，这世界是 okay 的，但是你对它有不切实际的预期）。</p></blockquote><p>很早的时候我经常怀疑，老师讲的东西有什么用？总有那么些课程，既不能直接用到工作中，又不能锻炼思维和逻辑能力。最后不得不承认一个危险的真相：上课太耽误学习了。而自己之所以为此感到痛苦，是因为抱有不合理的假设和预期。比如，当我产生怀疑的时候，其实我的假设就是老师教的就应该是有用的。可是凭什么呢？明明是我一厢情愿的这样认为，然后发现事实与预期不符，于是就不断痛苦。这样的假设还包括：</p><h2 id="H1-学校教的是对工作有用的-×"><a href="#H1-学校教的是对工作有用的-×" class="headerlink" title="H1. 学校教的是对工作有用的(×)"></a>H1. 学校教的是对工作有用的(×)</h2><p>学校是用来筛选的。学校有一套自己的游戏规则，按照这个规则走，就会得到老师和同学的赞赏、拿奖学金、保研、发论文、获得一系列荣誉奖项等——最终筛选出一个乖巧、听话、能干活、而且学习能力不错的人（或者至少愿意伪装成这样的人）。但把这个规则玩好，并不等同于拥有了工作需要的实力和能力，因为工作是另一套规则和玩法。由于从小一直灌输的是学校的规则，所以很难意识到游戏已经切换了，这就是常说的学生思维吧。</p><p>同时，这个世界上有无数个行业和领域，就有无数游戏和规则。诚然底层的东西是可以迁移，例如努力、负责等品质在哪里都是被认可的，但仍然要保持开放的心态，不能做出一个选择就否定其他选择，比如，一些马上想到但是不可描述的例子。这种心态的形成至少有3个因素：1.人的心理天然倾向于找各种理由支持自己的选择，而忽略对自己选择不利的方面。2.即使知道自己的选择有不好的一面，但出于虚荣坚决不能承认。3.即使知道自己的选择有不好的一面，但忽悠更多人进来才方便找人接盘。</p><p>最终通过身边的现象来看，乖乖听老师话的找不到工作只能考公（不是说公务员不好，而是说这种情况下并没有选择权），早早翘课去实习的拿到了令人羡慕的offer。用朋友圈里看到的一段话概括就是：主修LeetCode，辅修bilibili，旁听cs公开课，最后混一下学位必修课以达到毕业要求。这才是找到好工作的正确姿势。</p><h2 id="H2-世界是公平的-×"><a href="#H2-世界是公平的-×" class="headerlink" title="H2. 世界是公平的(×)"></a>H2. 世界是公平的(×)</h2><p>世界本来是不公平的，也不天然应该是公平的，因为人类文明的出现于是变好了一点点。所以那些声称只能接受这个糟糕的世界的说法也不对，这简直是在否定人类文明，没有什么是理所当然的。</p><h2 id="尝试放下所有假设和预期"><a href="#尝试放下所有假设和预期" class="headerlink" title="尝试放下所有假设和预期"></a>尝试放下所有假设和预期</h2><p>我们对于很多东西的判断是来自于书籍、电视、网络、和他人的只言片语、自己的经验。通过经验学习当然很有效，但问题是这些经验不都是客观理性的，如果我们放下这些先验知识，感受事物本身，通过自己的独立思考得到自己的结论，那么被忽悠的概率会大大降低吧。</p><h2 id="参考播客"><a href="#参考播客" class="headerlink" title="参考播客"></a>参考播客</h2><ul><li>BYM 职场12: 生活就是不公平的，你打算怎么办 <a href="https://www.xiaoyuzhoufm.com/episode/5e280fac418a84a0461fb16a?s=eyJ1IjogIjVlZWJiNWJkOTJkNzAyMzc2NzhkMDRkZSJ9">收听链接</a></li><li>BYM 职场14: 认清世界之残酷，你才可能成为那 5% 拥有自己事业的人 <a href="https://www.xiaoyuzhoufm.com/episode/5e280fac418a84a0461fb166?s=eyJ1IjogIjVlZWJiNWJkOTJkNzAyMzc2NzhkMDRkZSJ9">收听链接</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;校准对世界的预期，这个题目出自于播客里听到的一段话：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;从学校步入职场，要做的第一件事是重新校准你对世</summary>
      
    
    
    
    <category term="沉思录" scheme="http://example.com/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"/>
    
    
    <category term="沉思录" scheme="http://example.com/tags/%E6%B2%89%E6%80%9D%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>一些好用的中英文LaTeX简历模板</title>
    <link href="http://example.com/%E4%B8%80%E4%BA%9B%E5%A5%BD%E7%94%A8%E7%9A%84%E4%B8%AD%E8%8B%B1%E6%96%87LaTeX%E7%AE%80%E5%8E%86%E6%A8%A1%E6%9D%BF/"/>
    <id>http://example.com/%E4%B8%80%E4%BA%9B%E5%A5%BD%E7%94%A8%E7%9A%84%E4%B8%AD%E8%8B%B1%E6%96%87LaTeX%E7%AE%80%E5%8E%86%E6%A8%A1%E6%9D%BF/</id>
    <published>2021-07-15T08:23:20.000Z</published>
    <updated>2021-09-17T01:53:59.711Z</updated>
    
    <content type="html"><![CDATA[<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简历的结构主要包括：</p><ul><li>标题 Heading（姓名、联系方式等）</li><li>教育背景 Education</li><li>经历 Experience<ul><li>实习 Work</li><li>科研/论文 Publications</li><li>项目 Projects</li></ul></li><li>个人技能 Skills</li></ul><h2 id="GitHub模板"><a href="#GitHub模板" class="headerlink" title="GitHub模板"></a>GitHub模板</h2><h3 id="英文简历-https-github-com-sb2nov-resume"><a href="#英文简历-https-github-com-sb2nov-resume" class="headerlink" title="英文简历 https://github.com/sb2nov/resume"></a>英文简历 <a href="https://github.com/sb2nov/resume">https://github.com/sb2nov/resume</a></h3><img src="/images/resume/resume_01.png" width = "600" alt="rusume_01" align="center" /><h3 id="中文简历-https-github-com-hijiangtao-resume"><a href="#中文简历-https-github-com-hijiangtao-resume" class="headerlink" title="中文简历 https://github.com/hijiangtao/resume"></a>中文简历 <a href="https://github.com/hijiangtao/resume">https://github.com/hijiangtao/resume</a></h3><img src="/images/resume/resume_02.png" width = "600" alt="rusume_02" align="center" /><h3 id="中英文兼容的简历-https-github-com-billryan-resume-tree-zh-CN"><a href="#中英文兼容的简历-https-github-com-billryan-resume-tree-zh-CN" class="headerlink" title="中英文兼容的简历 https://github.com/billryan/resume/tree/zh_CN"></a>中英文兼容的简历 <a href="https://github.com/billryan/resume/tree/zh_CN">https://github.com/billryan/resume/tree/zh_CN</a></h3><img src="/images/resume/resume_03.png" width = "600" alt="rusume_03" align="center" /><img src="/images/resume/resume_04.png" width = "600" alt="rusume_04" align="center" /><h2 id="怎么写简历的参考文章"><a href="#怎么写简历的参考文章" class="headerlink" title="怎么写简历的参考文章"></a>怎么写简历的参考文章</h2><h3 id="师妹看了都说好的简历长啥样"><a href="#师妹看了都说好的简历长啥样" class="headerlink" title="师妹看了都说好的简历长啥样"></a>师妹看了都说好的简历长啥样</h3><p><a href="https://mp.weixin.qq.com/s/ea2Pq3ZbJ30lTV3etaECoQ">https://mp.weixin.qq.com/s/ea2Pq3ZbJ30lTV3etaECoQ</a></p><h3 id="一个简洁优雅的-XeLaTeX-简历模板"><a href="#一个简洁优雅的-XeLaTeX-简历模板" class="headerlink" title="一个简洁优雅的 XeLaTeX 简历模板"></a>一个简洁优雅的 XeLaTeX 简历模板</h3><p><a href="https://tiankuizhang.github.io/files/00CV_CN/README/">https://tiankuizhang.github.io/files/00CV_CN/README/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;总结&quot;&gt;&lt;a href=&quot;#总结&quot; class=&quot;headerlink&quot; title=&quot;总结&quot;&gt;&lt;/a&gt;总结&lt;/h2&gt;&lt;p&gt;简历的结构主要包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;标题 Heading（姓名、联系方式等）&lt;/li&gt;
&lt;li&gt;教育背景 Education&lt;/</summary>
      
    
    
    
    <category term="03 工具箱" scheme="http://example.com/categories/03-%E5%B7%A5%E5%85%B7%E7%AE%B1/"/>
    
    
    <category term="laTeX" scheme="http://example.com/tags/laTeX/"/>
    
    <category term="简历" scheme="http://example.com/tags/%E7%AE%80%E5%8E%86/"/>
    
  </entry>
  
  <entry>
    <title>SQL学习资料</title>
    <link href="http://example.com/SQL%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/"/>
    <id>http://example.com/SQL%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/</id>
    <published>2021-07-14T07:32:58.000Z</published>
    <updated>2021-09-17T01:53:15.370Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>&lt;&lt;SQL必知必会&gt;&gt; <a href="https://book.douban.com/subject/35167240/">https://book.douban.com/subject/35167240/</a></p></li><li><p>SQLZOO <a href="https://sqlzoo.net/">https://sqlzoo.net/</a></p></li><li><p>牛客网-SQL <a href="https://www.nowcoder.com/activity/oj?tab=1">https://www.nowcoder.com/activity/oj?tab=1</a></p></li><li><p>LeetCode-数据库 <a href="https://leetcode-cn.com/problemset/database/">https://leetcode-cn.com/problemset/database/</a></p></li><li><p>Datafrog-SQL经典45题 <a href="https://www.bilibili.com/video/BV1pp4y1Q7Yv">https://www.bilibili.com/video/BV1pp4y1Q7Yv</a></p></li><li><p>尚硅谷-MySQL基础教程 <a href="https://www.bilibili.com/video/BV1xW411u7ax">https://www.bilibili.com/video/BV1xW411u7ax</a></p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&amp;lt;&amp;lt;SQL必知必会&amp;gt;&amp;gt; &lt;a href=&quot;https://book.douban.com/subject/35167240/&quot;&gt;https://book.douban.com/subject/35167240/&lt;/a&gt;&lt;/p&gt;
&lt;/</summary>
      
    
    
    
    <category term="02 人工智能" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="数据分析" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
    <category term="SQL" scheme="http://example.com/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>概率论与数理统计</title>
    <link href="http://example.com/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"/>
    <id>http://example.com/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/</id>
    <published>2021-07-14T03:03:00.000Z</published>
    <updated>2021-09-17T01:53:40.027Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概率统计的基础概念"><a href="#概率统计的基础概念" class="headerlink" title="概率统计的基础概念"></a>概率统计的基础概念</h1><p>随机试验；随机变量；概率分布；概率分布函数；概率密度函数；累积分布函数；样本和总体。</p><h1 id="离散型随机变量及其分布"><a href="#离散型随机变量及其分布" class="headerlink" title="离散型随机变量及其分布"></a>离散型随机变量及其分布</h1><p>根据随机试验的结果数量是否可数，分为离散型随机变量和连续型随机变量。</p><h2 id="0-1分布-伯努利分布"><a href="#0-1分布-伯努利分布" class="headerlink" title="0-1分布/伯努利分布"></a><strong>0-1分布/伯努利分布</strong></h2><ol><li>定义：伯努利分布指的是对于随机变量X有参数为p（0&lt;1&lt;P），它分别以概率p和1-p取1和0为值。ex. 令X表示抛硬币的结果。</li><li>表示：<code>X ~ b( p)</code></li><li>公式：<code>P(X=1) = p, P(X=0) = 1-p, where p in [0,1]</code></li><li>期望与方差：<code>E(X)=p, D(X)=p(1-p)</code></li></ol><h2 id="二项分布-n个重复独立的伯努利分布"><a href="#二项分布-n个重复独立的伯努利分布" class="headerlink" title="二项分布/n个重复独立的伯努利分布"></a><strong>二项分布/n个重复独立的伯努利分布</strong></h2><ol><li>在n次独立重复的伯努利试验中，设每次试验中事件A发生的概率为p。用X表示n重伯努利试验中事件A发生的次数，则X的可能取值为0，1，…，n,且对每一个k（0≤k≤n）,事件{X=k}即为“n次试验中事件A恰好发生k次”，随机变量X的离散概率分布即为二项分布（Binomial Distribution）。</li><li>表示：<code>X ~ b(n, p)</code></li><li>概率函数：<img src="/images/statistic/bi.jpg" alt="" width="250"></li><li>期望与方差：<code>E(X)=np, D(X)=np(1-p)</code></li></ol><h2 id="几何分布"><a href="#几何分布" class="headerlink" title="几何分布"></a><strong>几何分布</strong></h2><ol><li>定义：几何分布（Geometric distribution）是离散型概率分布。其中一种定义为：在n次伯努利试验中，试验k次才得到第一次成功的机率。详细地说，是：前k-1次皆失败，第k次成功的概率。</li><li>表示：<code>X ~ g( p)</code></li><li>概率函数：<img src="/images/statistic/ge.jpg" alt="" width="300"></li><li>期望与方差：<code>E(X)=1/p, D(X)=(1-p)/p^2</code></li></ol><h2 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a><strong>泊松分布</strong></h2><ol><li>定义：Poisson分布，是一种统计与概率学里常见到的离散概率分布。泊松分布的参数λ是单位时间(或单位面积)内随机事件的平均发生次数。 泊松分布适合于描述单位时间内随机事件发生的次数。</li><li>表示：<code>X ~ p(λ)</code></li><li>概率函数：<img src="/images/statistic/po.jpg" alt="" width="300"></li><li>期望与方差：<code>E(X)=λ, D(X)=λ</code></li><li>ex. 某一服务设施在一定时间内到达的人数、某网站或APP在单位时间内的访问人数。</li><li>泊松分布与二项分布：当二项分布的n很大而p很小时，泊松分布可作为二项分布的近似，其中λ为np。通常当n≧20,p≦0.05时，就可以用泊松公式近似得计算。</li></ol><h1 id="连续型随机变量及其分布"><a href="#连续型随机变量及其分布" class="headerlink" title="连续型随机变量及其分布"></a>连续型随机变量及其分布</h1><h2 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a><strong>均匀分布</strong></h2><ol><li>定义：均匀分布也叫矩形分布，它是对称概率分布，在相同长度间隔的分布概率是等可能的。 均匀分布由两个参数a和b定义，它们是数轴上的最小值和最大值，通常缩写为U（a，b）。</li><li>表示：<code>X ~ u(a,b)</code></li><li>概率密度函数：<img src="/images/statistic/uni.jpg" alt="" width="250"></li><li>累积分布函数：<img src="/images/statistic/uni2.jpg" alt="" width="250"></li><li>期望和方差：<code>E(X)=(a+b)/2, D(X)=(a-b)^2/12</code></li></ol><h2 id="正态分布-高斯分布"><a href="#正态分布-高斯分布" class="headerlink" title="正态分布/高斯分布"></a><strong>正态分布/高斯分布</strong></h2><ol><li>定义：若随机变量X服从一个数学期望为μ、方差为σ^2的正态分布，记为<code>N(μ，σ^2)</code>。其概率密度函数为正态分布的期望值μ决定了其位置，其标准差σ决定了分布的幅度。当μ = 0,σ = 1时的正态分布是标准正态分布。</li><li>表示：<code>N(μ，σ^2)</code></li><li>标准化变换：<img src="/images/statistic/normal.jpg" alt="" width="350"></li><li>概率分布函数：<img src="/images/statistic/normal2.jpg" alt="" width="300"></li><li>期望和方差：<code>E(X)=μ，D(X)=σ^2</code></li><li>性质：<img src="./images/statistic/normal3.jpg" alt="todo" width="800"></li></ol><h2 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a><strong>指数分布</strong></h2><ol><li>定义：在概率理论和统计学中，指数分布（也称为负指数分布）是描述泊松过程中的事件之间的时间的概率分布，即事件以恒定平均速率连续且独立地发生的过程。</li><li><code>X ~ E(λ)</code> 其中λ &gt; 0是分布的一个参数，常被称为率参数（rate parameter）。即每单位时间内发生某事件的次数。指数分布的区间是[0,∞)。</li><li>概率密度函数：<img src="/images/statistic/e.jpeg" alt="" width="200"></li><li>累积分布函数：<img src="/images/statistic/e2.jpg" alt="" width="250"></li><li>期望和方差：<code>E(X)=1/λ, D(X)=1/λ^2</code></li></ol><h1 id="随机变量的分布特征与统计量"><a href="#随机变量的分布特征与统计量" class="headerlink" title="随机变量的分布特征与统计量"></a>随机变量的分布特征与统计量</h1><h2 id="数据的分布特征与统计量"><a href="#数据的分布特征与统计量" class="headerlink" title="数据的分布特征与统计量"></a>数据的分布特征与统计量</h2><img src="./images/statistic/feature.jpg" alt="" width="500"><h2 id="重要随机变量的期望和方差总结"><a href="#重要随机变量的期望和方差总结" class="headerlink" title="重要随机变量的期望和方差总结"></a><strong>重要随机变量的期望和方差总结</strong></h2><table><thead><tr><th>随机变量</th><th>表示</th><th>期望</th><th>方差</th></tr></thead><tbody><tr><td>0-1分布</td><td><code>X ~ b( p)</code></td><td><code>E(X)=p</code></td><td><code>D(X)=p(1-p)</code></td></tr><tr><td>二项分布</td><td><code>X ~ b(n, p)</code></td><td><code>E(X)=np</code></td><td><code>D(X)=np(1-p)</code></td></tr><tr><td>几何分布</td><td><code>X ~ g( p)</code></td><td><code>E(X)=1/p</code></td><td><code>D(X)=(1-p)/p^2</code></td></tr><tr><td>泊松分布</td><td><code>X ~ p(λ)</code></td><td><code>E(X)=λ</code></td><td><code>D(X)=λ</code></td></tr><tr><td>均匀分布</td><td><code>X ~ u(a,b)</code></td><td><code>E(X)=(a+b)/2</code></td><td><code>D(X)=(a-b)^2/12</code></td></tr><tr><td>正态分布</td><td><code>N(μ，σ^2)</code></td><td><code>E(X)=μ</code></td><td><code>D(X)=σ^2</code></td></tr><tr><td>指数分布</td><td><code>X ~ E(λ)</code></td><td><code>E(X)=1/λ</code></td><td><code>D(X)=1/λ^2</code></td></tr></tbody></table><h2 id="协方差和相关系数"><a href="#协方差和相关系数" class="headerlink" title="协方差和相关系数"></a><strong>协方差和相关系数</strong></h2><p><strong>协方差</strong><br>协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。协方差表示的是两个变量的总体的误差。<br><a href="https://baike.baidu.com/item/%E5%8D%8F%E6%96%B9%E5%B7%AE">https://baike.baidu.com/item/协方差</a></p><p><strong>相关系数</strong><br>相关关系是一种非确定性的关系，相关系数是研究变量之间线性相关程度的量。相关系数是最早由统计学家卡尔·皮尔逊设计的统计指标，是研究变量之间线性相关程度的量，一般用字母 r 表示。由于研究对象的不同，相关系数有多种定义方式，较为常用的是皮尔逊相关系数。<br><a href="https://baike.baidu.com/item/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0">https://baike.baidu.com/item/相关系数</a></p><h1 id="独立事件、条件概率、贝叶斯定理"><a href="#独立事件、条件概率、贝叶斯定理" class="headerlink" title="独立事件、条件概率、贝叶斯定理"></a>独立事件、条件概率、贝叶斯定理</h1><p><strong>独立事件</strong><br>独立事件：两个事件不论哪一个事件发生都不影响另一个发生的概率。<br>A和B是独立的, 当且仅当: <code>P(AB) = P(A)P(B)</code></p><p><strong>条件概率</strong><br>条件概率：当事件B已经发生时，事件A发生的概率。<br>事件B发生条件下事件A发生的概率为: <code>P(A|B) = P(AB)/P(B)</code><br>概率的乘法公式: <code>P(AB) = P(A|B)P(B) = P(B|A)P(A)</code></p><p><strong>全概率公式</strong><br><code>P(B) = ∑P(B|Ai)P(Ai)，i=1,2,...,k</code></p><p><strong>贝叶斯定理</strong><br><img src="./img/bys.jpg" alt="todo" width="300"><br>通常P(Ai)为A的先验概率，P(Ai|B)为A的后验概率。</p><h1 id="大数定理与中心极限定理"><a href="#大数定理与中心极限定理" class="headerlink" title="大数定理与中心极限定理"></a>大数定理与中心极限定理</h1><p><strong>大数定律</strong><br>大数定律：将随机变量X所对应的随机试验重复多次，随着试验次数的增加，X的均值会愈发趋近于E(X)。或当样本数据无限大时，样本均值趋于总体均值。</p><p><strong>中心极限定理</strong><br>中心极限定理（central limit theorem）:设从均值为μ、方差为o2 （有限）的任意一个总体中抽取样本量为n的样本，当n充分大时，样本均值X的抽样分布近似服从均值为μ、方差为o2/n的正态分布。</p><h1 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h1><p><strong>置信区间、置信度</strong><br>在区间估计中，由样本统计量所构造的总体参数的估计区间称为<strong>置信区间（confidence interval）</strong>，其中区间的最小值称为置信下限，最大值称为置信上限。由于统计学家在某种程度上确信这个区间会包含真正的总体参数，所以给它取名为置信区间。原因是，如果抽取了许多不同的样本，比如说抽取100个样本，根据每一个样本构造一个置信区间，这样，由100个样本构造的总体参数的100个置信区间中，有95%的区间包含了总体参数的真值，而5%则没包含，则95%这个值称为置信水平。一 般地，如果将构造置信区间的步骤重复多次，置信区间中包含总体参数真值的次数所占的比例称为<strong>置信水平（ confidence level）</strong>，也称为<strong>置信度</strong>或<strong>置信系数（ confidence coefficient）</strong>。</p><p><strong>评价估计量的标准：1.无偏性 2.有效性 3.一致性</strong></p><p><strong>一个总体参数的区间估计</strong></p><p><strong>两个总体参数的区间估计</strong></p><h1 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h1><p>H0: 原假设<br>H1: 备择假设</p><p>检验统计量：用于假设检验计算的统计量，基于样本检验统计量的值来接受或者拒绝原假设。在原假设成立的情况下，检验统计量服从一个特定的分布；而在备择假设成立的情况下，则不服从该分布。常用的检验统计量有t统计量、z统计量等。</p><h2 id="假设检验的基本思想"><a href="#假设检验的基本思想" class="headerlink" title="假设检验的基本思想"></a><strong>假设检验的基本思想</strong></h2><p>通过证明在原假设成立的前提下，检验统计量出现当前值或者更为极端的值属于“小概率”事件，以此推翻原假设，接受备择假设。</p><h2 id="p-value"><a href="#p-value" class="headerlink" title="p-value"></a><strong>p-value</strong></h2><p>“检验统计量出现当前值或者更为极端的值”的概率就是p-value.将p值与预先设定的显著性水平α进行对比，如果p值小于α，就可以推翻原假设，接受备择假设。</p><h2 id="两类错误"><a href="#两类错误" class="headerlink" title="两类错误"></a>两类错误</h2><table><thead><tr><th>项目</th><th>没有拒绝H0</th><th>拒绝H0</th></tr></thead><tbody><tr><td>H0为真</td><td>1-α (正确决策)</td><td>α (弃真错误/第i类错误)</td></tr><tr><td>H0为伪</td><td>β (取伪错误/第ii类错误)</td><td>1-β (正确决策)</td></tr></tbody></table><h2 id="抽样分布：z分布、卡方分布、t分布、F分布"><a href="#抽样分布：z分布、卡方分布、t分布、F分布" class="headerlink" title="抽样分布：z分布、卡方分布、t分布、F分布"></a>抽样分布：z分布、卡方分布、t分布、F分布</h2><p><strong>z分布</strong><br>z分布：正态分布（Normal distribution）又名高斯分布（Gaussiandistribution），若随机变量X服从一个数学期望为μ、方差为σ2的高斯分布，记为N(μ，σ2)。</p><p><strong>卡方分布</strong><br>若n个相互独立的随机变量ξ₁，ξ₂，…,ξn ，均服从标准正态分布（也称独立同分布于标准正态分布），则这n个服从标准正态分布的随机变量的平方和构成一新的随机变量，其分布规律称为卡方分布（chi-square distribution）。其中参数n称为自由度。卡方分布是由正态分布构造而成的一个新的分布，当自由度很大时，卡方分布近似为正态分布。<br>E = n, D = 2n</p><p><strong>t分布</strong><br>t分布:首先要提一句u分布，正态分布（normal distribution）是许多统计方法的理论基础。正态分布的两个参数μ和σ决定了正态分布的位置和形态。为了应用方便，常将一般的正态变 量X通过u变换(X一μ)/σ转化成标准正态变量u， 以使原来各种形态的正态分布都转换为μ=0，σ= 1的标准正态分布（standard normaldistribution） ，亦称u分布。根据中心极限定理，通过抽样模拟试验表明，在正态分布总体中以固定n抽取若干个样本时，样本均数的分布仍服从正态分布，即N (μ， σ)。所以，对样本均数的分布进行u变换，也可变换为标准正态分布N(0,1)<br><strong>由于在实际工作中，往往σ（总体方差）是未知的，常用s （样本方差）作为o的估计值，为了与u变换区别，称为t变换，统计量t值的分布称为t分布。</strong></p><p><strong>F分布</strong><br><img src="./img/F.jpg" alt="todo" width="700"></p><p><strong>一个总体参数的检验</strong></p><p><strong>两个总体参数的检验</strong></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1]《统计学完全教程》第一版 L.沃塞曼<br>[2]《统计学》第六版 贾俊平</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;概率统计的基础概念&quot;&gt;&lt;a href=&quot;#概率统计的基础概念&quot; class=&quot;headerlink&quot; title=&quot;概率统计的基础概念&quot;&gt;&lt;/a&gt;概率统计的基础概念&lt;/h1&gt;&lt;p&gt;随机试验；随机变量；概率分布；概率分布函数；概率密度函数；累积分布函数；样本和总体。</summary>
      
    
    
    
    <category term="02 人工智能" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="数据分析" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
    <category term="统计" scheme="http://example.com/tags/%E7%BB%9F%E8%AE%A1/"/>
    
  </entry>
  
</feed>
