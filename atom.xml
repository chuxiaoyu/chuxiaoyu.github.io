<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Memex</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-01-24T15:38:32.233Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Xiaoyu CHU</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Leetcode刷题（第1-2期）</title>
    <link href="http://example.com/leetcode/"/>
    <id>http://example.com/leetcode/</id>
    <published>2022-01-24T15:26:18.000Z</published>
    <updated>2022-01-24T15:38:32.233Z</updated>
    
    <content type="html"><![CDATA[<p>2021-11 Leetcode 刷题课程第一期 <a href="https://github.com/itcharge/LeetCode-Py/blob/main/Assets/Course/Course-01.md">https://github.com/itcharge/LeetCode-Py/blob/main/Assets/Course/Course-01.md</a><br>2022-01 Leetcode 刷题课程第一期 <a href="https://github.com/itcharge/LeetCode-Py/blob/main/Assets/Course/Course-01.md">https://github.com/itcharge/LeetCode-Py/blob/main/Assets/Course/Course-01.md</a><br>GitHub 地址：<a href="https://github.com/itcharge/LeetCode-Py">https://github.com/itcharge/LeetCode-Py</a><br>电子书地址：<a href="https://algo.itcharge.cn/">https://algo.itcharge.cn</a></p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leetcode/算法通关手册.png?raw=true" width="900" alt="" align="center" />]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;2021-11 Leetcode 刷题课程第一期 &lt;a href=&quot;https://github.com/itcharge/LeetCode-Py/blob/main/Assets/Course/Course-01.md&quot;&gt;https://github.com/itchar</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="2022-01 LeetCode刷题" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/2022-01-LeetCode%E5%88%B7%E9%A2%98/"/>
    
    
    <category term="leetcode" scheme="http://example.com/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>Hello-World|本站导航</title>
    <link href="http://example.com/hello-world/"/>
    <id>http://example.com/hello-world/</id>
    <published>2022-01-19T11:39:32.000Z</published>
    <updated>2022-01-28T12:08:08.224Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>费曼语录（翻译自Twitter）</title>
    <link href="http://example.com/feynman-quotes/"/>
    <id>http://example.com/feynman-quotes/</id>
    <published>2021-12-02T05:38:38.000Z</published>
    <updated>2021-12-02T07:13:48.922Z</updated>
    
    <content type="html"><![CDATA[<p>费曼是我最喜欢的科学家和教育家，以下语录摘自费曼教授的推特（<a href="https://twitter.com/ProfFeynman">https://twitter.com/ProfFeynman</a>）。<br>不过，账户当然不是费曼本人，我猜大概也是粉丝之类，且大部分语录出处也无从考证。</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/cs224n/feynman.jpg?raw=true" width="600" alt="" align="center" /><h1 id="费曼学习法"><a href="#费曼学习法" class="headerlink" title="费曼学习法"></a>费曼学习法</h1><blockquote><p>The Feynman-technique of learning:</p><ol><li>Pick a topic you wanna understand and start studying it</li><li>Pretend to teach the topic to a classroom</li><li>Go back to the books when you get stuck</li><li>Simplify and use analogies!</li></ol><p>If you wanna master something, teach it.</p><p>关于费曼学习法（又称费曼学习技巧）：</p><ol><li>选择一个主题开始学习</li><li>假装自己在把它教给别人</li><li>当你卡住的时候，回去翻课本</li><li>简化之，类比之</li></ol><p>如果你想精通什么东西，那么就尝试把它教给其他人。</p></blockquote><h1 id="关于学习"><a href="#关于学习" class="headerlink" title="关于学习"></a>关于学习</h1><blockquote><ol><li>Read and write more. 多读多写。</li><li>Don’t hesitate to admit when you’re wrong. 承认自己错误的时候不要犹豫。</li><li>Be comfortable changing your opinion. 乐于改变自己的观点。</li><li>Find a mentor. 找一个导师/老师/师傅/老司机（带带自己）。</li><li>Stay teachable. 保持孺子可教。</li><li>Make mistakes and learn. 犯错，并学习。</li><li>Don’t get offended easily. 不要轻易被冒犯到。</li><li>Ask questions. 君子之学必好问。非学无以致疑，非问无以广识。</li><li>Spend time with nature. 多和大自然相处（而不是计算机）。</li><li>Stay humble. 满招损，谦受益。</li></ol></blockquote><blockquote><p>Your intelligence cannot be measured by a number. It is defined by your willingness to learn, solve problems and try new things. You are more than just a number. Develop your skills. Share your brilliant ideas. Your skills are more valuable than your grades.<br>你的能力并不能被智商衡量，而是取决于你学习、解决问题和尝试新事物的意愿。你的能力比考试得分有价值得多。</p></blockquote><blockquote><p>Knowledge isn’t free. You have to pay attention.<br>知识不是免费的，你得花点注意力。（很难翻出原句的精髓，attention is all we need!）</p></blockquote><blockquote><p>If you cannot explain something in simple terms, you don’t understand it.<br>如果你不能用简单的词语解释一个东西，那么你并没有理解它。（有一则传闻是，费曼发现自己无法向本科生解释清楚量子力学，于是他说：“我确信没有人能够懂量子力学。”）</p></blockquote><h1 id="关于教育"><a href="#关于教育" class="headerlink" title="关于教育"></a>关于教育</h1><blockquote><p>The problem is not people being uneducated.<br>The problem is that people are educated just enough to believe what they have been taught, and not educated enough to question anything from what they have been taught.<br>人们的问题不在于没有受到教育。<br>问题在于他们受教育的程度只能让其相信所被教授的东西，却不足以让他们提出质疑。</p></blockquote><blockquote><p>The ultimate purpose of education is to change an empty mind into an open one.<br>教育的最终目的是把空白的大脑变成开放的。</p></blockquote><blockquote><p>The world is filled with educated fools and uneducated geniuses.<br>世界上充满了受教育的傻瓜和未受教育的天才。（结论：上学使人变傻 :D）</p></blockquote><h1 id="关于谦逊"><a href="#关于谦逊" class="headerlink" title="关于谦逊"></a>关于谦逊</h1><blockquote><p>Illusion of knowledge is more dangerous than ignorance:<br>It’s Okay to say “I don’t know” and admit that you don’t know it.<br>It’s shameful to pretend that you know everything.<br>知识的假象比无知更加可怕：<br>承认自己不知道没有关系，假装自己无所不知才是可耻的。</p></blockquote><blockquote><p>Five great signs of intelligence:<br>• You’re not afraid or ashamed to find errors in your understanding of things.<br>• You take mistakes as lessons.<br>• You don’t get offended with accepting the facts.<br>• You are highly adaptable and very curious.<br>• You know what you don’t know.<br>智力的5项标志：<br>• 对于自己认知中的错误之处，不会感到害怕或者羞耻<br>• 把错误当教训<br>• 接受事实而不会被冒犯（换言之，我的目标只是让自己正确——我并不关心正确的答案是不是来源于我。）<br>• 高适应性、强烈的好奇心<br>• 知道自己不知道</p></blockquote><h1 id="关于人生"><a href="#关于人生" class="headerlink" title="关于人生"></a>关于人生</h1><blockquote><p>Don’t regret anything in life. If it’s good, it’s wonderful. If it’s bad, it’s experience! When you win, you win. When you lose, you learn.<br>不要后悔生命中的任何事情。赢就赢了，输了还可以学习。</p></blockquote><blockquote><p>You can be a good person with a kind heart and still say No.<br>你是个好人，并且仍然有说不的权利。</p></blockquote><blockquote><p>Don’t be impressed by money, followers, degrees, and titles.<br>Be impressed by kindness, integrity, humility, and generosity.<br>不要迷恋一个人的财富、地位、学位、头衔，<br>而是他的善良、正直、谦虚、慷慨。</p></blockquote><blockquote><p>Life is too short to worry about stupid things. Have fun. Fall in love. Regret nothing, and don’t let people bring you down. Study, think, create, and grow. Teach yourself and teach others.<br>人生太短暂，不能浪费在担心愚蠢的事物上。玩得开心，谈谈恋爱；不要后悔，不要被任何人击垮。<br>学习，思考，创造，成长。自我教育，也教化别人。</p></blockquote><h1 id="关于成败"><a href="#关于成败" class="headerlink" title="关于成败"></a>关于成败</h1><blockquote><p>Don’t just teach your children how to be successful, teach them how to respond when they are not successful, teach them how to handle failures and learn from their mistakes.<br>不要教孩子们怎样才能成功，而是教他们在不成功的时候如何应对，教他们如何对待失败，教他们从错误中学习。</p></blockquote><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://twitter.com/ProfFeynman">https://twitter.com/ProfFeynman</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;费曼是我最喜欢的科学家和教育家，以下语录摘自费曼教授的推特（&lt;a href=&quot;https://twitter.com/ProfFeynman&quot;&gt;https://twitter.com/ProfFeynman&lt;/a&gt;）。&lt;br&gt;不过，账户当然不是费曼本人，我猜大概也是粉丝之类</summary>
      
    
    
    
    <category term="沉思录" scheme="http://example.com/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"/>
    
    
    <category term="feynman" scheme="http://example.com/tags/feynman/"/>
    
    <category term="quote" scheme="http://example.com/tags/quote/"/>
    
  </entry>
  
  <entry>
    <title>CS224n Lec01 Introduction and Word Vectors</title>
    <link href="http://example.com/cs224n-lec01/"/>
    <id>http://example.com/cs224n-lec01/</id>
    <published>2021-11-28T11:37:51.000Z</published>
    <updated>2021-11-29T03:24:42.686Z</updated>
    
    <content type="html"><![CDATA[<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>CS224n官方网站<a href="http://web.stanford.edu/class/cs224n/">http://web.stanford.edu/class/cs224n/</a></li><li>bilibili <a href="https://www.bilibili.com/video/BV1nP4y1j7rZ">https://www.bilibili.com/video/BV1nP4y1j7rZ</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;参考资料&quot;&gt;&lt;a href=&quot;#参考资料&quot; class=&quot;headerlink&quot; title=&quot;参考资料&quot;&gt;&lt;/a&gt;参考资料&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;CS224n官方网站&lt;a href=&quot;http://web.stanford.edu/class/cs224n/</summary>
      
    
    
    
    <category term="02 人工智能" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="CS224n 自然语言处理" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/CS224n-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="CS公开课" scheme="http://example.com/tags/CS%E5%85%AC%E5%BC%80%E8%AF%BE/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络</title>
    <link href="http://example.com/leeml-rnn/"/>
    <id>http://example.com/leeml-rnn/</id>
    <published>2021-10-24T08:15:10.000Z</published>
    <updated>2022-01-07T08:38:44.090Z</updated>
    
    <content type="html"><![CDATA[<p>本文学习的课程是：李宏毅机器学习(2017)-RNN <a href="https://www.bilibili.com/video/BV13x411v7US?p=36">https://www.bilibili.com/video/BV13x411v7US?p=36</a></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>李宏毅机器学习(2017)-RNN <a href="https://www.bilibili.com/video/BV13x411v7US?p=36">https://www.bilibili.com/video/BV13x411v7US?p=36</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文学习的课程是：李宏毅机器学习(2017)-RNN &lt;a href=&quot;https://www.bilibili.com/video/BV13x411v7US?p=36&quot;&gt;https://www.bilibili.com/video/BV13x411v7US?p=36&lt;/a</summary>
      
    
    
    
    <category term="02 人工智能" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="李宏毅机器学习" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="machine learning" scheme="http://example.com/tags/machine-learning/"/>
    
    <category term="rnn" scheme="http://example.com/tags/rnn/"/>
    
  </entry>
  
  <entry>
    <title>Chapter04 PyTorch基础实战——FashionMNIST图像分类</title>
    <link href="http://example.com/pytorch-chap04/"/>
    <id>http://example.com/pytorch-chap04/</id>
    <published>2021-10-14T10:02:55.000Z</published>
    <updated>2022-01-24T15:29:09.094Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第四章-PyTorch基础实战——FashionMNIST图像分类"><a href="#第四章-PyTorch基础实战——FashionMNIST图像分类" class="headerlink" title="第四章 PyTorch基础实战——FashionMNIST图像分类"></a>第四章 PyTorch基础实战——FashionMNIST图像分类</h1><h2 id="数据集和任务介绍"><a href="#数据集和任务介绍" class="headerlink" title="数据集和任务介绍"></a>数据集和任务介绍</h2><p>我们这里的任务是对10个类别的“时装”图像进行分类，使用FashionMNIST数据集。</p><p>FashionMNIST数据集中包含已经预先划分好的训练集和测试集，其中训练集共60,000张图像，测试集共10,000张图像。每张图像均为单通道黑白图像，大小为32*32pixel，分属10个类别。</p><h2 id="导入必要的包"><a href="#导入必要的包" class="headerlink" title="导入必要的包"></a>导入必要的包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br></pre></td></tr></table></figure><h2 id="配置训练环境和超参数"><a href="#配置训练环境和超参数" class="headerlink" title="配置训练环境和超参数"></a>配置训练环境和超参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置GPU，这里有两种方式</span></span><br><span class="line"><span class="comment">## 方案一：使用os.environ</span></span><br><span class="line"><span class="comment"># os.environ[&#x27;CUDA_VISIBLE_DEVICES&#x27;] = &#x27;0&#x27;</span></span><br><span class="line"><span class="comment"># 方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可</span></span><br><span class="line"><span class="comment"># device = torch.device(&quot;cuda:1&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 配置其他超参数，如batch_size, num_workers, learning rate, 以及总的epochs</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">num_workers = <span class="number">4</span></span><br><span class="line">lr = <span class="number">1e-4</span></span><br><span class="line">epochs = <span class="number">20</span></span><br></pre></td></tr></table></figure><h2 id="数据读入和加载"><a href="#数据读入和加载" class="headerlink" title="数据读入和加载"></a>数据读入和加载</h2><p>数据读入有两种方式:</p><ul><li>下载并使用PyTorch提供的内置数据集。这种方式只适用于常见的数据集，如MNIST，CIFAR10等，PyTorch官方提供了数据下载。这种方式往往适用于快速测试方法（比如测试下某个idea在MNIST数据集上是否有效）</li><li>从网站下载以csv格式存储的数据，读入并转成预期的格式。这种数据读入方式需要自己构建Dataset，这对于PyTorch应用于自己的工作中十分重要</li></ul><p>同时，还需要对数据进行必要的变换，比如说需要将图片统一为一致的大小，以便后续能够输入网络训练；需要将数据格式转为Tensor类，等等。这些变换可以很方便地借助torchvision包来完成，torchvision这是PyTorch官方用于图像处理的工具库。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先设置数据变换</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">image_size = <span class="number">28</span></span><br><span class="line">data_transform = transforms.Compose([</span><br><span class="line">    transforms.ToPILImage(),   <span class="comment"># 这一步取决于后续的数据读取方式，如果使用内置数据集则不需要</span></span><br><span class="line">    transforms.Resize(image_size),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>读取方式一：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 读取方式一：使用torchvision自带数据集，下载可能需要一段时间</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">train_data = datasets.FashionMNIST(root=<span class="string">&#x27;./&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=data_transform)</span><br><span class="line">test_data = datasets.FashionMNIST(root=<span class="string">&#x27;./&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=data_transform)</span><br></pre></td></tr></table></figure><p>读取方式二：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 读取方式二：读入csv格式的数据，自行构建Dataset类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FMDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, df, transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.df = df</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.images = df.iloc[:,<span class="number">1</span>:].values.astype(np.uint8)</span><br><span class="line">        self.labels = df.iloc[:, <span class="number">0</span>].values</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.images)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        image = self.images[idx].reshape(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)</span><br><span class="line">        label = <span class="built_in">int</span>(self.labels[idx])</span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            image = torch.tensor(image/<span class="number">255.</span>, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        label = torch.tensor(label, dtype=torch.long)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line">train_df = pd.read_csv(<span class="string">&quot;./FashionMNIST/fashion-mnist_train.csv&quot;</span>)</span><br><span class="line">test_df = pd.read_csv(<span class="string">&quot;./FashionMNIST/fashion-mnist_test.csv&quot;</span>)</span><br><span class="line">train_data = FMDataset(train_df, data_transform)</span><br><span class="line">test_data = FMDataset(test_df, data_transform)</span><br></pre></td></tr></table></figure><blockquote><p>注意：这里需要自己下载数据。可以从kaggle上下载（需科学上网）（但貌似也不是教程用的版本）：<a href="https://www.kaggle.com/zalando-research/fashionmnist/">https://www.kaggle.com/zalando-research/fashionmnist/</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义DataLoader类，以便在训练和测试时加载数据</span></span><br><span class="line">train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers, drop_last=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">image, label = <span class="built_in">next</span>(<span class="built_in">iter</span>(test_loader))</span><br><span class="line"><span class="built_in">print</span>(image.shape, label.shape)</span><br><span class="line">plt.imshow(image[<span class="number">0</span>][<span class="number">0</span>], cmap=<span class="string">&quot;gray&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>这里程序运行了很久，一直跑不出结果，改用了colab</p></blockquote><h2 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h2><p>手搭一个CNN</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.3</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.3</span>)</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="comment"># x = nn.functional.normalize(x)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line"><span class="comment"># model = model.cuda()  # 将模型放到GPU上用于训练</span></span><br><span class="line"><span class="comment"># model = nn.DataParallel(model).cuda()   # 多卡训练时的写法，之后的课程中会进一步讲解</span></span><br></pre></td></tr></table></figure><h2 id="设定损失函数"><a href="#设定损失函数" class="headerlink" title="设定损失函数"></a>设定损失函数</h2><p>使用torch.nn模块自带的CrossEntropy损失。<br>PyTorch会自动把整数型的label转为one-hot型，用于计算CE loss。<br>这里需要确保label是从0开始的，同时模型不加softmax层（使用logits计算）,这也说明了PyTorch训练中各个部分不是独立的，需要通盘考虑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># criterion = nn.CrossEntropyLoss(weight=[1,1,1,1,3,1,1,1,1,1])</span></span><br></pre></td></tr></table></figure><h2 id="设定优化器"><a href="#设定优化器" class="headerlink" title="设定优化器"></a>设定优化器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><h2 id="训练和测试"><a href="#训练和测试" class="headerlink" title="训练和测试"></a>训练和测试</h2><p><strong>训练和测试（验证）</strong><br>各自封装成函数，方便后续调用<br>关注两者的主要区别：</p><ul><li>模型状态设置</li><li>是否需要初始化优化器</li><li>是否需要将loss传回到网络</li><li>是否需要每步更新optimizer</li></ul><p>此外，对于测试或验证过程，可以计算分类准确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># data, label = data.cuda(), label.cuda()  # 不用cuda先</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = criterion(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        train_loss += loss.item()*data.size(<span class="number">0</span>)</span><br><span class="line">    train_loss = train_loss/<span class="built_in">len</span>(train_loader.dataset)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125; \tTraining Loss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, train_loss))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val</span>(<span class="params">epoch</span>):</span>       </span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    val_loss = <span class="number">0</span></span><br><span class="line">    gt_labels = []</span><br><span class="line">    pred_labels = []</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> test_loader:</span><br><span class="line">            <span class="comment"># data, label = data.cuda(), label.cuda()  # 不用cuda先</span></span><br><span class="line">            output = model(data)</span><br><span class="line">            preds = torch.argmax(output, <span class="number">1</span>)</span><br><span class="line">            gt_labels.append(label.cpu().data.numpy())</span><br><span class="line">            pred_labels.append(preds.cpu().data.numpy())</span><br><span class="line">            loss = criterion(output, label)</span><br><span class="line">            val_loss += loss.item()*data.size(<span class="number">0</span>)</span><br><span class="line">    val_loss = val_loss/<span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">    gt_labels, pred_labels = np.concatenate(gt_labels), np.concatenate(pred_labels)</span><br><span class="line">    acc = np.<span class="built_in">sum</span>(gt_labels==pred_labels)/<span class="built_in">len</span>(pred_labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125; \tValidation Loss: &#123;:.6f&#125;, Accuracy: &#123;:6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, val_loss, acc))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>):</span><br><span class="line">    train(epoch)</span><br><span class="line">    val(epoch)</span><br></pre></td></tr></table></figure><p>结果（不是很好）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/lib/python3<span class="number">.7</span>/dist-packages/torch/utils/data/dataloader.py:<span class="number">481</span>: UserWarning: This DataLoader will create <span class="number">4</span> worker processes <span class="keyword">in</span> total. Our suggested <span class="built_in">max</span> number of worker <span class="keyword">in</span> current system <span class="keyword">is</span> <span class="number">2</span>, which <span class="keyword">is</span> smaller than what this DataLoader <span class="keyword">is</span> going to create. Please be aware that excessive worker creation might get DataLoader running slow <span class="keyword">or</span> even freeze, lower the worker number to avoid potential slowness/freeze <span class="keyword">if</span> necessary.</span><br><span class="line">  cpuset_checked))</span><br><span class="line">/usr/local/lib/python3<span class="number">.7</span>/dist-packages/torch/nn/functional.py:<span class="number">718</span>: UserWarning: Named tensors <span class="keyword">and</span> <span class="built_in">all</span> their associated APIs are an experimental feature <span class="keyword">and</span> subject to change. Please do <span class="keyword">not</span> use them <span class="keyword">for</span> anything important until they are released <span class="keyword">as</span> stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:<span class="number">1156.</span>)</span><br><span class="line">  <span class="keyword">return</span> torch.max_pool2d(<span class="built_in">input</span>, kernel_size, stride, padding, dilation, ceil_mode)</span><br><span class="line">Epoch: <span class="number">1</span> Training Loss: <span class="number">1.859782</span></span><br><span class="line">Epoch: <span class="number">1</span> Validation Loss: <span class="number">1.252422</span>, Accuracy: <span class="number">0.504242</span></span><br><span class="line">Epoch: <span class="number">2</span> Training Loss: <span class="number">1.073511</span></span><br><span class="line">Epoch: <span class="number">2</span> Validation Loss: <span class="number">0.958262</span>, Accuracy: <span class="number">0.620891</span></span><br><span class="line">Epoch: <span class="number">3</span> Training Loss: <span class="number">0.912065</span></span><br><span class="line">Epoch: <span class="number">3</span> Validation Loss: <span class="number">0.859967</span>, Accuracy: <span class="number">0.682927</span></span><br><span class="line">Epoch: <span class="number">4</span> Training Loss: <span class="number">0.803673</span></span><br><span class="line">Epoch: <span class="number">4</span> Validation Loss: <span class="number">0.725328</span>, Accuracy: <span class="number">0.743902</span></span><br><span class="line">Epoch: <span class="number">5</span> Training Loss: <span class="number">0.723244</span></span><br><span class="line">Epoch: <span class="number">5</span> Validation Loss: <span class="number">0.699738</span>, Accuracy: <span class="number">0.725345</span></span><br><span class="line">Epoch: <span class="number">6</span> Training Loss: <span class="number">0.676728</span></span><br><span class="line">Epoch: <span class="number">6</span> Validation Loss: <span class="number">0.688325</span>, Accuracy: <span class="number">0.742312</span></span><br><span class="line">Epoch: <span class="number">7</span> Training Loss: <span class="number">0.624213</span></span><br><span class="line">Epoch: <span class="number">7</span> Validation Loss: <span class="number">0.633743</span>, Accuracy: <span class="number">0.744963</span></span><br><span class="line">Epoch: <span class="number">8</span> Training Loss: <span class="number">0.595873</span></span><br><span class="line">Epoch: <span class="number">8</span> Validation Loss: <span class="number">0.588029</span>, Accuracy: <span class="number">0.770414</span></span><br><span class="line">Epoch: <span class="number">9</span> Training Loss: <span class="number">0.561574</span></span><br><span class="line">Epoch: <span class="number">9</span> Validation Loss: <span class="number">0.578903</span>, Accuracy: <span class="number">0.765642</span></span><br><span class="line">Epoch: <span class="number">10</span> Training Loss: <span class="number">0.544152</span></span><br><span class="line">Epoch: <span class="number">10</span> Validation Loss: <span class="number">0.563249</span>, Accuracy: <span class="number">0.791622</span></span><br><span class="line">Epoch: <span class="number">11</span> Training Loss: <span class="number">0.532662</span></span><br><span class="line">Epoch: <span class="number">11</span> Validation Loss: <span class="number">0.561163</span>, Accuracy: <span class="number">0.790032</span></span><br><span class="line">Epoch: <span class="number">12</span> Training Loss: <span class="number">0.520769</span></span><br><span class="line">Epoch: <span class="number">12</span> Validation Loss: <span class="number">0.560051</span>, Accuracy: <span class="number">0.783139</span></span><br><span class="line">Epoch: <span class="number">13</span> Training Loss: <span class="number">0.495388</span></span><br><span class="line">Epoch: <span class="number">13</span> Validation Loss: <span class="number">0.537520</span>, Accuracy: <span class="number">0.794804</span></span><br><span class="line">Epoch: <span class="number">14</span> Training Loss: <span class="number">0.461928</span></span><br><span class="line">Epoch: <span class="number">14</span> Validation Loss: <span class="number">0.533855</span>, Accuracy: <span class="number">0.799046</span></span><br><span class="line">Epoch: <span class="number">15</span> Training Loss: <span class="number">0.453786</span></span><br><span class="line">Epoch: <span class="number">15</span> Validation Loss: <span class="number">0.534338</span>, Accuracy: <span class="number">0.805408</span></span><br><span class="line">Epoch: <span class="number">16</span> Training Loss: <span class="number">0.457692</span></span><br><span class="line">Epoch: <span class="number">16</span> Validation Loss: <span class="number">0.515626</span>, Accuracy: <span class="number">0.812831</span></span><br><span class="line">Epoch: <span class="number">17</span> Training Loss: <span class="number">0.449596</span></span><br><span class="line">Epoch: <span class="number">17</span> Validation Loss: <span class="number">0.504590</span>, Accuracy: <span class="number">0.816013</span></span><br><span class="line">Epoch: <span class="number">18</span> Training Loss: <span class="number">0.443980</span></span><br><span class="line">Epoch: <span class="number">18</span> Validation Loss: <span class="number">0.503526</span>, Accuracy: <span class="number">0.818134</span></span><br><span class="line">Epoch: <span class="number">19</span> Training Loss: <span class="number">0.420621</span></span><br><span class="line">Epoch: <span class="number">19</span> Validation Loss: <span class="number">0.488520</span>, Accuracy: <span class="number">0.826087</span></span><br><span class="line">Epoch: <span class="number">20</span> Training Loss: <span class="number">0.418917</span></span><br><span class="line">Epoch: <span class="number">20</span> Validation Loss: <span class="number">0.524965</span>, Accuracy: <span class="number">0.797985</span></span><br></pre></td></tr></table></figure><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>程序的colab链接：<a href="https://colab.research.google.com/drive/1kvaBEEgQ_a5G5xOHUe5ih4Qq8L5C9zYY?usp=sharing">https://colab.research.google.com/drive/1kvaBEEgQ_a5G5xOHUe5ih4Qq8L5C9zYY?usp=sharing</a></li><li>Datawhale开源项目：深入浅出PyTorch <a href="https://github.com/datawhalechina/thorough-pytorch/">https://github.com/datawhalechina/thorough-pytorch/</a></li><li>李宏毅机器学习2021春-PyTorch Tutorial <a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=5">https://www.bilibili.com/video/BV1Wv411h7kN?p=5</a></li><li>动手学深度学习pytorch版 <a href="https://zh-v2.d2l.ai/chapter_preface/index.html">https://zh-v2.d2l.ai/chapter_preface/index.html</a></li><li>PyTorch官方教程中文版 <a href="https://pytorch123.com/SecondSection/training_a_classifier/">https://pytorch123.com/SecondSection/training_a_classifier/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;第四章-PyTorch基础实战——FashionMNIST图像分类&quot;&gt;&lt;a href=&quot;#第四章-PyTorch基础实战——FashionMNIST图像分类&quot; class=&quot;headerlink&quot; title=&quot;第四章 PyTorch基础实战——FashionMN</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="2021-10 深入浅出PyTorch" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/2021-10-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAPyTorch/"/>
    
    
    <category term="PyTorch" scheme="http://example.com/tags/PyTorch/"/>
    
    <category term="team learning" scheme="http://example.com/tags/team-learning/"/>
    
    <category term="note" scheme="http://example.com/tags/note/"/>
    
  </entry>
  
  <entry>
    <title>Lecture01 机器学习和深度学习简介</title>
    <link href="http://example.com/leeml-lec01-introduction/"/>
    <id>http://example.com/leeml-lec01-introduction/</id>
    <published>2021-10-12T05:40:12.000Z</published>
    <updated>2021-11-24T09:54:31.117Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文学习的课程是：李宏毅2021春机器学习课程 Lecture01 机器学习和深度学习简介 <a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=2">https://www.bilibili.com/video/BV1Wv411h7kN?p=2</a></p><h1 id="李宏毅2021春机器学习课程大纲"><a href="#李宏毅2021春机器学习课程大纲" class="headerlink" title="李宏毅2021春机器学习课程大纲"></a>李宏毅2021春机器学习课程大纲</h1><p>第一节 Introduction  作业 HW1: Regression<br>第二节 Deep Learning  作业 HW2: Classification<br>第三节 Self-Attention  作业 HW3: CNN HW4: Self-Attention<br>第四节 Theory of ML<br>第五节 Transformer  作业 HW5: Transformer<br>第六节 Generative Model  作业 HW6: GAN<br>第七节 Self-Supervised Learning  作业 HW7: BERT HW8: Autoencoder<br>第八节 Explainable AI / Adversarial Attack  作业 HW9: Explainable AI HW10: Adversarial Attack<br>第九节 Domain Adaptation/ RL  作业 HW11: Adaptation<br>第十节 RL  作业 HW12: RL<br>第十一节  Privacy v.s. ML<br>第十二节  Quantum ML<br>第十三节  Life-Long/Compression  作业 HW13: Life-Long HW14: Compression<br>第十四节  Meta Learning  作业 HW15: Meta Learning</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/00_lec_schedule.jpg?raw=true" width="600" alt="" align="center" /><h1 id="Buffet-Style-Learning-自助式学习"><a href="#Buffet-Style-Learning-自助式学习" class="headerlink" title="Buffet Style Learning/自助式学习"></a>Buffet Style Learning/自助式学习</h1><blockquote><p>Everyone can take this course.</p><p>You decide how much you want to learn.</p><p>You decide how deep you want to learn.</p></blockquote><h1 id="机器学习基本概念简介"><a href="#机器学习基本概念简介" class="headerlink" title="机器学习基本概念简介"></a>机器学习基本概念简介</h1><p>Machine Learning 约等于 Looking for Function</p><h2 id="不同类型的函数"><a href="#不同类型的函数" class="headerlink" title="不同类型的函数"></a>不同类型的函数</h2><p>Regression/回归：函数的输出是数值。</p><p>Classification/分类：给定选项（类别），输出正确的类别。</p><p>Structured Learning：create something with structure (image, document etc.)</p><h2 id="How-to-find-a-function-A-case"><a href="#How-to-find-a-function-A-case" class="headerlink" title="How to find a function: A case"></a>How to find a function: A case</h2><p>Case: Predict YouTobe views</p><p>模型的训练：</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/05_framework.jpg?raw=true" width="400" alt="" align="center" /><h3 id="Step1-Function-with-Unknown-Parameters"><a href="#Step1-Function-with-Unknown-Parameters" class="headerlink" title="Step1. Function with Unknown Parameters"></a>Step1. Function with Unknown Parameters</h3><p>基于领域知识写出一个带有位置参数的函数</p><p><code>y = b + wx1</code> w: weight, b: bias </p><h3 id="Step2-Define-Loss-from-Training-Data"><a href="#Step2-Define-Loss-from-Training-Data" class="headerlink" title="Step2. Define Loss from Training Data"></a>Step2. Define Loss from Training Data</h3><p>Loss is a function of parameters. <code>L(b, w)</code></p><p>Loss is how good a set of values is.</p><h3 id="Step3-Optimization"><a href="#Step3-Optimization" class="headerlink" title="Step3. Optimization"></a>Step3. Optimization</h3><p><code>w*, b* = arg min L(w, b)</code></p><p>Gradient Descent/梯度下降（1个参数，多参数同理）</p><ul><li><p>随机选取一个初始点w0</p></li><li><p>计算w0对L的微分。learning rate/学习率。</p><ul><li>Negative: increase w</li><li>Positive: decrease w</li></ul></li><li><p>Update w iteratively/迭代更新w</p></li></ul><h3 id="Training-Step1-3"><a href="#Training-Step1-3" class="headerlink" title="Training: Step1-3"></a>Training: Step1-3</h3><h1 id="深度学习基本概念简介"><a href="#深度学习基本概念简介" class="headerlink" title="深度学习基本概念简介"></a>深度学习基本概念简介</h1><h2 id="Regression-回归，梦开始的地方"><a href="#Regression-回归，梦开始的地方" class="headerlink" title="Regression/回归，梦开始的地方"></a>Regression/回归，梦开始的地方</h2><h3 id="如何逼近复杂的曲线？——Sigmoid-Function"><a href="#如何逼近复杂的曲线？——Sigmoid-Function" class="headerlink" title="如何逼近复杂的曲线？——Sigmoid Function"></a>如何逼近复杂的曲线？——Sigmoid Function</h3><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/01_sigmoid.jpg?raw=true" width="400" alt="" align="center" /><p>用多段sigmoid逼近复杂函数：</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/02_sigmoid.jpg?raw=true" width="400" alt="" align="center" /><p>给sigmoid加更多的feature（就是多段 <code>y=b+wx</code>组合起来）：</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/03_sigmoid.jpg?raw=true" width="400" alt="" align="center" /><p>sigmoid的矩阵运算：</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/04_sigmoid.jpg?raw=true" width="400" alt="" align="center" /><h3 id="Loss-损失函数"><a href="#Loss-损失函数" class="headerlink" title="Loss/损失函数"></a>Loss/损失函数</h3><p>什么是损失函数：</p><ul><li>一个关于参数的函数<code>L(𝜃)</code></li><li>衡量这一组模型参数的效果</li></ul><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/06_loss.jpg?raw=true" width="400" alt="" align="center" /><h3 id="Optimization-优化器"><a href="#Optimization-优化器" class="headerlink" title="Optimization/优化器"></a>Optimization/优化器</h3><p>使用梯度下降更新参数：</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/07_optm.jpg?raw=true" width="400" alt="" align="center" /><p>一些概念：</p><ul><li><p>Batch: 数据包</p></li><li><p>Update：每次更新一次参数</p></li><li><p>Epoch：所有batch过一遍</p></li></ul><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/08_optm.jpg?raw=true" width="400" alt="" align="center" /><h3 id="Activation-function-激活函数"><a href="#Activation-function-激活函数" class="headerlink" title="Activation function/激活函数"></a>Activation function/激活函数</h3><ul><li>sigmoid</li><li>ReLU</li></ul><h3 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h3><p>A fancy name: Deep Learning. </p><blockquote><p>It is just so deep, right?</p></blockquote><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/09_dl.jpg?raw=true" width="400" alt="" align="center" /><p>Go deeper and deeper:</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/10_dl.jpg?raw=true" width="400" alt="" align="center" /><p>Think: Why deep network, not fat network?</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/11_dl.jpg?raw=true" width="400" alt="" align="center" /><h2 id="选修-深度学习简介"><a href="#选修-深度学习简介" class="headerlink" title="(选修)深度学习简介"></a>(选修)深度学习简介</h2><h3 id="Deep-Learning的发展历程"><a href="#Deep-Learning的发展历程" class="headerlink" title="Deep Learning的发展历程"></a>Deep Learning的发展历程</h3><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/12_dl.jpg?raw=true" width="400" alt="" align="center" /><h3 id="Deep-Learning的三个步骤"><a href="#Deep-Learning的三个步骤" class="headerlink" title="Deep Learning的三个步骤"></a>Deep Learning的三个步骤</h3><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/13_dl.jpg?raw=true" width="400" alt="" align="center" /><h3 id="Full-Connect-Feedforward-Network-全连接前馈神经网络"><a href="#Full-Connect-Feedforward-Network-全连接前馈神经网络" class="headerlink" title="Full Connect Feedforward Network/全连接前馈神经网络"></a>Full Connect Feedforward Network/全连接前馈神经网络</h3><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/14_dl.jpg?raw=true" width="400" alt="" align="center" /><h3 id="Matrix-Operation-矩阵运算"><a href="#Matrix-Operation-矩阵运算" class="headerlink" title="Matrix Operation/矩阵运算"></a>Matrix Operation/矩阵运算</h3><p>参数矩阵：</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/15_dl.jpg?raw=true" width="400" alt="" align="center" /><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/16_dl.jpg?raw=true" width="400" alt="" align="center" /><h3 id="Output-Layer"><a href="#Output-Layer" class="headerlink" title="Output Layer"></a>Output Layer</h3><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/17_dl.jpg?raw=true" width="400" alt="" align="center" /><h3 id="Example-Application"><a href="#Example-Application" class="headerlink" title="Example Application"></a>Example Application</h3><p>手写识别案例：</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/18_dl.jpg?raw=true" width="400" alt="" align="center" /><p>You need to decide the network structure to let a good function in your function set.</p><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>计算损失函数：</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/19_dl.jpg?raw=true" width="400" alt="" align="center" /><p>找一个最小化损失函数的函数，即找一组参数可以最小化损失函数：</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/leeml/20_dl.jpg?raw=true" width="400" alt="" align="center" /><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>如何寻找这样的参数？——梯度下降。</p><h3 id="Backpropagation-反向传播"><a href="#Backpropagation-反向传播" class="headerlink" title="Backpropagation/反向传播"></a>Backpropagation/反向传播</h3><p>如何计算梯度？——反向传播算法。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>李宏毅2021春机器学习课程 <a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=2">https://www.bilibili.com/video/BV1Wv411h7kN?p=2</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文学习的课程是：李宏毅2021春机器学习课程 Lecture01 机器学习和深度学习简介 &lt;a href=&quot;https://www.bil</summary>
      
    
    
    
    <category term="02 人工智能" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="李宏毅机器学习" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="machine learning" scheme="http://example.com/tags/machine-learning/"/>
    
    <category term="regression" scheme="http://example.com/tags/regression/"/>
    
  </entry>
  
  <entry>
    <title>Summary Transformer课程总结</title>
    <link href="http://example.com/nlp-transformer-summary/"/>
    <id>http://example.com/nlp-transformer-summary/</id>
    <published>2021-09-30T07:44:01.000Z</published>
    <updated>2022-01-24T15:28:06.964Z</updated>
    
    <content type="html"><![CDATA[<h1 id="我的背景"><a href="#我的背景" class="headerlink" title="我的背景"></a>我的背景</h1><p>第一次参加Datawhale组队学习课程，我的相关知识背景是：</p><ul><li>Transformer：0基础</li><li>PyTorch：0基础</li><li>NLP：0.1基础</li><li>Python：0基础</li></ul><p>作为NLP情感分析的领航员和Transformers的学员，我将从课程内容和运营两方面写一下自己的感受和想法。</p><h1 id="Transformer课程大纲"><a href="#Transformer课程大纲" class="headerlink" title="Transformer课程大纲"></a>Transformer课程大纲</h1><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/transformer_xmind.png?raw=true" width="800" alt="基于Transformers的自然语言处理" align="center" /><h1 id="课程内容方面"><a href="#课程内容方面" class="headerlink" title="课程内容方面"></a>课程内容方面</h1><ul><li>课程内容对零基础入门的人是比较友好的。比如我是第一次学习Transformer，但是图解系列很容易理解。</li><li>但是每个task的内容难度差别过大。比如Task02的Transformer原论文代码标注 ，和Task05 transformers源码讲解。</li><li>每个task的工作量不平衡，有的特别多，有的相对少。</li><li>第四章可以任选一个任务应用，把更多时间留给学习如何使用huggingface的transformers。（就是那个官方课程）</li></ul><h1 id="课程运营方面"><a href="#课程运营方面" class="headerlink" title="课程运营方面"></a>课程运营方面</h1><ul><li>优秀队员和优秀队长评选标准需要统一</li><li>补卡规则需要统一</li><li>逐步完善课程体系</li><li>小程序，用起来不是很方便</li><li>（脑洞1）每次打卡之后马上进行作业评审和反馈（过于耗费助教精力）</li><li>（脑洞2）给每个小组或个人安排一个期末大project，或者布置平时作业（过于耗费学员精力）</li><li>（脑洞3）直接用一个课程管理系统进行管理（类似于canvas,雨课堂）(逐渐学院化)</li></ul><h1 id="参考资料清单（总）"><a href="#参考资料清单（总）" class="headerlink" title="参考资料清单（总）"></a>参考资料清单（总）</h1><p>Transformer在网上有很多很多教程，其中公认的、普遍性的比较好的资料如下：</p><p><strong>理论部分</strong><br>[1] (强推)李宏毅2021春机器学习课程 <a href="https://www.bilibili.com/video/BV1Wv411h7kN?from=search&seid=17090062977285779802&spm_id_from=333.337.0.0">https://www.bilibili.com/video/BV1Wv411h7kN?from=search&amp;seid=17090062977285779802&amp;spm_id_from=333.337.0.0</a><br>[2] <strong>基于transformers的自然语言处理(NLP)入门（涵盖了图解系列、annotated transformer、huggingface）</strong> <a href="https://github.com/datawhalechina/learn-nlp-with-transformers">https://github.com/datawhalechina/learn-nlp-with-transformers</a><br>[3] 图解transformer|The Illustrated Transformer <a href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a><br>[4] 图解seq2seq, attention|Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a></p><p><strong>代码部分</strong><br>[5] The Annotated Transformer <a href="http://nlp.seas.harvard.edu//2018/04/03/attention.html">http://nlp.seas.harvard.edu//2018/04/03/attention.html</a><br>[6] Huggingface/transformers <a href="https://github.com/huggingface/transformers/blob/master/README_zh-hans.md">https://github.com/huggingface/transformers/blob/master/README_zh-hans.md</a></p><p><strong>论文部分</strong><br>Attention is all “we” need.</p><p><strong>其他不错的博客或教程</strong><br>[7] 基于transformers的自然语言处理(NLP)入门–在线阅读 <a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/">https://datawhalechina.github.io/learn-nlp-with-transformers/#/</a><br>[8] 李宏毅2021春机器学习课程笔记——自注意力机制 <a href="https://www.cnblogs.com/sykline/p/14730088.html">https://www.cnblogs.com/sykline/p/14730088.html</a><br>[9] 李宏毅2021春机器学习课程笔记——Transformer模型 <a href="https://www.cnblogs.com/sykline/p/14785552.html">https://www.cnblogs.com/sykline/p/14785552.html</a><br>[10] 李宏毅机器学习学习笔记——自注意力机制 <a href="https://blog.csdn.net/p_memory/article/details/116271274">https://blog.csdn.net/p_memory/article/details/116271274</a><br>[11] 车万翔-自然语言处理新范式：基于预训练的方法【讲座+PPT】 <a href="https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true">https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true</a><br>[12] 苏剑林-《Attention is All You Need》浅读（简介+代码）<a href="https://spaces.ac.cn/archives/4765">https://spaces.ac.cn/archives/4765</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;我的背景&quot;&gt;&lt;a href=&quot;#我的背景&quot; class=&quot;headerlink&quot; title=&quot;我的背景&quot;&gt;&lt;/a&gt;我的背景&lt;/h1&gt;&lt;p&gt;第一次参加Datawhale组队学习课程，我的相关知识背景是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformer：0基础&lt;/</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="2021-09 基于transformer的NLP" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/2021-09-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"/>
    
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="预训练模型" scheme="http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="transfomer" scheme="http://example.com/tags/transfomer/"/>
    
    <category term="总结" scheme="http://example.com/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>Chapter03 PyTorch的主要组成模块</title>
    <link href="http://example.com/pytorch-chap03/"/>
    <id>http://example.com/pytorch-chap03/</id>
    <published>2021-09-28T01:31:42.000Z</published>
    <updated>2022-01-24T15:29:02.544Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第三章-PyTorch的主要组成模块"><a href="#第三章-PyTorch的主要组成模块" class="headerlink" title="第三章 PyTorch的主要组成模块"></a>第三章 PyTorch的主要组成模块</h1><h2 id="完成深度学习的必要部分"><a href="#完成深度学习的必要部分" class="headerlink" title="完成深度学习的必要部分"></a>完成深度学习的必要部分</h2><p>机器学习：</p><ol><li>数据预处理（数据格式、数据转换、划分数据集）</li><li>选择模型，设定损失和优化函数，设置超参数</li><li>训练模型，拟合训练集</li><li>评估模型，在并在验证集/测试集上计算模型表现</li></ol><p>深度学习的注意事项：</p><ol><li>数据预处理（数据加载、批处理）</li><li>逐层搭建模型，组装不同模块</li><li>GPU的配置和操作</li></ol><img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/11_dnn.jpg?raw=true" width="600" alt="" align="center" /><h2 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h2><p>导入必须的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optimizer</span><br></pre></td></tr></table></figure><p>超参数设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">16</span>  <span class="comment"># batch size</span></span><br><span class="line">lr = <span class="number">1e-4</span>  <span class="comment"># 初始学习率</span></span><br><span class="line">max_epochs = <span class="number">100</span>  <span class="comment"># 训练次数 </span></span><br></pre></td></tr></table></figure><p>GPU的设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方案一：使用os.environ，这种情况如果使用GPU不需要设置</span></span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;0,1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:1&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="数据加载和处理"><a href="#数据加载和处理" class="headerlink" title="数据加载和处理"></a>数据加载和处理</h2><p>PyTorch数据读入是通过Dataset+Dataloader的方式完成的，Dataset定义好数据的格式和数据变换形式，Dataloader用iterative的方式不断读入批次数据。</p><p>我们可以定义自己的Dataset类来实现灵活的数据读取，定义的类需要继承PyTorch自身的Dataset类。主要包含三个函数：</p><ul><li><code>__init__</code>: 用于向类中传入外部参数，同时定义样本集</li><li><code>__getitem__</code>: 用于逐个读取样本集合中的元素，可以进行一定的变换，并将返回训练/验证所需的数据</li><li><code>__len__</code>: 用于返回数据集的样本数</li></ul><img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/12_dataset.jpg?raw=true" width="600" alt="" align="center" /><ul><li>batch_size：样本是按“批”读入的，batch_size就是每次读入的样本数</li><li>num_workers：有多少个进程用于读取数据</li><li>shuffle：是否将读入的数据打乱</li><li>drop_last：对于样本最后一部分没有达到批次数的样本，不再参与训练</li></ul><p>下面是本部分代码在notebook中的运行情况。主要参考 PyTorch官方教程中文版 <a href="https://pytorch123.com/SecondSection/training_a_classifier/">https://pytorch123.com/SecondSection/training_a_classifier/</a></p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/chap03.jpg?raw=true" width="" alt="" align="center" /><h2 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h2><h3 id="神经网络的构造"><a href="#神经网络的构造" class="headerlink" title="神经网络的构造"></a>神经网络的构造</h3><p>PyTorch中神经网络构造一般是基于 Module 类的模型来完成的。Module 类是 nn 模块里提供的一个模型构造类，是所有神经⽹网络模块的基类，我们可以继承它来定义我们想要的模型。下面继承 Module 类构造多层感知机（MLP）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">    <span class="comment"># 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例例时还可以指定其他函数</span></span><br><span class="line">    <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br><span class="line">    self.hidden = nn.Linear(<span class="number">784</span>, <span class="number">256</span>)</span><br><span class="line">    self.act = nn.ReLU()</span><br><span class="line">    self.output = nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">   <span class="comment"># 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    o = self.act(self.hidden(x))</span><br><span class="line">    <span class="keyword">return</span> self.output(o)</span><br></pre></td></tr></table></figure><p>我们可以实例化 MLP 类得到模型变量 net 。下⾯的代码初始化 net 并传入输⼊数据 X 做一次前向计算。其中， net(X) 会调用 MLP 继承⾃自 Module 类的 <strong>call</strong> 函数，这个函数将调⽤用 MLP 类定义的forward 函数来完成前向计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = torch.rand(<span class="number">2</span>, <span class="number">784</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X</span><br><span class="line">tensor([[<span class="number">0.3277</span>, <span class="number">0.2204</span>, <span class="number">0.5239</span>,  ..., <span class="number">0.4333</span>, <span class="number">0.1906</span>, <span class="number">0.1318</span>],</span><br><span class="line">        [<span class="number">0.9850</span>, <span class="number">0.2121</span>, <span class="number">0.8405</span>,  ..., <span class="number">0.3796</span>, <span class="number">0.2717</span>, <span class="number">0.5553</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"><span class="meta">... </span>  <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line"><span class="meta">... </span>  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="comment"># 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例例时还可以指定其他函数</span></span><br><span class="line"><span class="meta">... </span>    <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br><span class="line"><span class="meta">... </span>    self.hidden = nn.Linear(<span class="number">784</span>, <span class="number">256</span>)</span><br><span class="line"><span class="meta">... </span>    self.act = nn.ReLU()</span><br><span class="line"><span class="meta">... </span>    self.output = nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line"><span class="meta">... </span>    </span><br><span class="line"><span class="meta">... </span>   <span class="comment"># 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出</span></span><br><span class="line"><span class="meta">... </span>  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line"><span class="meta">... </span>    o = self.act(self.hidden(x))</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> self.output(o)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = MLP()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net</span><br><span class="line">MLP(</span><br><span class="line">  (hidden): Linear(in_features=<span class="number">784</span>, out_features=<span class="number">256</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (act): ReLU()</span><br><span class="line">  (output): Linear(in_features=<span class="number">256</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net(X)</span><br><span class="line">tensor([[ <span class="number">0.1317</span>,  <span class="number">0.0702</span>,  <span class="number">0.1707</span>, -<span class="number">0.0081</span>, -<span class="number">0.2730</span>,  <span class="number">0.2837</span>,  <span class="number">0.0700</span>,  <span class="number">0.1718</span>,</span><br><span class="line">          <span class="number">0.0299</span>,  <span class="number">0.2082</span>],</span><br><span class="line">        [ <span class="number">0.1094</span>,  <span class="number">0.0936</span>,  <span class="number">0.2474</span>, -<span class="number">0.0139</span>, -<span class="number">0.1861</span>,  <span class="number">0.1846</span>,  <span class="number">0.1658</span>,  <span class="number">0.2051</span>,</span><br><span class="line">          <span class="number">0.2609</span>,  <span class="number">0.2227</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br></pre></td></tr></table></figure><h3 id="神经网络中常见的层"><a href="#神经网络中常见的层" class="headerlink" title="神经网络中常见的层"></a>神经网络中常见的层</h3><h4 id="不含模型参数的层"><a href="#不含模型参数的层" class="headerlink" title="不含模型参数的层"></a>不含模型参数的层</h4><p>下⾯构造的 MyLayer 类通过继承 Module 类自定义了一个<strong>将输入减掉均值后输出</strong>的层。这个层里不含模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">MyLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line"><span class="meta">... </span>        <span class="built_in">super</span>(MyLayer, self).__init__(**kwargs)</span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line"><span class="meta">... </span>        <span class="keyword">return</span> x - x.mean()  </span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>layer = MyLayer()  <span class="comment"># 实例化该层</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>layer</span><br><span class="line">MyLayer()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>layer(torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.<span class="built_in">float</span>))</span><br><span class="line">tensor([-<span class="number">2.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>])</span><br></pre></td></tr></table></figure><h4 id="含模型参数的层"><a href="#含模型参数的层" class="headerlink" title="含模型参数的层"></a>含模型参数的层</h4><p>我们还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。</p><p>Parameter 类其实是 Tensor 的子类，如果一 个 Tensor 是 Parameter ，那么它会⾃动被添加到模型的参数列表里。所以在⾃定义含模型参数的层时，我们应该将参数定义成 Parameter ，除了直接定义成 Parameter 类外，还可以使⽤ ParameterList 和 ParameterDict 分别定义参数的列表和字典。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyListDense</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyListDense, self).__init__()</span><br><span class="line">        self.params = nn.ParameterList([nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)])</span><br><span class="line">        self.params.append(nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.params)):</span><br><span class="line">            x = torch.mm(x, self.params[i])</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">      </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = MyListDense()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(net)</span><br><span class="line">MyListDense(</span><br><span class="line">  (params): ParameterList(</span><br><span class="line">      (<span class="number">0</span>): Parameter containing: [torch.FloatTensor of size 4x4]</span><br><span class="line">      (<span class="number">1</span>): Parameter containing: [torch.FloatTensor of size 4x4]</span><br><span class="line">      (<span class="number">2</span>): Parameter containing: [torch.FloatTensor of size 4x4]</span><br><span class="line">      (<span class="number">3</span>): Parameter containing: [torch.FloatTensor of size 4x1]</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDictDense</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyDictDense, self).__init__()</span><br><span class="line">        self.params = nn.ParameterDict(&#123;</span><br><span class="line">                <span class="string">&#x27;linear1&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)),</span><br><span class="line">                <span class="string">&#x27;linear2&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        self.params.update(&#123;<span class="string">&#x27;linear3&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">2</span>))&#125;) <span class="comment"># 新增</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, choice=<span class="string">&#x27;linear1&#x27;</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.mm(x, self.params[choice])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = MyDictDense()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(net)</span><br><span class="line">MyDictDense(</span><br><span class="line">  (params): ParameterDict(</span><br><span class="line">      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]</span><br><span class="line">      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]</span><br><span class="line">      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>下面给出常见的神经网络的一些层，比如卷积层、池化层，以及较为基础的AlexNet，LeNet等。</p><h4 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h4><p>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积运算（二维互相关）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span>(<span class="params">X, K</span>):</span> </span><br><span class="line">    h, w = K.shape</span><br><span class="line">    X, K = X.<span class="built_in">float</span>(), K.<span class="built_in">float</span>()</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i: i + h, j: j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维卷积层</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, kernel_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Conv2D, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure><p>填充(padding)是指在输⼊入⾼高和宽的两侧填充元素(通常是0元素)。</p><p>在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下 的顺序，依次在输⼊数组上滑动。我们将每次滑动的行数和列数称为步幅(stride)。</p><p>（skip）</p><h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>池化层每次对输入数据的一个固定形状窗口(⼜称池化窗口)中的元素计算输出。不同于卷积层里计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也 分别叫做最大池化或平均池化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span></span><br><span class="line"><span class="meta">... </span>    p_h, p_w = pool_size</span><br><span class="line"><span class="meta">... </span>    Y = np.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line"><span class="meta">... </span>        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line"><span class="meta">... </span>            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line"><span class="meta">... </span>                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line"><span class="meta">... </span>            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line"><span class="meta">... </span>                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> Y</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pool2d(X, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">array([[<span class="number">4.</span>, <span class="number">5.</span>],</span><br><span class="line">       [<span class="number">7.</span>, <span class="number">8.</span>]])</span><br></pre></td></tr></table></figure><h4 id="模型示例：LeNet"><a href="#模型示例：LeNet" class="headerlink" title="模型示例：LeNet"></a>模型示例：LeNet</h4><p>（待补充）</p><h4 id="模型示例：AlexNet"><a href="#模型示例：AlexNet" class="headerlink" title="模型示例：AlexNet"></a>模型示例：AlexNet</h4><p>（待补充）</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>一个好的训练离不开优质的负反馈，这里的损失函数就是模型的负反馈。</p><p>这里将列出PyTorch中常用的损失函数（一般通过torch.nn调用），并详细介绍每个损失函数的功能介绍、数学公式和调用代码。</p><h3 id="二分类交叉熵损失函数"><a href="#二分类交叉熵损失函数" class="headerlink" title="二分类交叉熵损失函数"></a>二分类交叉熵损失函数</h3><p><code>torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;)</code></p><p><strong>功能</strong>：计算二分类任务时的交叉熵（Cross Entropy）函数。在二分类中，label是{0,1}。对于进入交叉熵函数的input为概率分布的形式。一般来说，input为sigmoid激活层的输出，或者softmax的输出。</p><p><strong>主要参数</strong>：</p><ul><li><code>weight</code>:每个类别的loss设置权值</li><li><code>size_average</code>:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和.</li><li><code>reduce</code>:数据类型为bool，为True时，loss的返回是标量。</li></ul><h3 id="其他损失函数"><a href="#其他损失函数" class="headerlink" title="其他损失函数"></a>其他损失函数</h3><p>交叉熵损失函数</p><p>L1损失函数</p><p>MSE损失函数</p><p>平滑L1 (Smooth L1)损失函数</p><p>目标泊松分布的负对数似然损失</p><p>KL散度</p><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><h3 id="什么是优化器"><a href="#什么是优化器" class="headerlink" title="什么是优化器"></a>什么是优化器</h3><p>深度学习的目标是通过不断改变网络参数，使得参数能够对输入做各种非线性变换拟合输出，本质上就是一个函数去寻找最优解，只不过这个最优解使一个矩阵。那么我们如何计算出来这么多的系数，有以下两种方法：</p><ol><li>第一种是最直接的暴力穷举一遍参数，这种方法的实施可能性基本为0，堪比愚公移山plus的难度。</li><li>为了使求解参数过程更加快，人们提出了第二种办法，即就是是BP+优化器逼近求解。</li></ol><p>因此，优化器就是根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值，使得模型输出更加接近真实标签。</p><h3 id="PyTorch提供的优化器"><a href="#PyTorch提供的优化器" class="headerlink" title="PyTorch提供的优化器"></a>PyTorch提供的优化器</h3><p>Pytorch很人性化的给我们提供了一个优化器的库torch.optim，在这里面给我们提供了十种优化器。</p><ul><li>torch.optim.ASGD</li><li>torch.optim.Adadelta</li><li>torch.optim.Adagrad</li><li>torch.optim.Adam</li><li>torch.optim.AdamW</li><li>torch.optim.Adamax</li><li>torch.optim.LBFGS</li><li>torch.optim.RMSprop</li><li>torch.optim.Rprop</li><li>torch.optim.SGD</li><li>torch.optim.SparseAdam</li></ul><h2 id="训练与评估"><a href="#训练与评估" class="headerlink" title="训练与评估"></a>训练与评估</h2><p>完成了上述设定后就可以加载数据开始训练模型了。首先应该设置模型的状态：如果是训练状态，那么模型的参数应该支持反向传播的修改；如果是验证/测试状态，则不应该修改模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.train()   <span class="comment"># 训练状态</span></span><br><span class="line">model.<span class="built_in">eval</span>()   <span class="comment"># 验证/测试状态</span></span><br></pre></td></tr></table></figure><p>训练过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_loader:  <span class="comment"># 此时要用for循环读取DataLoader中的全部数据。</span></span><br><span class="line">        data, label = data.cuda(), label.cuda()  <span class="comment"># 之后将数据放到GPU上用于后续计算，此处以.cuda()为例</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 开始用当前批次数据做训练时，应当先将优化器的梯度置零</span></span><br><span class="line">        output = model(data)  <span class="comment"># 之后将data送入模型中训练</span></span><br><span class="line">        loss = criterion(label, output)   <span class="comment"># 根据预先定义的criterion计算损失函数</span></span><br><span class="line">        loss.backward()  <span class="comment"># 将loss反向传播回网络</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 使用优化器更新模型参数</span></span><br><span class="line">        train_loss += loss.item()*data.size(<span class="number">0</span>)</span><br><span class="line">    train_loss = train_loss/<span class="built_in">len</span>(train_loader.dataset)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125; \tTraining Loss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, train_loss))</span><br></pre></td></tr></table></figure><p>验证/测试的流程基本与训练过程一致，不同点在于：</p><ul><li>需要预先设置torch.no_grad，以及将model调至eval模式</li><li>不需要将优化器的梯度置零</li><li>不需要将loss反向回传到网络</li><li>不需要更新optimizer</li></ul><p>验证/测试过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val</span>(<span class="params">epoch</span>):</span>       </span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    val_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> val_loader:</span><br><span class="line">            data, label = data.cuda(), label.cuda()</span><br><span class="line">            output = model(data)</span><br><span class="line">            preds = torch.argmax(output, <span class="number">1</span>)</span><br><span class="line">            loss = criterion(output, label)</span><br><span class="line">            val_loss += loss.item()*data.size(<span class="number">0</span>)</span><br><span class="line">            running_accu += torch.<span class="built_in">sum</span>(preds == label.data)</span><br><span class="line">    val_loss = val_loss/<span class="built_in">len</span>(val_loader.dataset)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125; \tTraining Loss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, val_loss))</span><br></pre></td></tr></table></figure><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>在PyTorch深度学习中，可视化是一个可选项，指的是某些任务在训练完成后，需要对一些必要的内容进行可视化，比如分类的ROC曲线，卷积网络中的卷积核，以及训练/验证过程的损失函数曲线等等。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>Datawhale开源项目：深入浅出PyTorch <a href="https://github.com/datawhalechina/thorough-pytorch/">https://github.com/datawhalechina/thorough-pytorch/</a></li><li>李宏毅机器学习2021春-PyTorch Tutorial <a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=5">https://www.bilibili.com/video/BV1Wv411h7kN?p=5</a></li><li>动手学深度学习pytorch版 <a href="https://zh-v2.d2l.ai/chapter_preface/index.html">https://zh-v2.d2l.ai/chapter_preface/index.html</a></li><li>PyTorch官方教程中文版 <a href="https://pytorch123.com/SecondSection/training_a_classifier/">https://pytorch123.com/SecondSection/training_a_classifier/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;第三章-PyTorch的主要组成模块&quot;&gt;&lt;a href=&quot;#第三章-PyTorch的主要组成模块&quot; class=&quot;headerlink&quot; title=&quot;第三章 PyTorch的主要组成模块&quot;&gt;&lt;/a&gt;第三章 PyTorch的主要组成模块&lt;/h1&gt;&lt;h2 id=&quot;完</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="2021-10 深入浅出PyTorch" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/2021-10-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAPyTorch/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="PyTorch" scheme="http://example.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Task07 使用Transformers解决文本分类任务</title>
    <link href="http://example.com/nlp-transformer-task07/"/>
    <id>http://example.com/nlp-transformer-task07/</id>
    <published>2021-09-25T08:57:45.000Z</published>
    <updated>2022-01-24T15:27:59.755Z</updated>
    
    <content type="html"><![CDATA[<p><em>该部分的内容翻译自🤗HuggingFace/notebooks <a href="https://github.com/huggingface/notebooks/tree/master/examples">https://github.com/huggingface/notebooks/tree/master/examples</a></em><br><em>中文翻译：Datawhale/learn-nlp-with-transformers/4.1-文本分类 <a href="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.md">Datawhale/learn-nlp-with-transformers/4.1-文本分类</a></em></p><h1 id="微调预训练模型进行文本分类"><a href="#微调预训练模型进行文本分类" class="headerlink" title="微调预训练模型进行文本分类"></a>微调预训练模型进行文本分类</h1><p>我们将使用 🤗 Transformers代码库中的模型来解决文本分类任务，任务来源于GLUE Benchmark.<br>GLUE榜单包含了9个句子级别的分类任务，分别是：</p><ul><li>CoLA (Corpus of Linguistic Acceptability) 鉴别一个句子是否语法正确.</li><li>MNLI (Multi-Genre Natural Language Inference) 给定一个假设，判断另一个句子与该假设的关系：entails, contradicts 或者 unrelated。</li><li>MRPC (Microsoft Research Paraphrase Corpus) 判断两个句子是否互为paraphrases.</li><li>QNLI (Question-answering Natural Language Inference) 判断第2句是否包含第1句问题的答案。</li><li>QQP (Quora Question Pairs2) 判断两个问句是否语义相同。</li><li>RTE (Recognizing Textual Entailment)判断一个句子是否与假设成entail关系。</li><li>SST-2 (Stanford Sentiment Treebank) 判断一个句子的情感正负向.</li><li>STS-B (Semantic Textual Similarity Benchmark) 判断两个句子的相似性（分数为1-5分）。</li><li>WNLI (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not.</li></ul><p>对于以上任务，我们将展示如何使用简单的Dataset库加载数据集，同时使用transformer中的Trainer接口对预训练模型进行微调。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GLUE_TASKS = [<span class="string">&quot;cola&quot;</span>, <span class="string">&quot;mnli&quot;</span>, <span class="string">&quot;mnli-mm&quot;</span>, <span class="string">&quot;mrpc&quot;</span>, <span class="string">&quot;qnli&quot;</span>, <span class="string">&quot;qqp&quot;</span>, <span class="string">&quot;rte&quot;</span>, <span class="string">&quot;sst2&quot;</span>, <span class="string">&quot;stsb&quot;</span>, <span class="string">&quot;wnli&quot;</span>]</span><br></pre></td></tr></table></figure><p>This notebook is built to run on any of the tasks in the list above, with any model checkpoint from the Model Hub as long as that model has a version with a classification head. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:<br>本notebook理论上可以使用各种各样的transformer模型（模型面板），解决任何文本分类分类任务。如果您所处理的任务有所不同，大概率只需要很小的改动便可以使用本notebook进行处理。同时，您应该根据您的GPU显存来调整微调训练所需要的btach size大小，避免显存溢出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">task = <span class="string">&quot;cola&quot;</span></span><br><span class="line">model_checkpoint = <span class="string">&quot;distilbert-base-uncased&quot;</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br></pre></td></tr></table></figure><h1 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h1><p>We will use the 🤗 Datasets library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions <code>load_dataset</code> and <code>load_metric</code>.<br>我们将会使用🤗 Datasets库来加载数据和对应的评测方式。数据加载和评测方式加载只需要简单使用load_dataset和load_metric即可。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from datasets import load_dataset, load_metric</span><br></pre></td></tr></table></figure><p>Apart from mnli-mm being a special code, we can directly pass our task name to those functions. <code>load_dataset</code> will cache the dataset to avoid downloading it again the next time you run this cell.<br>除了mnli-mm以外，其他任务都可以直接通过任务名字进行加载。数据加载之后会自动缓存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">actual_task = <span class="string">&quot;mnli&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;mnli-mm&quot;</span> <span class="keyword">else</span> task</span><br><span class="line">dataset = load_dataset(<span class="string">&quot;glue&quot;</span>, actual_task)</span><br><span class="line">metric = load_metric(<span class="string">&#x27;glue&#x27;</span>, actual_task)</span><br></pre></td></tr></table></figure><p><em>上节讲过，这里，最好手动下载glue.py和gule_metric.py，不下载到本地的话，容易出现连接错误。</em></p><p>The dataset object itself is DatasetDict, which contains one key for the training, validation and test set (with more keys for the mismatched validation and test set in the special case of mnli).<br>这个datasets对象本身是一种DatasetDict数据结构.对于训练集、验证集和测试集，只需要使用对应的key（train，validation，test）即可得到相应的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>dataset</span><br><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">8551</span></span><br><span class="line">    &#125;)</span><br><span class="line">    validation: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">1043</span></span><br><span class="line">    &#125;)</span><br><span class="line">    test: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">1063</span></span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>dataset[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br><span class="line">&#123;<span class="string">&#x27;sentence&#x27;</span>: <span class="string">&quot;Our friends won&#x27;t buy this analysis, let alone the next one we propose.&quot;</span>,</span><br><span class="line"><span class="string">&#x27;label&#x27;</span>: <span class="number">1</span>, </span><br><span class="line"><span class="string">&#x27;idx&#x27;</span>: <span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure><p>To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset.<br>为了能够进一步理解数据长什么样子，下面的函数将从数据集里随机选择几个例子进行展示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display, HTML</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_random_elements</span>(<span class="params">dataset, num_examples=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="keyword">assert</span> num_examples &lt;= <span class="built_in">len</span>(dataset), <span class="string">&quot;Can&#x27;t pick more elements than there are in the dataset.&quot;</span></span><br><span class="line">    picks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_examples):</span><br><span class="line">        pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">while</span> pick <span class="keyword">in</span> picks:</span><br><span class="line">            pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        picks.append(pick)</span><br><span class="line">    </span><br><span class="line">    df = pd.DataFrame(dataset[picks])</span><br><span class="line">    <span class="keyword">for</span> column, typ <span class="keyword">in</span> dataset.features.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(typ, datasets.ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> i: typ.names[i])</span><br><span class="line">    display(HTML(df.to_html()))</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_random_elements(dataset[<span class="string">&quot;train&quot;</span>])</span><br></pre></td></tr></table></figure><p>The metric is an instance of datasets.Metric:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>You can call its <code>compute</code> method with your predictions and labels directly and it will return a dictionary with the metric(s) value:<br>直接调用metric的compute方法，传入labels和predictions即可得到metric的值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">fake_preds = np.random.randint(<span class="number">0</span>, <span class="number">2</span>, size=(<span class="number">64</span>,))</span><br><span class="line">fake_labels = np.random.randint(<span class="number">0</span>, <span class="number">2</span>, size=(<span class="number">64</span>,))</span><br><span class="line">metric.compute(predictions=fake_preds, references=fake_labels)</span><br></pre></td></tr></table></figure><p>Note that load_metric has loaded the proper metric associated to your task, which is:<br>每一个文本分类任务所对应的metic有所不同，具体如下:</p><ul><li>for CoLA: Matthews Correlation Coefficient</li><li>for MNLI (matched or mismatched): Accuracy</li><li>for MRPC: Accuracy and F1 score</li><li>for QNLI: Accuracy</li><li>for QQP: Accuracy and F1 score</li><li>for RTE: Accuracy</li><li>for SST-2: Accuracy</li><li>for STS-B: Pearson Correlation Coefficient and Spearman’s_Rank_Correlation_Coefficient</li><li>for WNLI: Accuracy</li></ul><p>so the metric object only computes the one(s) needed for your task.</p><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>Before we can feed those texts to our model, we need to preprocess them. This is done by a 🤗 Transformers <code>Tokenizer</code> which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.<br>在将数据喂入模型之前，我们需要对数据进行预处理。预处理的工具叫Tokenizer。Tokenizer首先对输入进行tokenize，然后将tokens转化为预模型中需要对应的token ID，再转化为模型需要的输入格式。</p><p>To do all of this, we instantiate our tokenizer with the <code>AutoTokenizer.from_pretrained</code> method, which will ensure:</p><ul><li>we get a tokenizer that corresponds to the model architecture we want to use,</li><li>we download the vocabulary used when pretraining this specific checkpoint.</li></ul><p>为了达到数据预处理的目的，我们使用AutoTokenizer.from_pretrained方法实例化我们的tokenizer，这样可以确保：</p><ul><li>我们得到一个与预训练模型一一对应的tokenizer。</li><li>使用指定的模型checkpoint对应的tokenizer的时候，我们也下载了模型需要的词表库vocabulary，准确来说是tokens vocabulary。</li></ul><p>That vocabulary will be cached, so it’s not downloaded again the next time we run the cell.<br>这个被下载的tokens vocabulary会被缓存起来，从而再次使用的时候不会重新下载。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">    </span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>You can directly call this tokenizer on one sentence or a pair of sentences:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer(<span class="string">&quot;Hello, this one sentence!&quot;</span>, <span class="string">&quot;And this sentence goes with it.&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出为：<br>pass</p><p>To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">task_to_keys = &#123;</span><br><span class="line">    <span class="string">&quot;cola&quot;</span>: (<span class="string">&quot;sentence&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">    <span class="string">&quot;mnli&quot;</span>: (<span class="string">&quot;premise&quot;</span>, <span class="string">&quot;hypothesis&quot;</span>),</span><br><span class="line">    <span class="string">&quot;mnli-mm&quot;</span>: (<span class="string">&quot;premise&quot;</span>, <span class="string">&quot;hypothesis&quot;</span>),</span><br><span class="line">    <span class="string">&quot;mrpc&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;qnli&quot;</span>: (<span class="string">&quot;question&quot;</span>, <span class="string">&quot;sentence&quot;</span>),</span><br><span class="line">    <span class="string">&quot;qqp&quot;</span>: (<span class="string">&quot;question1&quot;</span>, <span class="string">&quot;question2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;rte&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;sst2&quot;</span>: (<span class="string">&quot;sentence&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">    <span class="string">&quot;stsb&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;wnli&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>We can double check it does work on our current dataset:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sentence1_key, sentence2_key = task_to_keys[task]</span><br><span class="line"><span class="keyword">if</span> sentence2_key <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sentence: <span class="subst">&#123;dataset[<span class="string">&#x27;train&#x27;</span>][<span class="number">0</span>][sentence1_key]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sentence 1: <span class="subst">&#123;dataset[<span class="string">&#x27;train&#x27;</span>][<span class="number">0</span>][sentence1_key]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sentence 2: <span class="subst">&#123;dataset[<span class="string">&#x27;train&#x27;</span>][<span class="number">0</span>][sentence2_key]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sentence: Our friends won&#x27;t buy this analysis, let alone the next one we propose.</span><br></pre></td></tr></table></figure><p>We can them write the function that will preprocess our samples. We just feed them to the <code>tokenizer</code> with the argument <code>truncation=True</code>. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="keyword">if</span> sentence2_key <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> tokenizer(examples[sentence1_key], truncation=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>preprocess_function(dataset[<span class="string">&#x27;train&#x27;</span>][:<span class="number">5</span>])</span><br><span class="line">&#123;<span class="string">&#x27;input_ids&#x27;</span>: [[<span class="number">101</span>, <span class="number">2256</span>, <span class="number">2814</span>, <span class="number">2180</span>, <span class="number">1005</span>, <span class="number">1056</span>, <span class="number">4965</span>, <span class="number">2023</span>, <span class="number">4106</span>, <span class="number">1010</span>, <span class="number">2292</span>, <span class="number">2894</span>, <span class="number">1996</span>, <span class="number">2279</span>, <span class="number">2028</span>, <span class="number">2057</span>, <span class="number">16599</span>, <span class="number">1012</span>, <span class="number">102</span>], [<span class="number">101</span>, <span class="number">2028</span>, <span class="number">2062</span>, <span class="number">18404</span>, <span class="number">2236</span>, <span class="number">3989</span>, <span class="number">1998</span>, <span class="number">1045</span>, <span class="number">1005</span>, <span class="number">1049</span>, <span class="number">3228</span>, <span class="number">2039</span>, <span class="number">1012</span>, <span class="number">102</span>], [<span class="number">101</span>, <span class="number">2028</span>, <span class="number">2062</span>, <span class="number">18404</span>, <span class="number">2236</span>, <span class="number">3989</span>, <span class="number">2030</span>, <span class="number">1045</span>, <span class="number">1005</span>, <span class="number">1049</span>, <span class="number">3228</span>, <span class="number">2039</span>, <span class="number">1012</span>, <span class="number">102</span>], [<span class="number">101</span>, <span class="number">1996</span>, <span class="number">2062</span>, <span class="number">2057</span>, <span class="number">2817</span>, <span class="number">16025</span>, <span class="number">1010</span>, <span class="number">1996</span>, <span class="number">13675</span>, <span class="number">16103</span>, <span class="number">2121</span>, <span class="number">2027</span>, <span class="number">2131</span>, <span class="number">1012</span>, <span class="number">102</span>], [<span class="number">101</span>, <span class="number">2154</span>, <span class="number">2011</span>, <span class="number">2154</span>, <span class="number">1996</span>, <span class="number">8866</span>, <span class="number">2024</span>, <span class="number">2893</span>, <span class="number">14163</span>, <span class="number">8024</span>, <span class="number">3771</span>, <span class="number">1012</span>, <span class="number">102</span>]], <span class="string">&#x27;attention_mask&#x27;</span>: [[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]&#125;</span><br></pre></td></tr></table></figure><p>To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the <code>map</code> method of our <code>dataset</code> object we created earlier. This will apply the function on all the elements of all the splits in <code>dataset</code>, so our training, validation and testing data will be preprocessed in one single command.<br>接下来对数据集datasets里面的所有样本进行预处理，处理的方式是使用map函数，将预处理函数prepare_train_features应用到（map)所有样本上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoded_dataset = dataset.<span class="built_in">map</span>(preprocess_function, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h1 id="微调模型"><a href="#微调模型" class="headerlink" title="微调模型"></a>微调模型</h1><p>Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about sentence classification, we use the AutoModelForSequenceClassification class. Like with the tokenizer, the from_pretrained method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which is always 2, except for STS-B which is a regression problem and MNLI where we have 3 labels):<br>既然数据已经准备好了，现在我们需要下载并加载我们的预训练模型，然后微调预训练模型。既然我们是做seq2seq任务，那么我们需要一个能解决这个任务的模型类。我们使用AutoModelForSequenceClassification 这个类。和tokenizer相似，from_pretrained方法同样可以帮助我们下载并加载模型，同时也会对模型进行缓存，就不会重复下载模型啦。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification, TrainingArguments, Trainer</span><br><span class="line"></span><br><span class="line">num_labels = <span class="number">3</span> <span class="keyword">if</span> task.startswith(<span class="string">&quot;mnli&quot;</span>) <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">if</span> task==<span class="string">&quot;stsb&quot;</span> <span class="keyword">else</span> <span class="number">2</span></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: [&#x27;vocab_transform.bias&#x27;, &#x27;vocab_projector.weight&#x27;, &#x27;vocab_layer_norm.bias&#x27;, &#x27;vocab_layer_norm.weight&#x27;, &#x27;vocab_projector.bias&#x27;, &#x27;vocab_transform.weight&#x27;]</span><br><span class="line">- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).</span><br><span class="line">- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).</span><br><span class="line">Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: [&#x27;classifier.weight&#x27;, &#x27;classifier.bias&#x27;, &#x27;pre_classifier.bias&#x27;, &#x27;pre_classifier.weight&#x27;]</span><br><span class="line">You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</span><br></pre></td></tr></table></figure><p>The warning is telling us we are throwing away some weights (the vocab_transform and vocab_layer_norm layers) and randomly initializing some other (the pre_classifier and classifier layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don’t have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.<br>由于我们微调的任务是文本分类任务，而我们加载的是预训练的语言模型，所以会提示我们加载模型的时候扔掉了一些不匹配的神经网络参数（比如：预训练语言模型的神经网络head被扔掉了，同时随机初始化了文本分类的神经网络head）。</p><p>To instantiate a <code>Trainer</code>, we will need to define two more things. The most important is the <code>TrainingArguments</code>, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:<br>为了能够得到一个Trainer训练工具，我们还需要3个要素，其中最重要的是训练的设定/参数 TrainingArguments。这个训练设定包含了能够定义训练过程的所有属性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">metric_name = <span class="string">&quot;pearson&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;stsb&quot;</span> <span class="keyword">else</span> <span class="string">&quot;matthews_correlation&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;cola&quot;</span> <span class="keyword">else</span> <span class="string">&quot;accuracy&quot;</span></span><br><span class="line">model_name = model_checkpoint.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">args = TrainingArguments(</span><br><span class="line">    <span class="string">&quot;test-glue&quot;</span>,</span><br><span class="line">    evaluation_strategy = <span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    save_strategy = <span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=batch_size,</span><br><span class="line">    per_device_eval_batch_size=batch_size,</span><br><span class="line">    num_train_epochs=<span class="number">5</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">    load_best_model_at_end=<span class="literal">True</span>,</span><br><span class="line">    metric_for_best_model=metric_name,</span><br><span class="line">    push_to_hub=<span class="literal">False</span>,</span><br><span class="line">    push_to_hub_model_id=<span class="string">f&quot;<span class="subst">&#123;model_name&#125;</span>-finetuned-<span class="subst">&#123;task&#125;</span>&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the <code>batch_size</code> defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay. Since the best model might not be the one at the end of training, we ask the <code>Trainer</code> to load the best model it saved (according to <code>metric_name</code>) at the end of training.<br>上面evaluation_strategy = “epoch”参数告诉训练代码：我们每个epcoh会做一次验证评估。<br>上面batch_size在这个notebook之前定义好了。</p><p>The last two arguments are to setup everything so we can push the model to the <code>Hub</code> at the end of training. Remove the two of them if you didn’t follow the installation steps at the top of the notebook, otherwise you can change the value of <code>push_to_hub_model_id</code> to something you would prefer.<br><em>(后面需要连接到hub客户端，太麻烦，所以先设为False)</em></p><p>The last thing to define for our <code>Trainer</code> is how to compute the metrics from the predictions. We need to define a function for this, which will just use the <code>metric</code> we loaded earlier, the only preprocessing we have to do is to take the argmax of our predicted logits (our just squeeze the last axis in the case of STS-B):<br>最后，由于不同的任务需要不同的评测指标，我们定一个函数来根据任务名字得到评价方法:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_metrics</span>(<span class="params">eval_pred</span>):</span></span><br><span class="line">    predictions, labels = eval_pred</span><br><span class="line">    <span class="keyword">if</span> task != <span class="string">&quot;stsb&quot;</span>:</span><br><span class="line">        predictions = np.argmax(predictions, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        predictions = predictions[:, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> metric.compute(predictions=predictions, references=labels)</span><br></pre></td></tr></table></figure><p>Then we just need to pass all of this along with our datasets to the Trainer:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">validation_key = <span class="string">&quot;validation_mismatched&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;mnli-mm&quot;</span> <span class="keyword">else</span> <span class="string">&quot;validation_matched&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;mnli&quot;</span> <span class="keyword">else</span> <span class="string">&quot;validation&quot;</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model,</span><br><span class="line">    args,</span><br><span class="line">    train_dataset=encoded_dataset[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=encoded_dataset[validation_key],</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics</span><br><span class="line">)</span><br></pre></td></tr></table></figure><blockquote><p>BUG:<br>ValueError: You must login to the Hugging Face hub on this computer by typing <code>transformers-cli login</code> and entering your credentials to use <code>use_auth_token=True</code>. Alternatively, you can pass your own token as the <code>use_auth_token</code> argument.<br>把args里面的该项参数改为False   <code>push_to_hub=False,</code>。</p></blockquote><p>We can now finetune our model by just calling the <code>train</code> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>We can check with the <code>evaluate</code> method that our <code>Trainer</code> did reload the best model properly (if it was not the last one):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer.evaluate()</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h1 id="超参搜索"><a href="#超参搜索" class="headerlink" title="超参搜索"></a>超参搜索</h1><p>The Trainer supports hyperparameter search using optuna or Ray Tune. </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install optuna</span><br><span class="line">pip install ray[tune]</span><br></pre></td></tr></table></figure><p>During hyperparameter search, the Trainer will run several trainings, so it needs to have the model defined via a function (so it can be reinitialized at each new run) instead of just having it passed. We jsut use the same function as before:<br>超参搜索时，Trainer将会返回多个训练好的模型，所以需要传入一个定义好的模型从而让Trainer可以不断重新初始化该传入的模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_init</span>():</span></span><br><span class="line">    <span class="keyword">return</span> AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)</span><br></pre></td></tr></table></figure><p>And we can instantiate our Trainer like before:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">trainer = Trainer(</span><br><span class="line">    model_init=model_init,</span><br><span class="line">    args=args,</span><br><span class="line">    train_dataset=encoded_dataset[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=encoded_dataset[validation_key],</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>The method we call this time is <code>hyperparameter_search</code>. Note that it can take a long time to run on the full dataset for some of the tasks. You can try to find some good hyperparameter on a portion of the training dataset by replacing the <code>train_dataset</code> line above by:<br><code>train_dataset = encoded_dataset[&quot;train&quot;].shard(index=1, num_shards=10) </code><br>for 1/10th of the dataset. Then you can run a full training on the best hyperparameters picked by the search.<br>调用方法hyperparameter_search。注意，这个过程可能很久，我们可以先用部分数据集进行超参搜索，再进行全量训练。 比如使用1/10的数据进行搜索：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">best_run = trainer.hyperparameter_search(n_trials=<span class="number">10</span>, direction=<span class="string">&quot;maximize&quot;</span>)</span><br></pre></td></tr></table></figure><p>The hyperparameter_search method returns a <code>BestRun</code> objects, which contains the value of the objective maximized (by default the sum of all metrics) and the hyperparameters it used for that run.<br>hyperparameter_search会返回效果最好的模型相关的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>best_run</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>To reproduce the best training, just set the hyperparameters in your TrainingArgument before creating a Trainer:<br>将Trainner设置为搜索到的最好参数，进行训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> n, v <span class="keyword">in</span> best_run.hyperparameters.items():</span><br><span class="line">    <span class="built_in">setattr</span>(trainer.args, n, v)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>HuggingFace/transfomers/BERT <a href="https://huggingface.co/transformers/model_doc/bert.html#">https://huggingface.co/transformers/model_doc/bert.html#</a></li><li>基于transformers的自然语言处理(NLP)入门–在线阅读 <a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/">https://datawhalechina.github.io/learn-nlp-with-transformers/#/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;该部分的内容翻译自🤗HuggingFace/notebooks &lt;a href=&quot;https://github.com/huggingface/notebooks/tree/master/examples&quot;&gt;https://github.com/huggingfa</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="2021-09 基于transformer的NLP" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/2021-09-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="预训练模型" scheme="http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="BERT" scheme="http://example.com/tags/BERT/"/>
    
    <category term="transfomer" scheme="http://example.com/tags/transfomer/"/>
    
    <category term="文本分类" scheme="http://example.com/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Task06 BERT应用、训练和优化</title>
    <link href="http://example.com/nlp-transformer-task06/"/>
    <id>http://example.com/nlp-transformer-task06/</id>
    <published>2021-09-24T07:18:42.000Z</published>
    <updated>2022-01-24T15:27:54.286Z</updated>
    
    <content type="html"><![CDATA[<p><em>该部分的内容翻译自🤗HuggingFace官网教程第1部分（1-4章），见 <a href="https://huggingface.co/course/chapter1">https://huggingface.co/course/chapter1</a>。该系列教程由3大部分共12章组成（如图），其中第1部分介绍transformers库的主要概念、模型的工作原理和使用方法、怎样在特定数据集上微调等内容。</em><br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_1.png?raw=true" width="500" alt="" align="center" /></p><h1 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h1><p>简单的说，有两种可以跑模型代码的方式：</p><ol><li>Google Colab</li><li>本地虚拟环境 <code>pip install transformers</code></li></ol><p>详见 <a href="https://huggingface.co/course/chapter0?fw=pt">https://huggingface.co/course/chapter0?fw=pt</a></p><h1 id="Transformer模型概述"><a href="#Transformer模型概述" class="headerlink" title="Transformer模型概述"></a>Transformer模型概述</h1><h2 id="Transformers-可以做什么？"><a href="#Transformers-可以做什么？" class="headerlink" title="Transformers, 可以做什么？"></a>Transformers, 可以做什么？</h2><p>目前可用的一些pipeline是：</p><ul><li>feature-extraction 获取文本的向量表示</li><li>fill-mask 完形填空</li><li>ner (named entity recognition) 命名实体识别</li><li>question-answering 问答</li><li>sentiment-analysis 情感分析</li><li>summarization 摘要生成</li><li>text-generation 文本生成</li><li>translation 翻译</li><li>zero-shot-classification 零样本分类</li></ul><p><em>pipeline: 直译管道/流水线，可以理解为流程。</em></p><h2 id="Transformers-如何工作？"><a href="#Transformers-如何工作？" class="headerlink" title="Transformers, 如何工作？"></a>Transformers, 如何工作？</h2><h3 id="Transformer简史"><a href="#Transformer简史" class="headerlink" title="Transformer简史"></a>Transformer简史</h3><p>Transformer 架构于 2017 年 6 月推出。原始研究的重点是翻译任务。随后推出了几个有影响力的模型，包括：</p><ul><li>2018 年 6 月：GPT，第一个预训练的 Transformer 模型，用于各种 NLP 任务的微调并获得最先进的结果</li><li>2018 年 10 月：BERT，另一个大型预训练模型，该模型旨在生成更好的句子摘要</li><li>2019 年 2 月：GPT-2，GPT 的改进（和更大）版本</li><li>2019 年 10 月：DistilBERT，BERT 的蒸馏版本，速度提高 60%，内存减轻 40%，但仍保留 BERT 97% 的性能</li><li>2019 年 10 月：BART 和 T5，两个使用与原始 Transformer 模型相同架构的大型预训练模型（第一个这样做）</li><li>2020 年 5 月，GPT-3，GPT-2 的更大版本，无需微调即可在各种任务上表现良好（称为零样本学习zero-shot learning）</li></ul><p>大体上，它们可以分为三类：</p><ul><li>GPT类（又称为自回归 Transformer 模型）：只使用transformer-decoder部分</li><li>BERT类（又称为自编码 Transformer 模型）：只使用transformer-encoder部分</li><li>BART/T5类（又称为序列到序列 Transformer 模型）：使用Transformer-encoder-decoder部分</li></ul><p>它们的分类、具体模型、主要应用任务如下：<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_2.jpg?raw=true" width="800" alt="" align="center" /></p><p>其他需要知道的：</p><ul><li>Transformers是语言模型</li><li>Transformers是大模型</li><li>Transformers的应用通过预训练和微调两个过程</li></ul><h3 id="名词解释：Architecture和Checkpoints"><a href="#名词解释：Architecture和Checkpoints" class="headerlink" title="名词解释：Architecture和Checkpoints"></a>名词解释：Architecture和Checkpoints</h3><p><strong>Architecture/架构</strong>：定义了模型的基本结构和基本运算。<br><strong>Checkpoints/检查点</strong>：模型的某个训练状态，加载此checkpoint会加载此时的权重。训练时可以选择自动保存checkpoint。模型在训练时可以设置自动保存于某个时间点（比如模型训练了一轮epoch，更新了参数，将这个状态的模型保存下来，为一个checkpoint。） 所以每个checkpoint对应模型的一个状态，一组权重。</p><h1 id="使用Transformers"><a href="#使用Transformers" class="headerlink" title="使用Transformers"></a>使用Transformers</h1><h2 id="3个处理步骤"><a href="#3个处理步骤" class="headerlink" title="3个处理步骤"></a>3个处理步骤</h2><p>将一些文本传递到pipeline时涉及3个主要步骤：</p><ol><li>文本被预处理为模型可以理解的格式。</li><li>预处理后的输入传递给模型。</li><li>模型的预测结果被后处理为人类可以理解的格式。</li></ol><p>Pipeline将3个步骤组合在一起：预处理/Tokenizer、通过模型传递输入/Model和后处理/Post-Processing：<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_3.png?raw=true" width="800" alt="" align="center" /></p><h2 id="Tokenizer-预处理"><a href="#Tokenizer-预处理" class="headerlink" title="Tokenizer/预处理"></a>Tokenizer/预处理</h2><p>Tokenizer的作用：</p><ul><li>将输入拆分为称为token的单词、子词/subword或符号/symbols（如标点符号）</li><li>将每个token映射到一个整数</li><li>添加可能对模型有用的其他输入</li></ul><h2 id="Going-Through-Models-穿过模型"><a href="#Going-Through-Models-穿过模型" class="headerlink" title="Going Through Models/穿过模型"></a>Going Through Models/穿过模型</h2><h3 id="模型实例化"><a href="#模型实例化" class="headerlink" title="模型实例化"></a>模型实例化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span><br><span class="line">model = AutoModel.from_pretrained(checkpoint)</span><br></pre></td></tr></table></figure><p>在这段代码中，我们下载了在pipeline中使用的相同检查点（实际上已经缓存）并将模型实例化。</p><h3 id="模型的输出：高维向量"><a href="#模型的输出：高维向量" class="headerlink" title="模型的输出：高维向量"></a>模型的输出：高维向量</h3><p>模型的输出向量通常有三个维度：</p><ul><li>Batch size: 一次处理的序列数</li><li>Sequence length: 序列向量的长度</li><li>Hidden size: 每个模型输入处理后的向量维度（hidden state vector）</li></ul><h3 id="Model-Heads：为了处理不同的任务"><a href="#Model-Heads：为了处理不同的任务" class="headerlink" title="Model Heads：为了处理不同的任务"></a>Model Heads：为了处理不同的任务</h3><p>Model heads:将隐藏状态的高维向量作为输入，并将它们投影到不同的维度上。它们通常由一个或几个线性层组成。<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/hf_4.png?raw=true" width="800" alt="这个图表示了Pipeline第二步在经过模型时发生的事情。" align="center" /><br>如上图所示，紫色代表向量，粉色代表模组，Embeddings+layers表示Transformer的架构，经过这层架构后的输出送入Model Head进行处理，从而应用到不同的下游任务。<br>🤗 Transformers 中有许多不同的Head架构可用，每一种架构都围绕着处理特定任务而设计。 下面列举了部分Model heads：</p><ul><li>*Model (retrieve the hidden states)</li><li>*ForCausalLM</li><li>*ForMaskedLM</li><li>*ForMultipleChoice</li><li>*ForQuestionAnswering</li><li>*ForSequenceClassification</li><li>*ForTokenClassification</li><li>and others 🤗</li></ul><h2 id="Post-processing-后处理"><a href="#Post-processing-后处理" class="headerlink" title="Post-processing/后处理"></a>Post-processing/后处理</h2><p>从模型中获得的作为输出的值本身并不一定有意义。要转换为概率，它们需要经过一个 SoftMax 层。</p><h1 id="微调一个预训练模型"><a href="#微调一个预训练模型" class="headerlink" title="微调一个预训练模型"></a>微调一个预训练模型</h1><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>在本节中，我们将使用MRPC（Microsoft Research Praphrase Corpus）数据集作为示例。该DataSet由5,801对句子组成，标签指示它们是否是同义句（即两个句子是否表示相同的意思）。 我们选择它是因为它是一个小型数据集，因此可以轻松训练。</p><h3 id="从Hub上加载数据集"><a href="#从Hub上加载数据集" class="headerlink" title="从Hub上加载数据集"></a>从Hub上加载数据集</h3><p>Hub不仅包含模型，还含有多种语言的datasets。<br>例如，MRPC数据集是构成 GLUE benchmark的 10 个数据集之一。GLUE（General Language Understanding Evaluation）是一个多任务的自然语言理解基准和分析平台。GLUE包含九项NLU任务，语言均为英语。GLUE九项任务涉及到自然语言推断、文本蕴含、情感分析、语义相似等多个任务。像BERT、XLNet、RoBERTa、ERINE、T5等知名模型都会在此基准上进行测试。</p><p>🤗 Datasets库提供了一个非常简单的命令来下载和缓存Hub上的dataset。 我们可以像这样下载 MRPC 数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>raw_datasets</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence1&#x27;</span>, <span class="string">&#x27;sentence2&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">3668</span></span><br><span class="line">    &#125;)</span><br><span class="line">    validation: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence1&#x27;</span>, <span class="string">&#x27;sentence2&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">408</span></span><br><span class="line">    &#125;)</span><br><span class="line">    test: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence1&#x27;</span>, <span class="string">&#x27;sentence2&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">1725</span></span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>这样就得到一个DatasetDict对象，包含训练集、验证集和测试集，训练集中有3,668 个句子对，验证集中有408对，测试集中有1,725 对。每个句子对包含四个字段：’sentence1’, ‘sentence2’, ‘label’和 ‘idx’。</p><p>我们可以通过索引访问raw_datasets 的句子对：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>raw_train_dataset = raw_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>raw_train_dataset[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;sentence1&#x27;</span>: <span class="string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>, </span><br><span class="line"><span class="string">&#x27;sentence2&#x27;</span>: <span class="string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>, </span><br><span class="line"><span class="string">&#x27;label&#x27;</span>: <span class="number">1</span>, </span><br><span class="line"><span class="string">&#x27;idx&#x27;</span>: <span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure><p>我们可以通过features获得数据集的字段类型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>raw_train_dataset.features</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="string">&#x27;string&#x27;</span>, <span class="built_in">id</span>=<span class="literal">None</span>), </span><br><span class="line"><span class="string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="string">&#x27;string&#x27;</span>, <span class="built_in">id</span>=<span class="literal">None</span>), </span><br><span class="line"><span class="string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="number">2</span>, names=[<span class="string">&#x27;not_equivalent&#x27;</span>, <span class="string">&#x27;equivalent&#x27;</span>], names_file=<span class="literal">None</span>, <span class="built_in">id</span>=<span class="literal">None</span>), </span><br><span class="line"><span class="string">&#x27;idx&#x27;</span>: Value(dtype=<span class="string">&#x27;int32&#x27;</span>, <span class="built_in">id</span>=<span class="literal">None</span>)&#125;</span><br></pre></td></tr></table></figure><blockquote><p>TIPS：</p><ol><li>没有数据集的话首先安装一下：<code>pip install datasets</code></li><li>这里很容易出现连接错误，解决方法如下：<a href="https://blog.csdn.net/qq_20849045/article/details/117462846?utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.no_search_link">https://blog.csdn.net/qq_20849045/article/details/117462846?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link</a></li></ol></blockquote><h3 id="数据集预处理"><a href="#数据集预处理" class="headerlink" title="数据集预处理"></a>数据集预处理</h3><p>通过数据集预处理，我们将文本转换成模型能理解的向量。这个过程通过Tokenizer实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenized_sentences_1 = tokenizer(raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="string">&quot;sentence1&quot;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenized_sentences_2 = tokenizer(raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="string">&quot;sentence2&quot;</span>])</span><br></pre></td></tr></table></figure><p>（TODO）</p><h2 id="使用Trainer-API微调一个模型"><a href="#使用Trainer-API微调一个模型" class="headerlink" title="使用Trainer API微调一个模型"></a>使用Trainer API微调一个模型</h2><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><h3 id="评估函数"><a href="#评估函数" class="headerlink" title="评估函数"></a>评估函数</h3><h1 id="补充部分"><a href="#补充部分" class="headerlink" title="补充部分"></a>补充部分</h1><h2 id="为什么4中用Trainer来微调模型？"><a href="#为什么4中用Trainer来微调模型？" class="headerlink" title="为什么4中用Trainer来微调模型？"></a>为什么4中用Trainer来微调模型？</h2><h2 id="Training-Arguments主要参数"><a href="#Training-Arguments主要参数" class="headerlink" title="Training Arguments主要参数"></a>Training Arguments主要参数</h2><h2 id="不同模型的加载方式"><a href="#不同模型的加载方式" class="headerlink" title="不同模型的加载方式"></a>不同模型的加载方式</h2><h2 id="Dynamic-Padding——动态填充技术"><a href="#Dynamic-Padding——动态填充技术" class="headerlink" title="Dynamic Padding——动态填充技术"></a>Dynamic Padding——动态填充技术</h2><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>基于transformers的自然语言处理(NLP)入门–在线阅读 <a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/">https://datawhalechina.github.io/learn-nlp-with-transformers/#/</a></li><li>Huggingface官方教程 <a href="https://huggingface.co/course/chapter1">https://huggingface.co/course/chapter1</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;该部分的内容翻译自🤗HuggingFace官网教程第1部分（1-4章），见 &lt;a href=&quot;https://huggingface.co/course/chapter1&quot;&gt;https://huggingface.co/course/chapter1&lt;/a&gt;。该系</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="2021-09 基于transformer的NLP" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/2021-09-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="预训练模型" scheme="http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="BERT" scheme="http://example.com/tags/BERT/"/>
    
    <category term="transfomer" scheme="http://example.com/tags/transfomer/"/>
    
  </entry>
  
  <entry>
    <title>Task05 编写BERT模型</title>
    <link href="http://example.com/nlp-transformer-task05/"/>
    <id>http://example.com/nlp-transformer-task05/</id>
    <published>2021-09-20T19:01:35.000Z</published>
    <updated>2022-01-24T15:27:48.459Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>本部分是BERT源码的解读，来自HuggingFace/transfomers/BERT[1]。<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/bert_1.png?raw=true" width="600" alt="" align="center" /></p><p>如图所示，代码结构和作用如下：</p><ul><li>BertTokenizer 预处理和切词</li><li>BertModel<ul><li>BertEmbeddings 词嵌入</li><li>BertEncoder<ul><li>BertAttention 注意力机制</li><li>BertIntermediate 全连接和激活函数</li><li>BertOutput 全连接、残差链接和正则化</li></ul></li><li>BertPooler 取出[CLS]对应的向量，然后通过全连接层和激活函数后输出结果</li></ul></li></ul><h1 id="BERT的实现"><a href="#BERT的实现" class="headerlink" title="BERT的实现"></a>BERT的实现</h1><h2 id="BertConfig"><a href="#BertConfig" class="headerlink" title="BertConfig"></a>BertConfig</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">classtransformers.BertConfig(vocab_size=<span class="number">30522</span>, hidden_size=<span class="number">768</span>, </span><br><span class="line">    num_hidden_layers=<span class="number">12</span>, num_attention_heads=<span class="number">12</span>, intermediate_size=<span class="number">3072</span>, </span><br><span class="line">    hidden_act=<span class="string">&#x27;gelu&#x27;</span>, hidden_dropout_prob=<span class="number">0.1</span>, attention_probs_dropout_prob=<span class="number">0.1</span></span><br><span class="line">    , max_position_embeddings=<span class="number">512</span>, type_vocab_size=<span class="number">2</span>, initializer_range=<span class="number">0.02</span>, </span><br><span class="line">    layer_norm_eps=<span class="number">1e-12</span>, pad_token_id=<span class="number">0</span>, gradient_checkpointing=<span class="literal">False</span>, </span><br><span class="line">    position_embedding_type=<span class="string">&#x27;absolute&#x27;</span>, use_cache=<span class="literal">True</span>, classifier_dropout=<span class="literal">None</span>,</span><br><span class="line">     **kwargs)</span><br></pre></td></tr></table></figure><p>这是存储BertModel（Torch.nn.Module的子类）或TFBertModel（tf.keras.Model的子类）配置的配置类。它用于根据指定的参数来实例化BERT模型，定义模型架构。</p><p>配置对象从PretrainedConfig继承，可用于控制模型输出。</p><p>参数：</p><ul><li>vocab_size: BERT模型的词汇量，定义了能被inputs_ids表示的token数量。</li><li>hidden_size: </li></ul><h2 id="BertTokenizer"><a href="#BertTokenizer" class="headerlink" title="BertTokenizer"></a>BertTokenizer</h2><h2 id="BertModel"><a href="#BertModel" class="headerlink" title="BertModel"></a>BertModel</h2><h1 id="BERT的应用"><a href="#BERT的应用" class="headerlink" title="BERT的应用"></a>BERT的应用</h1><h2 id="BertForPreTraining"><a href="#BertForPreTraining" class="headerlink" title="BertForPreTraining"></a>BertForPreTraining</h2><h2 id="BertForNextSentencePrediction"><a href="#BertForNextSentencePrediction" class="headerlink" title="BertForNextSentencePrediction"></a>BertForNextSentencePrediction</h2><h2 id="BertForSequenceClassification"><a href="#BertForSequenceClassification" class="headerlink" title="BertForSequenceClassification"></a>BertForSequenceClassification</h2><h2 id="BertForMultipleChoice"><a href="#BertForMultipleChoice" class="headerlink" title="BertForMultipleChoice"></a>BertForMultipleChoice</h2><h2 id="BertForTokenClassification"><a href="#BertForTokenClassification" class="headerlink" title="BertForTokenClassification"></a>BertForTokenClassification</h2><h2 id="BertForQuestionAnswering"><a href="#BertForQuestionAnswering" class="headerlink" title="BertForQuestionAnswering"></a>BertForQuestionAnswering</h2><h1 id="BERT的训练和优化"><a href="#BERT的训练和优化" class="headerlink" title="BERT的训练和优化"></a>BERT的训练和优化</h1><h2 id="Pre-Training"><a href="#Pre-Training" class="headerlink" title="Pre-Training"></a>Pre-Training</h2><h2 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine-Tuning"></a>Fine-Tuning</h2><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>HuggingFace/transfomers/BERT <a href="https://huggingface.co/transformers/model_doc/bert.html#">https://huggingface.co/transformers/model_doc/bert.html#</a></li><li>基于transformers的自然语言处理(NLP)入门–在线阅读 <a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/">https://datawhalechina.github.io/learn-nlp-with-transformers/#/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h1&gt;&lt;p&gt;本部分是BERT源码的解读，来自HuggingFace/transfomers/BERT[1</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="2021-09 基于transformer的NLP" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/2021-09-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="预训练模型" scheme="http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="BERT" scheme="http://example.com/tags/BERT/"/>
    
    <category term="transfomer" scheme="http://example.com/tags/transfomer/"/>
    
  </entry>
  
  <entry>
    <title>Task04 学习GPT</title>
    <link href="http://example.com/nlp-transformer-task04/"/>
    <id>http://example.com/nlp-transformer-task04/</id>
    <published>2021-09-19T12:02:17.000Z</published>
    <updated>2022-01-24T15:27:42.270Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从语言模型说起"><a href="#从语言模型说起" class="headerlink" title="从语言模型说起"></a>从语言模型说起</h1><h2 id="自编码语言模型（auto-encoder）"><a href="#自编码语言模型（auto-encoder）" class="headerlink" title="自编码语言模型（auto-encoder）"></a>自编码语言模型（auto-encoder）</h2><p>自编码语言模型通过随机Mask输入的部分单词，然后预训练的目标是预测被Mask的单词，不仅可以融入上文信息，还可以自然的融入下文信息。ex. BERT.</p><ul><li>优点：自然地融入双向语言模型，同时看到被预测单词的上文和下文</li><li>缺点：训练和预测不一致。训练的时候输入引入了[Mask]标记，但是在预测阶段往往没有这个[Mask]标记，导致预训练阶段和Fine-tuning阶段不一致。</li></ul><h2 id="自回归语言模型（auto-regressive）"><a href="#自回归语言模型（auto-regressive）" class="headerlink" title="自回归语言模型（auto-regressive）"></a>自回归语言模型（auto-regressive）</h2><p>语言模型根据输入句子的一部分文本来预测下一个词。ex. GPT-2</p><ul><li>优点：对于生成类的NLP任务，比如文本摘要，机器翻译等，从左向右的生成内容，天然和自回归语言模型契合。</li><li>缺点：由于一般是从左到右（当然也可能从右到左），所以只能利用上文或者下文的信息，不能同时利用上文和下文的信息。</li></ul><h1 id="Transformer-BERT-GPT-2的关系"><a href="#Transformer-BERT-GPT-2的关系" class="headerlink" title="Transformer, BERT, GPT-2的关系"></a>Transformer, BERT, GPT-2的关系</h1><p>Transformer的Encoder进化成了BERT，Decoder进化成了GPT2。</p><p>如果要使用Transformer来解决语言模型任务，并不需要完整的Encoder部分和Decoder部分，于是在原始Transformer之后的许多研究工作中，人们尝试只使用Transformer Encoder或者Decoder进行预训练。比如BERT只使用了Encoder部分进行masked language model（自编码）训练，GPT-2便是只使用了Decoder部分进行自回归（auto regressive）语言模型训练。</p><h1 id="GPT-2概述"><a href="#GPT-2概述" class="headerlink" title="GPT-2概述"></a>GPT-2概述</h1><h2 id="模型的输入"><a href="#模型的输入" class="headerlink" title="模型的输入"></a>模型的输入</h2><p>输入的处理分为两步：token embedding + position encoding。即:</p><ol><li>在嵌入矩阵中查找输入的单词的对应的embedding向量</li><li>融入位置编码</li></ol><h2 id="Decoder层"><a href="#Decoder层" class="headerlink" title="Decoder层"></a>Decoder层</h2><p>每一层decoder的组成：Masked Self-Attention + Feed Forward Neural Network</p><p>Self-Attention所做的事情是：它通过对句子片段中每个词的相关性打分，并将这些词的表示向量根据相关性加权求和，从而让模型能够将词和其他相关词向量的信息融合起来。</p><p>Masked Self-Attention做的是：将mask位置对应的的attention score变成一个非常小的数字或者0，让其他单词再self attention的时候（加权求和的时候）不考虑这些单词。</p><h2 id="模型的输出"><a href="#模型的输出" class="headerlink" title="模型的输出"></a>模型的输出</h2><p>当模型顶部的Decoder层产生输出向量时，模型会将这个向量乘以一个巨大的嵌入矩阵（vocab size x embedding size）来计算该向量和所有单词embedding向量的相关得分。这个相乘的结果，被解释为模型词汇表中每个词的分数，经过softmax之后被转换成概率。</p><p>我们可以选择最高分数的 token（top_k=1），也可以同时考虑其他词（top k）。假设每个位置输出k个token，假设总共输出n个token，那么基于n个单词的联合概率选择的输出序列会更好。</p><p>模型完成一次迭代，输出一个单词。模型会继续迭代，直到所有的单词都已经生成，或者直到输出了表示句子末尾的token。</p><h1 id="关于Self-Attention-Masked-Self-Attention"><a href="#关于Self-Attention-Masked-Self-Attention" class="headerlink" title="关于Self-Attention, Masked Self-Attention"></a>关于Self-Attention, Masked Self-Attention</h1><h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>Self-Attention 主要通过 3 个步骤来实现：</p><ol><li>为每个路径创建 Query、Key、Value 矩阵。</li><li>对于每个输入的token，使用它的Query向量为所有其他的Key向量进行打分。</li><li>将 Value 向量乘以它们对应的分数后求和。</li></ol><h2 id="Masked-Self-Attention"><a href="#Masked-Self-Attention" class="headerlink" title="Masked Self-Attention"></a>Masked Self-Attention</h2><p>在Self-Attention的第2步，把未来的 token 评分设置为0，因此模型不能看到未来的词。</p><p>这个屏蔽（masking）经常用一个矩阵来实现，称为 attention mask矩阵。<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/mask_1.jpg?raw=true" width="600" alt="" align="center" /><br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/mask_2.jpg?raw=true" width="600" alt="" align="center" /></p><h2 id="GPT-2中的Self-Attention"><a href="#GPT-2中的Self-Attention" class="headerlink" title="GPT-2中的Self-Attention"></a>GPT-2中的Self-Attention</h2><p>(skip)</p><h1 id="自回归语言模型的应用"><a href="#自回归语言模型的应用" class="headerlink" title="自回归语言模型的应用"></a>自回归语言模型的应用</h1><p>应用在下游并取得不错效果的NLP任务有：机器翻译、摘要生成、音乐生成。<em>（可见，主要是跟预训练任务相似的生成类任务。）</em></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>基于transformers的自然语言处理(NLP)入门–在线阅读 <a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/">https://datawhalechina.github.io/learn-nlp-with-transformers/#/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;从语言模型说起&quot;&gt;&lt;a href=&quot;#从语言模型说起&quot; class=&quot;headerlink&quot; title=&quot;从语言模型说起&quot;&gt;&lt;/a&gt;从语言模型说起&lt;/h1&gt;&lt;h2 id=&quot;自编码语言模型（auto-encoder）&quot;&gt;&lt;a href=&quot;#自编码语言模型（auto</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="2021-09 基于transformer的NLP" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/2021-09-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="预训练模型" scheme="http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="GPT" scheme="http://example.com/tags/GPT/"/>
    
  </entry>
  
  <entry>
    <title>Chapter01-02 PyTorch的简介和安装、PyTorch基础知识</title>
    <link href="http://example.com/pytorch-chap01-02/"/>
    <id>http://example.com/pytorch-chap01-02/</id>
    <published>2021-09-18T02:26:03.000Z</published>
    <updated>2022-01-24T15:28:56.569Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第一章-PyTorch的简介和安装"><a href="#第一章-PyTorch的简介和安装" class="headerlink" title="第一章 PyTorch的简介和安装"></a>第一章 PyTorch的简介和安装</h1><h2 id="PyTorch简介"><a href="#PyTorch简介" class="headerlink" title="PyTorch简介"></a>PyTorch简介</h2><p>PyTorch是由Facebook人工智能研究小组开发的一种基于Lua编写的Torch库的Python实现的深度学习库，目前被广泛应用于学术界和工业界，而随着Caffe2项目并入Pytorch， Pytorch开始影响到TensorFlow在深度学习应用框架领域的地位。总的来说，PyTorch是当前难得的简洁优雅且高效快速的框架。因此本课程我们选择了PyTorch来进行开源学习。</p><h2 id="PyTorch的安装"><a href="#PyTorch的安装" class="headerlink" title="PyTorch的安装"></a>PyTorch的安装</h2><p>PyTorch官网：<a href="https://pytorch.org/">https://pytorch.org/</a></p><h2 id="PyTorch的发展和优势"><a href="#PyTorch的发展和优势" class="headerlink" title="PyTorch的发展和优势"></a>PyTorch的发展和优势</h2><p>“All in Pytorch”.</p><h2 id="PyTorch-VS-TensorFlow"><a href="#PyTorch-VS-TensorFlow" class="headerlink" title="PyTorch VS TensorFlow"></a>PyTorch VS TensorFlow</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/03_torch_vs_tf.jpg?raw=true" width="600" alt="" align="center" /><h1 id="第二章-PyTorch的基础知识"><a href="#第二章-PyTorch的基础知识" class="headerlink" title="第二章 PyTorch的基础知识"></a>第二章 PyTorch的基础知识</h1><h2 id="Tensor-张量"><a href="#Tensor-张量" class="headerlink" title="Tensor/张量"></a>Tensor/张量</h2><p>张量是基于向量和矩阵的推广，比如我们可以将标量视为零阶张量，矢量可以视为一阶张量，矩阵就是二阶张量。</p><ul><li>0维张量/标量 标量是1个数字</li><li>1维张量/向量 1维张量称为“向量”</li><li>2维张量 2维张量称为“矩阵”</li><li>3维张量 时间序列数据、股价、文本数据、彩色图片(RGB)</li><li>4维=图像</li><li>5维=视频</li></ul><p>在PyTorch中， torch.Tensor 是存储和变换数据的主要工具。</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/04_tensor.jpg?raw=true" width="600" alt="" align="center" /><h3 id="tensor-构造"><a href="#tensor-构造" class="headerlink" title="tensor-构造"></a>tensor-构造</h3><p>创建一个随机初始化的矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">4</span>, <span class="number">3</span>)  <span class="comment"># 构造张量</span></span><br><span class="line"><span class="built_in">print</span>(x.size())  <span class="comment"># 获取维度信息</span></span><br><span class="line"><span class="built_in">print</span>(x.shape)  <span class="comment"># 获取维度信息</span></span><br></pre></td></tr></table></figure><p>还有一些常见的构造Tensor的函数：<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/01_tensor_1.jpg?raw=true" width="400" alt="" align="center" /></p><p>PyTorch中的Tensor支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考官方文档。</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/05_tensor.jpg?raw=true" width="600" alt="" align="center" /><h3 id="tensor-squeeze-增加-删除一个维度"><a href="#tensor-squeeze-增加-删除一个维度" class="headerlink" title="tensor-squeeze 增加/删除一个维度"></a>tensor-squeeze 增加/删除一个维度</h3><img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/06_tensor.jpg?raw=true" width="600" alt="" align="center" /><img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/07_tensor.jpg?raw=true" width="600" alt="" align="center" /><h3 id="tensor-transpose-转置"><a href="#tensor-transpose-转置" class="headerlink" title="tensor-transpose 转置"></a>tensor-transpose 转置</h3><img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/08_tensor.jpg?raw=true" width="600" alt="" align="center" /><h3 id="tensor-cat-concatenate多个tensor"><a href="#tensor-cat-concatenate多个tensor" class="headerlink" title="tensor-cat concatenate多个tensor"></a>tensor-cat concatenate多个tensor</h3><img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/09_tensor.jpg?raw=true" width="600" alt="" align="center" /><h2 id="自动求导-自动微分"><a href="#自动求导-自动微分" class="headerlink" title="自动求导/自动微分"></a>自动求导/自动微分</h2><p>PyTorch中，所有神经网络的核心是autograd包。autograd包为张量上的所有操作提供了自动求导机制。</p><h3 id="How-to-Calculate-Gradient"><a href="#How-to-Calculate-Gradient" class="headerlink" title="How to Calculate Gradient"></a>How to Calculate Gradient</h3><img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/10_tensor.jpg?raw=true" width="600" alt="" align="center" /><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([[<span class="number">1.</span>, <span class="number">0.</span>], [-<span class="number">1.</span>, <span class="number">1.</span>]], requires_grad = <span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [-<span class="number">1.</span>,  <span class="number">1.</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = x.<span class="built_in">pow</span>(<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]], grad_fn=&lt;PowBackward0&gt;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = z.<span class="built_in">sum</span>()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z</span><br><span class="line">tensor(<span class="number">3.</span>, grad_fn=&lt;SumBackward0&gt;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.backward()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z</span><br><span class="line">tensor(<span class="number">3.</span>, grad_fn=&lt;SumBackward0&gt;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.grad</span><br><span class="line">tensor([[ <span class="number">2.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [-<span class="number">2.</span>,  <span class="number">2.</span>]])</span><br></pre></td></tr></table></figure><h2 id="并行计算简介"><a href="#并行计算简介" class="headerlink" title="并行计算简介"></a>并行计算简介</h2><p>在利用PyTorch做深度学习的过程中，可能会遇到数据量较大无法在单块GPU上完成，或者需要提升计算速度的场景，这时就需要用到并行计算。<br>GPU的出现让我们可以训练的更快，更好。PyTorch可以在编写完模型之后，让多个GPU来参与训练。</p><p><code>CUDA</code>是我们使用GPU的提供商——NVIDIA提供的GPU并行计算框架。对于GPU本身的编程，使用的是<code>CUDA</code>语言来实现的。但是，在我们使用PyTorch编写深度学习代码时，使用的<code>CUDA</code>又是另一个意思。在PyTorch使用 <code>CUDA</code>表示要开始要求我们的模型或者数据开始使用GPU了。</p><p>在编写程序中，当我们使用了 <code>cuda()</code> 时，其功能是让我们的模型或者数据迁移到GPU当中，通过GPU开始计算。</p><p>不同的数据分布到不同的设备中，执行相同的任务(Data parallelism):<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/pytorch/02_pc_1.png?raw=true" width="250" alt="" align="center" /></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>Datawhale开源项目：深入浅出PyTorch <a href="https://github.com/datawhalechina/thorough-pytorch/">https://github.com/datawhalechina/thorough-pytorch/</a></li><li>李宏毅机器学习2021春-PyTorch Tutorial <a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=5">https://www.bilibili.com/video/BV1Wv411h7kN?p=5</a></li><li>What is a gpu and do you need one in deep learning <a href="https://towardsdatascience.com/what-is-a-gpu-and-do-you-need-one-in-deep-learning-718b9597aa0d">https://towardsdatascience.com/what-is-a-gpu-and-do-you-need-one-in-deep-learning-718b9597aa0d</a></li><li>动手学深度学习pytorch版 <a href="https://zh-v2.d2l.ai/chapter_preface/index.html">https://zh-v2.d2l.ai/chapter_preface/index.html</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;第一章-PyTorch的简介和安装&quot;&gt;&lt;a href=&quot;#第一章-PyTorch的简介和安装&quot; class=&quot;headerlink&quot; title=&quot;第一章 PyTorch的简介和安装&quot;&gt;&lt;/a&gt;第一章 PyTorch的简介和安装&lt;/h1&gt;&lt;h2 id=&quot;PyTor</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="2021-10 深入浅出PyTorch" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/2021-10-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAPyTorch/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="PyTorch" scheme="http://example.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Task03 学习BERT</title>
    <link href="http://example.com/nlp-transformer-task03/"/>
    <id>http://example.com/nlp-transformer-task03/</id>
    <published>2021-09-17T01:44:18.000Z</published>
    <updated>2022-01-24T15:27:36.314Z</updated>
    
    <content type="html"><![CDATA[<h1 id="BERT简介"><a href="#BERT简介" class="headerlink" title="BERT简介"></a>BERT简介</h1><p>BERT首先在大规模无监督语料上进行预训练，然后在预训练好的参数基础上增加一个与任务相关的神经网络层，并在该任务的数据上进行微调训，最终取得很好的效果。<strong>BERT的这个训练过程可以简述为：预训练（pre-train）+微调（fine-tune/fine-tuning），已经成为最近几年最流行的NLP解决方案的范式。</strong></p><h2 id="如何直接应用BERT"><a href="#如何直接应用BERT" class="headerlink" title="如何直接应用BERT"></a>如何直接应用BERT</h2><ol><li>下载在无监督语料上预训练好的BERT模型，一般来说对应了3个文件：BERT模型配置文件（用来确定Transformer的层数，隐藏层大小等），BERT模型参数，BERT词表（BERT所能处理的所有token）。</li><li>针对特定任务需要，在BERT模型上增加一个任务相关的神经网络，比如一个简单的分类器，然后在特定任务监督数据上进行微调训练。（微调的一种理解：学习率较小，训练epoch数量较少，对模型整体参数进行轻微调整）</li></ol><h2 id="BERT的结构"><a href="#BERT的结构" class="headerlink" title="BERT的结构"></a>BERT的结构</h2><p><strong>BERT模型结构基本上就是Transformer的encoder部分。</strong></p><h2 id="BERT的输入和输出"><a href="#BERT的输入和输出" class="headerlink" title="BERT的输入和输出"></a>BERT的输入和输出</h2><p>BERT模型输入有一点特殊的地方是在一句话最开始拼接了一个[CLS] token，如下图所示。这个特殊的[CLS] token经过BERT得到的向量表示通常被用作当前的句子表示。我们直接使用第1个位置的向量输出（对应的是[CLS]）传入classifier网络，然后进行分类任务。</p><h1 id="BERT的预训练任务"><a href="#BERT的预训练任务" class="headerlink" title="BERT的预训练任务"></a>BERT的预训练任务</h1><p>BERT是一个多任务模型，它的任务是由两个自监督任务组成。</p><h2 id="Masked-Language-Model（MLM）"><a href="#Masked-Language-Model（MLM）" class="headerlink" title="Masked Language Model（MLM）"></a>Masked Language Model（MLM）</h2><p>MLM：将输入文本序列的部分（15%）单词随机Mask掉，让BERT来预测这些被Mask的词语。<em>（可以说是完形填空）</em></p><blockquote><p>Masked Language Model（MLM）和核心思想取自Wilson Taylor在1953年发表的一篇论文《cloze procedure: A new tool for measuring readability》。所谓MLM是指在训练的时候随即从输入预料上mask掉一些单词，然后通过的上下文预测该单词，该任务非常像我们在中学时期经常做的完形填空。正如传统的语言模型算法和RNN匹配那样，MLM的这个性质和Transformer的结构是非常匹配的。</p></blockquote><h2 id="Next-Sentence-Prediction（NSP）"><a href="#Next-Sentence-Prediction（NSP）" class="headerlink" title="Next Sentence Prediction（NSP）"></a>Next Sentence Prediction（NSP）</h2><p>NSP：判断两个句子是否是相邻句子。即，输入是sentence A和sentence B，经过BERT编码之后，使用CLS token的向量表示来预测两个句子是否是相邻句子。</p><blockquote><p>Next Sentence Prediction（NSP）的任务是判断句子B是否是句子A的下文。如果是的话输出’IsNext‘，否则输出’NotNext‘。训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在[CLS]符号中。</p></blockquote><h1 id="BERT的应用"><a href="#BERT的应用" class="headerlink" title="BERT的应用"></a>BERT的应用</h1><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><p>由于BERT模型可以得到输入序列所对应的所有token的向量表示，因此不仅可以使用最后一程BERT的输出连接上任务网络进行微调，还可以直接使用这些token的向量当作特征。比如，可以直接提取每一层encoder的token表示当作特征，输入现有的特定任务神经网络中进行训练。</p><h2 id="Pretrain-Fine-tune"><a href="#Pretrain-Fine-tune" class="headerlink" title="Pretrain + Fine tune"></a>Pretrain + Fine tune</h2><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>基于transformers的自然语言处理(NLP)入门–在线阅读 <a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/">https://datawhalechina.github.io/learn-nlp-with-transformers/#/</a></li><li>李宏毅机器学习2019-ELMO,BERT,GPT <a href="https://www.bilibili.com/video/BV1Gb411n7dE?p=61">https:// www.bilibili.com/video/BV1Gb411n7dE?p=61</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;BERT简介&quot;&gt;&lt;a href=&quot;#BERT简介&quot; class=&quot;headerlink&quot; title=&quot;BERT简介&quot;&gt;&lt;/a&gt;BERT简介&lt;/h1&gt;&lt;p&gt;BERT首先在大规模无监督语料上进行预训练，然后在预训练好的参数基础上增加一个与任务相关的神经网络层，并在该</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="2021-09 基于transformer的NLP" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/2021-09-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="预训练模型" scheme="http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="BERT" scheme="http://example.com/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>Task02 学习Attentioin和Transformer</title>
    <link href="http://example.com/nlp-transformer-task02/"/>
    <id>http://example.com/nlp-transformer-task02/</id>
    <published>2021-09-17T01:09:24.000Z</published>
    <updated>2022-01-24T15:27:29.756Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><h2 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h2><p>seq2seq是一种常见的NLP模型结构，全称是：sequence to sequence，翻译为“序列到序列”。顾名思义：从一个文本序列得到一个新的文本序列。典型的任务有：机器翻译任务，文本摘要任务。</p><p>seq2seq模型由编码器（encoder）和解码器（decoder）组成，编码器用来分析输入序列，解码器用来生成输出序列。编码器会处理输入序列中的每个元素，把这些信息转换成为一个背景向量（context vector）。当我们处理完整个输入序列后，编码器把背景向量发送给解码器，解码器通过背景向量中的信息，逐个元素输出新的序列。</p><p><strong>在transformer模型之前，seq2seq中的编码器和解码器一般采用循环神经网络（RNN）</strong>，虽然非常经典，但是局限性也非常大。最大的局限性就在于编码器和解码器之间的唯一联系就是一个固定长度的context向量。也就是说，编码器要将整个序列的信息压缩进一个固定长度的向量中。这样做存在两个弊端：</p><ul><li>语义向量可能无法完全表示整个序列的信息</li><li>先输入到网络的内容携带的信息会被后输入的信息覆盖掉，输入序列越长，这个现象就越严重</li></ul><h2 id="Attention-1"><a href="#Attention-1" class="headerlink" title="Attention"></a>Attention</h2><p>为了解决seq2seq模型中的两个弊端，Bahdanau等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》中提出使用Attention机制，使得seq2seq模型可以有区分度、有重点地关注输入序列，从而极大地提高了机器翻译的质量。</p><p>一个有注意力机制的seq2seq与经典的seq2seq主要有2点不同：</p><ol><li>首先，编码器会把更多的数据传递给解码器。编码器把所有时间步的 hidden state（隐藏层状态）传递给解码器，而不是只传递最后一个 hidden state（隐藏层状态）</li><li>注意力模型的解码器在产生输出之前，做了一个额外的attention处理</li></ol><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>transformer原论文的架构图：</p><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/trm_1.png?raw=true" width="400" alt="" align="center" /><p>一个更清晰的架构图：<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/trm_2.png?raw=true" width="600" alt="" align="center" /></p><p>从输入到输出拆开看就是：</p><ul><li>INPUT：input vector + position encoding</li><li>ENCODERs（×6），and each encoder includes：<ul><li>input</li><li>multi-head self-attention</li><li>residual connection&amp;norm</li><li>full-connected network</li><li>residual connection&amp;norm</li><li>output</li></ul></li><li>DECODERs（×6），and each decoder includes：<ul><li>input </li><li>Masked multihead self-attention</li><li>residual connection&amp;norm</li><li>multi-head self-attention</li><li>residual connection&amp;norm</li><li>full-connected network</li><li>residual connection&amp;norm</li><li>output</li></ul></li><li>OUTPUT：<ul><li>output (decoder’s)</li><li>linear layer</li><li>softmax layer</li><li>output</li></ul></li></ul><h2 id="模型输入"><a href="#模型输入" class="headerlink" title="模型输入"></a>模型输入</h2><h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>和常见的NLP任务一样，我们首先会使用词嵌入算法（embedding），将输入文本序列的每个词转换为一个词向量。</p><h3 id="位置向量"><a href="#位置向量" class="headerlink" title="位置向量"></a>位置向量</h3><p>Transformer模型对每个输入的词向量都加上了一个位置向量。这些向量有助于确定每个单词的位置特征，或者句子中不同单词之间的距离特征。词向量加上位置向量背后的直觉是：将这些表示位置的向量添加到词向量中，得到的新向量，可以为模型提供更多有意义的信息，比如词的位置，词之间的距离等。</p><p><em>（生成位置编码向量的方法有很多种）</em></p><h2 id="编码器和解码器"><a href="#编码器和解码器" class="headerlink" title="编码器和解码器"></a>编码器和解码器</h2><p><em>注：1. 编码器和解码器中有相似的模块和结构，所以合并到一起介绍。</em><br><em>2. 本部分按照李宏毅老师的Attention，Transformer部分的课程PPT来，因为lee的课程对新手更友好。</em></p><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>self-attention对于每个向量都会考虑整个sequence的信息后输出一个向量，self-attention结构如下：<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/04_attention_1.png?raw=true" width="600" alt="" align="center" /><br>FC：Fully-connected network 全连接网络<br>ai: 输入变量。可能是整个网络的输入，也可能是某个隐藏层的输出<br>bi: 考虑整个sequence信息后的输出变量</p><p>矩阵计算：<br><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/13_matrix_4.jpg?raw=true" width="300" alt="" align="center" /><br>目标：根据输入向量矩阵I，计算输出向量矩阵O。矩阵运算过程：</p><ol><li>矩阵I分别乘以Wq, Wk, Wv（参数矩阵，需要模型进行学习），得到矩阵Q, K, V。</li><li>矩阵K的转置乘以Q，得到注意力权重矩阵A，归一化得到矩阵A’。</li><li>矩阵V乘矩阵A‘，得到输出向量矩阵O。</li></ol><h3 id="Multi-Head-Self-Attention"><a href="#Multi-Head-Self-Attention" class="headerlink" title="Multi Head Self-Attention"></a>Multi Head Self-Attention</h3><p><em>简单地说，多了几组Q，K，V。在Self-Attention中，我们是使用𝑞去寻找与之相关的𝑘，但是这个相关性并不一定有一种。那多种相关性体现到计算方式上就是有多个矩阵𝑞，不同的𝑞负责代表不同的相关性。</em></p><p>Transformer 的论文通过增加多头注意力机制（一组注意力称为一个 attention head），进一步完善了Self-Attention。这种机制从如下两个方面增强了attention层的能力：</p><ul><li>它扩展了模型关注不同位置的能力。</li><li>多头注意力机制赋予attention层多个“子表示空间”。</li></ul><h3 id="残差链接和归一化"><a href="#残差链接和归一化" class="headerlink" title="残差链接和归一化"></a>残差链接和归一化</h3><p>残差链接：一种把input向量和output向量直接加起来的架构。<br>归一化：把数据映射到0～1范围之内处理。</p><h2 id="模型输出"><a href="#模型输出" class="headerlink" title="模型输出"></a>模型输出</h2><h3 id="线性层和softmax"><a href="#线性层和softmax" class="headerlink" title="线性层和softmax"></a>线性层和softmax</h3><p>Decoder 最终的输出是一个向量，其中每个元素是浮点数。我们怎么把这个向量转换为单词呢？这是线性层和softmax完成的。</p><p>线性层就是一个普通的全连接神经网络，可以把解码器输出的向量，映射到一个更大的向量，这个向量称为 logits 向量：假设我们的模型有 10000 个英语单词（模型的输出词汇表），此 logits 向量便会有 10000 个数字，每个数表示一个单词的分数。</p><p>然后，Softmax 层会把这些分数转换为概率（把所有的分数转换为正数，并且加起来等于 1）。然后选择最高概率的那个数字对应的词，就是这个时间步的输出单词。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>Transformer训练的时候，需要将解码器的输出和label一同送入损失函数，以获得loss，最终模型根据loss进行方向传播。</p><p>只要Transformer解码器预测了组概率，我们就可以把这组概率和正确的输出概率做对比，然后使用反向传播来调整模型的权重，使得输出的概率分布更加接近整数输出。</p><p>那我们要怎么比较两个概率分布呢？：我们可以简单的用两组概率向量的的空间距离作为loss（向量相减，然后求平方和，再开方），当然也可以使用交叉熵(cross-entropy)]和KL 散度(Kullback–Leibler divergence)。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><strong>理论部分</strong><br>[1] (强推)李宏毅2021春机器学习课程 <a href="https://www.bilibili.com/video/BV1Wv411h7kN?from=search&seid=17090062977285779802&spm_id_from=333.337.0.0">https://www.bilibili.com/video/BV1Wv411h7kN?from=search&amp;seid=17090062977285779802&amp;spm_id_from=333.337.0.0</a><br>[2] <strong>基于transformers的自然语言处理(NLP)入门（涵盖了图解系列、annotated transformer、huggingface）</strong> <a href="https://github.com/datawhalechina/learn-nlp-with-transformers">https://github.com/datawhalechina/learn-nlp-with-transformers</a><br>[3] 图解transformer|The Illustrated Transformer <a href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a><br>[4] 图解seq2seq, attention|Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a></p><p><strong>代码部分</strong><br>[5] The Annotated Transformer <a href="http://nlp.seas.harvard.edu//2018/04/03/attention.html">http://nlp.seas.harvard.edu//2018/04/03/attention.html</a><br>[6] Huggingface/transformers <a href="https://github.com/huggingface/transformers/blob/master/README_zh-hans.md">https://github.com/huggingface/transformers/blob/master/README_zh-hans.md</a></p><p><strong>论文部分</strong><br>Attention is all “we” need.</p><p><strong>其他不错的博客或教程</strong><br>[7] 基于transformers的自然语言处理(NLP)入门–在线阅读 <a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/">https://datawhalechina.github.io/learn-nlp-with-transformers/#/</a><br>[8] 李宏毅2021春机器学习课程笔记——自注意力机制 <a href="https://www.cnblogs.com/sykline/p/14730088.html">https://www.cnblogs.com/sykline/p/14730088.html</a><br>[9] 李宏毅2021春机器学习课程笔记——Transformer模型 <a href="https://www.cnblogs.com/sykline/p/14785552.html">https://www.cnblogs.com/sykline/p/14785552.html</a><br>[10] 李宏毅机器学习学习笔记——自注意力机制 <a href="https://blog.csdn.net/p_memory/article/details/116271274">https://blog.csdn.net/p_memory/article/details/116271274</a><br>[11] 车万翔-自然语言处理新范式：基于预训练的方法【讲座+PPT】 <a href="https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true">https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_611f48f3e4b02ac39d12246f/3?fromH5=true</a><br>[12] 苏剑林-《Attention is All You Need》浅读（简介+代码）<a href="https://spaces.ac.cn/archives/4765">https://spaces.ac.cn/archives/4765</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Attention&quot;&gt;&lt;a href=&quot;#Attention&quot; class=&quot;headerlink&quot; title=&quot;Attention&quot;&gt;&lt;/a&gt;Attention&lt;/h1&gt;&lt;h2 id=&quot;seq2seq&quot;&gt;&lt;a href=&quot;#seq2seq&quot; class=&quot;he</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="2021-09 基于transformer的NLP" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/2021-09-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="预训练模型" scheme="http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="attention" scheme="http://example.com/tags/attention/"/>
    
    <category term="transfomer" scheme="http://example.com/tags/transfomer/"/>
    
  </entry>
  
  <entry>
    <title>Task01 NLP学习概览</title>
    <link href="http://example.com/nlp-transformer-task01/"/>
    <id>http://example.com/nlp-transformer-task01/</id>
    <published>2021-09-12T16:14:06.000Z</published>
    <updated>2022-01-24T15:26:56.969Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NLP思维导图-最近更新日期：2021-09-13"><a href="#NLP思维导图-最近更新日期：2021-09-13" class="headerlink" title="NLP思维导图(最近更新日期：2021-09-13)"></a>NLP思维导图(最近更新日期：2021-09-13)</h1><img src="https://github.com/chuxiaoyu/blog_image/blob/master/nlp/NLP.png?raw=true" width="900" alt="" align="center" /><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>Datawhale-基于transformers的自然语言处理(NLP)入门 <a href="https://github.com/datawhalechina/learn-nlp-with-transformers">https://github.com/datawhalechina/learn-nlp-with-transformers</a></li><li>《自然语言处理-基于预训练模型的方法》 <a href="https://item.jd.com/13344628.html">https://item.jd.com/13344628.html</a></li><li>刘知远老师-NLP研究入门之道 <a href="https://github.com/zibuyu/research_tao">https://github.com/zibuyu/research_tao</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;NLP思维导图-最近更新日期：2021-09-13&quot;&gt;&lt;a href=&quot;#NLP思维导图-最近更新日期：2021-09-13&quot; class=&quot;headerlink&quot; title=&quot;NLP思维导图(最近更新日期：2021-09-13)&quot;&gt;&lt;/a&gt;NLP思维导图(最近</summary>
      
    
    
    
    <category term="04 组队学习" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="2021-09 基于transformer的NLP" scheme="http://example.com/categories/04-%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/2021-09-%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84NLP/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="组队学习" scheme="http://example.com/tags/%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="预训练模型" scheme="http://example.com/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>CS61A Week1 Comupter_Science, Functions</title>
    <link href="http://example.com/cs61a-week1/"/>
    <id>http://example.com/cs61a-week1/</id>
    <published>2021-09-01T08:46:52.000Z</published>
    <updated>2021-09-17T01:51:24.035Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>CS61A作为61系列基础课程的第一门课程，是一门计算机入门导论课程，伯克利大一新生的第一门计算机课程。该课程主要使用Python语言，简要介绍了计算机的各种概念，范围广而涉猎不深，包括高阶函数，抽象，递归和树，OOP，简单的SQL语句，Scheme语法和解释器等概念。</p><p>目前推荐的课程是20年秋季学期(fa20)的课程。</p><p>——名校公开课程评价网 <a href="https://conanhujinming.github.io/comments-for-awesome-courses/UC%20BerkeleyCS61A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9E%84%E9%80%A0%E4%B8%8E%E8%A7%A3%E9%87%8A/">名校公开课程评价网-cs61a</a></p></blockquote><h2 id="文档组织"><a href="#文档组织" class="headerlink" title="文档组织"></a>文档组织</h2><p>对应不同教学内容，文档组织如下：</p><img src="/images/cs61a/01/zuzhi.jpg" width = "200" alt="" align="center" /><ul><li>0_课件：lecture</li><li>1_代码：lecture代码</li><li>2_笔记：学习笔记<ul><li>使用markdown</li><li>内容包括：Weekx内容(x=week number), Lecture Notes, Lab Notes, Homework Notes</li></ul></li><li>3_实验：lab</li><li>4_作业：homework</li><li>5_项目：project</li></ul><h2 id="Week1内容"><a href="#Week1内容" class="headerlink" title="Week1内容"></a>Week1内容</h2><img src="/images/cs61a/01/00.jpg" alt="" align="center" /><p>Week1主要内容：</p><ul><li><p>Lecture01 Computer Science; Lecture02 Functions 介绍计算机科学和函数基础知识</p></li><li><p>Lab00: Getting Started 安装Python3，终端的使用，常用命令行，文档测试（doctest）的使用，测试和提交使用OK系统</p><img src="/images/cs61a/01/02.jpg" width = "500" alt="" align="center" /></li><li><p>HW01: Variables &amp; Functions, Control。掌握函数特性</p></li></ul><h2 id="Lecture-Notes"><a href="#Lecture-Notes" class="headerlink" title="Lecture Notes"></a>Lecture Notes</h2><h3 id="What-is-Computer-Science"><a href="#What-is-Computer-Science" class="headerlink" title="What is Computer Science"></a>What is Computer Science</h3><img src="/images/cs61a/01/01.jpg" alt="" width = "700" align="center" /><p>计算机科学是一门定义和解决计算问题的方法和技术的学科。它的分支结构参考CSRanking的分类方式（ <a href="http://csrankings.org/#/index?all&world">http://csrankings.org/#/index?all&amp;world</a>），大概可以分为人工智能（计算机视觉、机器学习、自然语言处理、信息检索…）、系统（计算机结构、网络、安全、数据库、操作系统、分布式…）、理论（算法和复杂度、formal method…）、交叉（计算生物/生物计算、人机交互、机器人…）等方向。</p><h3 id="Anatomy-of-a-Call-Expression-Operator-Operand"><a href="#Anatomy-of-a-Call-Expression-Operator-Operand" class="headerlink" title="Anatomy of a Call Expression: Operator, Operand"></a>Anatomy of a Call Expression: Operator, Operand</h3><img src="/images/cs61a/01/03.jpg" width = "700" alt="" align="center" /><blockquote><p>在程式語言中, 指示程式進行運算(計算、比較或連結) 的符號, 稱為operators (運算子), 被運算的資料稱為operands (運算元), 一句中有operators 及operands 就稱為expression。</p></blockquote><h3 id="Environment-Diagrams"><a href="#Environment-Diagrams" class="headerlink" title="Environment Diagrams"></a>Environment Diagrams</h3><img src="/images/cs61a/01/04.jpg" width = "700" alt="" align="center" /><p>Environment Diagrams Tools: <a href="http://pythontutor.com/composingprograms.html#mode=edit">http://pythontutor.com/composingprograms.html#mode=edit</a></p><h3 id="Defining-Functions"><a href="#Defining-Functions" class="headerlink" title="Defining Functions"></a>Defining Functions</h3><img src="/images/cs61a/01/05.jpg" alt="" width = "700" align="center" /><img src="/images/cs61a/01/06.jpg" alt="" width = "700" align="center" /><img src="/images/cs61a/01/07.jpg" alt="" width = "700" align="center" /><p>这里涉及到全局变量（Global Variable）和局部变量（Local Variable）的问题，全局变量是整个程序都可访问的变量，生存期从程序开始到程序结束；局部变量存在于模块中(比如某个函数)，只有在模块中才可以访问，生存期从模块开始到模块结束。简单的说，</p><ul><li>全局变量：在模块内、在所有函数的外面、在class外面</li><li>局部变量：在函数内、在class的方法内</li></ul><h2 id="Lab-Notes"><a href="#Lab-Notes" class="headerlink" title="Lab Notes"></a>Lab Notes</h2><p>常用命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ls: lists all files in the current directory</span><br><span class="line">cd &lt;path to directory&gt;: change into the specified directory</span><br><span class="line">mkdir &lt;directory name&gt;: make a new directory with the given name</span><br><span class="line">mv &lt;source path&gt; &lt;destination path&gt;: move the file at the given source to the given destination</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python3 xxx.py # 运行程序</span><br><span class="line">python3 -i xxx.py # 运行程序并打开交互式会话</span><br><span class="line">python3 -m doctest xxx.py # 运行文档测试</span><br><span class="line">python3 -m doctest - xxx.py # 运行文档测试并显示样例</span><br></pre></td></tr></table></figure><h2 id="Homework-Notes"><a href="#Homework-Notes" class="headerlink" title="Homework Notes"></a>Homework Notes</h2><h3 id="Bug"><a href="#Bug" class="headerlink" title="Bug"></a>Bug</h3><p><code>TypeError: &#39;int&#39; object is not callable</code></p><p>修改程序名就可以了。</p><h3 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h3><p>Q5: If Function vs Statement</p><img src="/images/cs61a/01/hwp01.jpg" alt="" align="center" /><p>while…<br><img src="/images/cs61a/01/hwp02.jpg" alt=""  width="200" align="left" /><br><br><br><br></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li>cs61a 20fall 官网 <a href="https://inst.eecs.berkeley.edu/~cs61a/fa20/">https://inst.eecs.berkeley.edu/~cs61a/fa20/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;CS61A作为61系列基础课程的第一门课程，是一门计算机入门导论课程，伯克利大一新生的第一门计算机课程。该课程主要</summary>
      
    
    
    
    <category term="01 计算机基础" scheme="http://example.com/categories/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
    <category term="CS61A 计算机程序的构造与解释" scheme="http://example.com/categories/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/CS61A-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9E%84%E9%80%A0%E4%B8%8E%E8%A7%A3%E9%87%8A/"/>
    
    
    <category term="笔记" scheme="http://example.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="CS公开课" scheme="http://example.com/tags/CS%E5%85%AC%E5%BC%80%E8%AF%BE/"/>
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="function" scheme="http://example.com/tags/function/"/>
    
  </entry>
  
  <entry>
    <title>公式之美，EVERYTHING IS EPHEMERAL BUT FORMULA IS ETERNAL</title>
    <link href="http://example.com/formula/"/>
    <id>http://example.com/formula/</id>
    <published>2021-08-04T12:27:54.000Z</published>
    <updated>2021-11-28T12:03:57.137Z</updated>
    
    <content type="html"><![CDATA[<p>一个有点意思的科普书，尤其在不想写论文的时候，宁愿去看勾股定理的N种推导也不愿意碰论文。更好玩的是，这里面的插图要比内容更没有争议的获得一致好评。</p><blockquote><p>1854年之前，欧洲数学家灿若星辰，笛卡儿、拉格朗日、牛顿、贝叶斯、拉普拉斯、柯西、傅里叶、伽罗瓦等，无一不是数学天才。<br>1854—1935年，高斯、黎曼等人在数学界领袖群伦，德国取代英法成为世界的数学中心。<br>1935年之后，希特勒给美国送上“科学大礼包”：哥德尔、爱因斯坦、德拜、冯.诺依曼、费米、冯.卡门、外尔……很多科学家逃至北美，数学大本营从德国转向美国，美国成为世界的数学中心。</p></blockquote><blockquote><p>古希腊几何学家阿波洛尼乌斯总结了圆锥曲线理论，一千多年后，德国天文学家开普勒才将其应用于行星轨道；高斯被认为最早发现非欧几何，半个世纪后，由他弟子创立的黎曼几何成为广义相对论的数学基础。伴随着杠杆原理、牛顿三大定律、麦克斯韦方程、香农公式、贝叶斯定理等，人类向蒸汽时代、电力时代、信息时代乃至人工智能时代徐徐迈进。</p></blockquote><h2 id="1-1-2：数学的溯源"><a href="#1-1-2：数学的溯源" class="headerlink" title="1+1=2：数学的溯源"></a>1+1=2：数学的溯源</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/00-1plus1.jpg?raw=true" width="600" alt="" align="center" /><h2 id="勾股定理：数与形的结合"><a href="#勾股定理：数与形的结合" class="headerlink" title="勾股定理：数与形的结合"></a>勾股定理：数与形的结合</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/01-gougudingli.jpg?raw=true" width="600" alt="" align="center" /><h2 id="费马大定理：困扰人类358年"><a href="#费马大定理：困扰人类358年" class="headerlink" title="费马大定理：困扰人类358年"></a>费马大定理：困扰人类358年</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/02-feima.jpg?raw=true" width="600" alt="" align="center" /><h2 id="牛顿-莱布尼茨公式：无穷小的秘密"><a href="#牛顿-莱布尼茨公式：无穷小的秘密" class="headerlink" title="牛顿-莱布尼茨公式：无穷小的秘密"></a>牛顿-莱布尼茨公式：无穷小的秘密</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/03-weijifen.jpg?raw=true" width="600" alt="" align="center" /><h2 id="万有引力：从混沌到光明"><a href="#万有引力：从混沌到光明" class="headerlink" title="万有引力：从混沌到光明"></a>万有引力：从混沌到光明</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/04-wanyouyinli.jpg?raw=true" width="600" alt="" align="center" /><h2 id="欧拉公式：最美的等式"><a href="#欧拉公式：最美的等式" class="headerlink" title="欧拉公式：最美的等式"></a>欧拉公式：最美的等式</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/05-oula.jpg?raw=true" width="600" alt="" align="center" /><h2 id="伽罗瓦理论：无解的方程"><a href="#伽罗瓦理论：无解的方程" class="headerlink" title="伽罗瓦理论：无解的方程"></a>伽罗瓦理论：无解的方程</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/06-jialuowa.jpg?raw=true" width="600" alt="" align="center" /><h2 id="危险的黎曼猜想"><a href="#危险的黎曼猜想" class="headerlink" title="危险的黎曼猜想"></a>危险的黎曼猜想</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/07-liman.jpg?raw=true" width="600" alt="" align="center" /><h2 id="熵增定律：寂灭是宇宙宿命？"><a href="#熵增定律：寂灭是宇宙宿命？" class="headerlink" title="熵增定律：寂灭是宇宙宿命？"></a>熵增定律：寂灭是宇宙宿命？</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/08-shang.jpg?raw=true" width="600" alt="" align="center" /><h2 id="麦克斯韦方程组：让黑暗消失"><a href="#麦克斯韦方程组：让黑暗消失" class="headerlink" title="麦克斯韦方程组：让黑暗消失"></a>麦克斯韦方程组：让黑暗消失</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/09-mksw.jpg?raw=true" width="600" alt="" align="center" /><h2 id="质能方程：开启潘多拉的魔盒"><a href="#质能方程：开启潘多拉的魔盒" class="headerlink" title="质能方程：开启潘多拉的魔盒"></a>质能方程：开启潘多拉的魔盒</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/10-zhineng.jpg?raw=true" width="600" alt="" align="center" /><h2 id="薛定谔方程：猫与量子世界"><a href="#薛定谔方程：猫与量子世界" class="headerlink" title="薛定谔方程：猫与量子世界"></a>薛定谔方程：猫与量子世界</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/11-uedinge.jpg?raw=true" width="600" alt="" align="center" /><h2 id="狄拉克方程：反物质的“先知”"><a href="#狄拉克方程：反物质的“先知”" class="headerlink" title="狄拉克方程：反物质的“先知”"></a>狄拉克方程：反物质的“先知”</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/12-dilake.jpg?raw=true" width="600" alt="" align="center" /><h2 id="杨-米尔斯规范场论：大统一之路"><a href="#杨-米尔斯规范场论：大统一之路" class="headerlink" title="杨-米尔斯规范场论：大统一之路"></a>杨-米尔斯规范场论：大统一之路</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/13-yang.jpg?raw=true" width="600" alt="" align="center" /><h2 id="香农公式：5G背后的主宰"><a href="#香农公式：5G背后的主宰" class="headerlink" title="香农公式：5G背后的主宰"></a>香农公式：5G背后的主宰</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/14-xiangnong.jpg?raw=true" width="600" alt="" align="center" /><h2 id="布莱克-斯科尔斯方程：金融“巫师”"><a href="#布莱克-斯科尔斯方程：金融“巫师”" class="headerlink" title="布莱克-斯科尔斯方程：金融“巫师”"></a>布莱克-斯科尔斯方程：金融“巫师”</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/15-bulaike.jpg?raw=true" width="600" alt="" align="center" /><h2 id="枪械：弹道里的“技术哲学”"><a href="#枪械：弹道里的“技术哲学”" class="headerlink" title="枪械：弹道里的“技术哲学”"></a>枪械：弹道里的“技术哲学”</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/16-qiangxie.jpg?raw=true" width="600" alt="" align="center" /><h2 id="胡克定律：机械表的心脏"><a href="#胡克定律：机械表的心脏" class="headerlink" title="胡克定律：机械表的心脏"></a>胡克定律：机械表的心脏</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/17-huke.jpg?raw=true" width="600" alt="" align="center" /><h2 id="混沌理论：一只蝴蝶引发的思考"><a href="#混沌理论：一只蝴蝶引发的思考" class="headerlink" title="混沌理论：一只蝴蝶引发的思考"></a>混沌理论：一只蝴蝶引发的思考</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/18-hundun.jpg?raw=true" width="600" alt="" align="center" /><h2 id="凯利公式：赌场上的最大赢家"><a href="#凯利公式：赌场上的最大赢家" class="headerlink" title="凯利公式：赌场上的最大赢家"></a>凯利公式：赌场上的最大赢家</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/19-kaili.jpg?raw=true" width="600" alt="" align="center" /><h2 id="贝叶斯定理：AI如何思考？"><a href="#贝叶斯定理：AI如何思考？" class="headerlink" title="贝叶斯定理：AI如何思考？"></a>贝叶斯定理：AI如何思考？</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/20-beiyesi.jpg?raw=true" width="600" alt="" align="center" /><h2 id="三体问题：挥之不去的乌云"><a href="#三体问题：挥之不去的乌云" class="headerlink" title="三体问题：挥之不去的乌云"></a>三体问题：挥之不去的乌云</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/21-santi.jpg?raw=true" width="600" alt="" align="center" /><h2 id="椭圆曲线方程：比特币的基石"><a href="#椭圆曲线方程：比特币的基石" class="headerlink" title="椭圆曲线方程：比特币的基石"></a>椭圆曲线方程：比特币的基石</h2><img src="https://github.com/chuxiaoyu/blog_image/blob/master/formula/22-bitcoin.jpg?raw=true" width="600" alt="" align="center" /><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>《公式之美》<a href="https://book.douban.com/subject/35218287/">https://book.douban.com/subject/35218287/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;一个有点意思的科普书，尤其在不想写论文的时候，宁愿去看勾股定理的N种推导也不愿意碰论文。更好玩的是，这里面的插图要比内容更没有争议的获得一致好评。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1854年之前，欧洲数学家灿若星辰，笛卡儿、拉格朗日、牛顿、贝叶斯、拉普拉斯、柯西、傅</summary>
      
    
    
    
    <category term="沉思录" scheme="http://example.com/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"/>
    
    
    <category term="阅读" scheme="http://example.com/tags/%E9%98%85%E8%AF%BB/"/>
    
    <category term="math" scheme="http://example.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>SQL表连接&amp;聚合函数&amp;窗口函数</title>
    <link href="http://example.com/SQL%E9%87%8D%E7%82%B9/"/>
    <id>http://example.com/SQL%E9%87%8D%E7%82%B9/</id>
    <published>2021-07-26T19:33:19.000Z</published>
    <updated>2021-09-17T01:53:20.789Z</updated>
    
    <content type="html"><![CDATA[<h1 id="表连接-join"><a href="#表连接-join" class="headerlink" title="表连接 join"></a>表连接 join</h1><p>join: 以字段（列）为单位进行多表连接。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Inner</span> <span class="keyword">join</span> # 只保留两个表中同时存在的记录。</span><br><span class="line"><span class="keyword">Left</span> <span class="keyword">join</span> # 保留左表所有的记录，无论其是否能够在右表中匹配到对应的记录。若无匹配记录，则需要用<span class="keyword">NULL</span>填补。</span><br><span class="line"><span class="keyword">Right</span> <span class="keyword">join</span> # 保留右表所有的记录，无论其是否能够在左表中匹配到对应的记录。若无匹配记录，则需要用<span class="keyword">NULL</span>填补。</span><br><span class="line"><span class="keyword">Full</span> <span class="keyword">join</span> # 左表和右表所有的记录都会保留，没有匹配记录的用<span class="keyword">NULL</span>填补。</span><br></pre></td></tr></table></figure><h1 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sum</span>() # 返回分组后组内所有记录的和</span><br><span class="line"><span class="built_in">avg</span>() # 返回分组后组内所有记录的均值</span><br><span class="line"><span class="built_in">count</span>() # 返回分组后组内所有记录的计数</span><br><span class="line"><span class="built_in">max</span>()<span class="operator">/</span><span class="built_in">min</span>() # 返回分组后组内所有记录的最大值、最小值</span><br></pre></td></tr></table></figure><h1 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h1><p>窗口函数对记录分组之后进行聚合计算，为分组中的每条记录返回特定值。</p><p>窗口函数的基本结构是：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&lt;</span>窗口函数<span class="operator">&gt;</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> <span class="operator">&lt;</span>col1, col2<span class="operator">&gt;</span> </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> <span class="operator">&lt;</span>col3 <span class="keyword">desc</span><span class="operator">/</span><span class="keyword">asc</span>, col4 <span class="keyword">asc</span><span class="operator">/</span><span class="keyword">desc</span><span class="operator">&gt;</span>)</span><br></pre></td></tr></table></figure><table><thead><tr><th>窗口函数</th><th>介绍</th></tr></thead><tbody><tr><td><code>rank() over()</code></td><td>返回记录在同一分组内的排序，如果有并列名次的行，会占用下一名次的位置</td></tr><tr><td><code>dense_rank() over()</code></td><td>返回记录在同一分组内的排序，如果有并列名次的行，不占用下一名次的位置</td></tr><tr><td><code>row_number() over()</code></td><td>返回记录在同一分组内的排序，不考虑并列名次的情况</td></tr><tr><td><code>percent_rank() over()</code></td><td>返回记录在同一分组内排序的分位数，为0~1</td></tr><tr><td><code>sum(col) over()</code></td><td>返回同一分组内所有记录col值的和，同一分组内记录的返回值相同</td></tr><tr><td><code>avg(col) over()</code></td><td>返回同一分组内所有记录col值的平均值，同一分组内记录的返回值相同</td></tr><tr><td><code>max/min(col) over()</code></td><td>返回同一分组内所有记录col值的最大值/最小值，同一分组内记录的返回值相同</td></tr></tbody></table><p>聚合函数在窗口函数中，是对自身记录、及位于自身记录以上的数据进行运算的结果。聚合函数作为窗口函数，可以在每一行的数据里直观的看到，截止到本行数据，统计数据是多少（最大值、最小值等）。同时可以看出每一行数据，对整体统计数据的影响。</p><h1 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h1><p>索引用来排序数据以加快搜索和排序操作的速度。可以在一个或多个列上定义索引，使DBMS保存其内容的一个排过序的列表。在定义了索引后，DBMS以使用书的索引类似的方法使用它。DBMS 搜索排过序的索引，找出匹配的位置，然后检索这些行。</p><p>索引是关系数据库中对某一列或多个列的值进行预排序的数据结构。通过使用索引，可以让数据库系统不必扫描整个表，而是直接定位到符合条件的记录，这样就大大加快了查询速度。</p><p>索引用CREATE INDEX 语句创建（不同DBMS创建索引的语句变化很大）。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1]通俗易懂的学会：SQL窗口函数 <a href="https://zhuanlan.zhihu.com/p/92654574">https://zhuanlan.zhihu.com/p/92654574</a><br>[2]《拿下Offer:数据分析师求职面试指南》 <a href="https://item.jd.com/12686131.html">https://item.jd.com/12686131.html</a><br>[3]《SQL必知必会》 <a href="https://book.douban.com/subject/24250054/">https://book.douban.com/subject/24250054/</a><br>[4]廖雪峰的官方网站-SQL教程 <a href="https://www.liaoxuefeng.com/wiki/1177760294764384">https://www.liaoxuefeng.com/wiki/1177760294764384</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;表连接-join&quot;&gt;&lt;a href=&quot;#表连接-join&quot; class=&quot;headerlink&quot; title=&quot;表连接 join&quot;&gt;&lt;/a&gt;表连接 join&lt;/h1&gt;&lt;p&gt;join: 以字段（列）为单位进行多表连接。&lt;/p&gt;
&lt;figure class=&quot;high</summary>
      
    
    
    
    <category term="02 人工智能" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="数据分析" scheme="http://example.com/categories/02-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
    <category term="SQL" scheme="http://example.com/tags/SQL/"/>
    
  </entry>
  
</feed>
